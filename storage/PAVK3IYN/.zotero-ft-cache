Hands-On Machine Learning
with Scikit-Learn, Keras, and
TensorFlow
THIRD EDITION
Concepts, Tools, and Techniques to Build Intelligent
Systems
Aurélien Géron


Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow
by Aurélien Géron
Copyright © 2023 Aurélien Géron. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North,
Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales
promotional use. Online editions are also available for most titles
(https://oreilly.com). For more information, contact our
corporate/institutional sales department: 800-998-9938 or
corporate@oreilly.com.
Acquisitions Editor: Nicole Butterfield
Development Editors: Nicole Taché and
Michele Cronin
Production Editor: Beth Kelly
Copyeditor: Kim Cofer
Proofreader: Rachel Head
Indexer: Potomac Indexing, LLC
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
March 2017: First Edition


September 2019: Second Edition
October 2022: Third Edition
Revision History for the Third Edition
2022-10-03: First Release
See https://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release
details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands
On Machine Learning with Scikit-Learn, Keras, and TensorFlow, the cover
image, and related trade dress are trademarks of O’Reilly Media, Inc.
The views expressed in this work are those of the author, and do not
represent the publisher’s views. While the publisher and the author have
used good faith efforts to ensure that the information and instructions
contained in this work are accurate, the publisher and the author disclaim all
responsibility for errors or omissions, including without limitation
responsibility for damages resulting from the use of or reliance on this
work. Use of the information and instructions contained in this work is at
your own risk. If any code samples or other technology this work contains
or describes is subject to open source licenses or the intellectual property
rights of others, it is your responsibility to ensure that your use thereof
complies with such licenses and/or rights.
978-1-098-12597-4
[LSI]


Preface
The Machine Learning Tsunami
In 2006, Geoffrey Hinton et al. published a paper showing how to train a
deep neural network capable of recognizing handwritten digits with state
of-the-art precision (>98%). They branded this technique “deep learning”.
A deep neural network is a (very) simplified model of our cerebral cortex,
composed of a stack of layers of artificial neurons. Training a deep neural
net was widely considered impossible at the time, and most researchers
had abandoned the idea in the late 1990s. This paper revived the interest of
the scientific community, and before long many new papers demonstrated
that deep learning was not only possible, but capable of mind-blowing
achievements that no other machine learning (ML) technique could hope to
match (with the help of tremendous computing power and great amounts of
data). This enthusiasm soon extended to many other areas of machine
learning.
A decade later, machine learning had conquered the industry, and today it is
at the heart of much of the magic in high-tech products, ranking your web
search results, powering your smartphone’s speech recognition,
recommending videos, and perhaps even driving your car.
Machine Learning in Your Projects
So, naturally you are excited about machine learning and would love to join
the party!
Perhaps you would like to give your homemade robot a brain of its own?
Make it recognize faces? Or learn to walk around?
Or maybe your company has tons of data (user logs, financial data,
production data, machine sensor data, hotline stats, HR reports, etc.), and
1
2


more than likely you could unearth some hidden gems if you just knew
where to look. With machine learning, you could accomplish the following
and much more:
Segment customers and find the best marketing strategy for each
group.
Recommend products for each client based on what similar clients
bought.
Detect which transactions are likely to be fraudulent.
Forecast next year’s revenue.
Whatever the reason, you have decided to learn machine learning and
implement it in your projects. Great idea!
Objective and Approach
This book assumes that you know close to nothing about machine learning.
Its goal is to give you the concepts, tools, and intuition you need to
implement programs capable of learning from data.
We will cover a large number of techniques, from the simplest and most
commonly used (such as linear regression) to some of the deep learning
techniques that regularly win competitions. For this, we will be using
production-ready Python frameworks:
Scikit-Learn is very easy to use, yet it implements many machine
learning algorithms efficiently, so it makes for a great entry point to
learning machine learning. It was created by David Cournapeau in
2007, and is now led by a team of researchers at the French Institute
for Research in Computer Science and Automation (Inria).
TensorFlow is a more complex library for distributed numerical
computation. It makes it possible to train and run very large neural
networks efficiently by distributing the computations across potentially


hundreds of multi-GPU (graphics processing unit) servers. TensorFlow
(TF) was created at Google and supports many of its large-scale
machine learning applications. It was open sourced in November 2015,
and version 2.0 was released in September 2019.
Keras is a high-level deep learning API that makes it very simple to
train and run neural networks. Keras comes bundled with TensorFlow,
and it relies on TensorFlow for all the intensive computations.
The book favors a hands-on approach, growing an intuitive understanding
of machine learning through concrete working examples and just a little bit
of theory.
TIP
While you can read this book without picking up your laptop, I highly recommend you experiment with the code examples.
Code Examples
All the code examples in this book are open source and available online at
https://github.com/ageron/handson-ml3, as Jupyter notebooks. These are
interactive documents containing text, images, and executable code snippets
(Python in our case). The easiest and quickest way to get started is to run
these notebooks using Google Colab: this is a free service that allows you to
run any Jupyter notebook directly online, without having to install anything
on your machine. All you need is a web browser and a Google account.
NOTE
In this book, I will assume that you are using Google Colab, but I have also tested the notebooks on other online platforms such as Kaggle and Binder, so you can use those if you prefer. Alternatively, you can install the required libraries and tools (or the Docker image for this book) and run the notebooks directly on your own machine. See the instructions at https://homl.info/install.


This book is here to help you get your job done. If you wish to use
additional content beyond the code examples, and that use falls outside the
scope of fair use guidelines, (such as selling or distributing content from
O’Reilly books, or incorporating a significant amount of material from this
book into your product’s documentation), please reach out to us for
permission, at permissions@oreilly.com.
We appreciate, but do not require, attribution. An attribution usually
includes the title, author, publisher, and ISBN. For example: “Hands-On
Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien
Géron. Copyright 2023 Aurélien Géron, 978-1-098-12597-4.”
Prerequisites
This book assumes that you have some Python programming experience. If
you don’t know Python yet, https://learnpython.org is a great place to start.
The official tutorial on Python.org is also quite good.
This book also assumes that you are familiar with Python’s main scientific
libraries—in particular, NumPy, Pandas, and Matplotlib. If you have never
used these libraries, don’t worry; they’re easy to learn, and I’ve created a
tutorial for each of them. You can access them online at
https://homl.info/tutorials.
Moreover, if you want to fully understand how the machine learning
algorithms work (not just how to use them), then you should have at least a
basic understanding of a few math concepts, especially linear algebra.
Specifically, you should know what vectors and matrices are, and how to
perform some simple operations like adding vectors, or transposing and
multiplying matrices. If you need a quick introduction to linear algebra (it’s
really not rocket science!), I provide a tutorial at https://homl.info/tutorials.
You will also find a tutorial on differential calculus, which may be helpful
to understand how neural networks are trained, but it’s not entirely essential
to grasp the important concepts. This book also uses other mathematical
concepts occasionally, such as exponentials and logarithms, a bit of
probability theory, and some basic statistics concepts, but nothing too


advanced. If you need help on any of these, please check out
https://khanacademy.org, which offers many excellent and free math
courses online.
Roadmap
This book is organized in two parts. Part I, “The Fundamentals of Machine
Learning”, covers the following topics:
What machine learning is, what problems it tries to solve, and the main
categories and fundamental concepts of its systems
The steps in a typical machine learning project
Learning by fitting a model to data
Optimizing a cost function
Handling, cleaning, and preparing data
Selecting and engineering features
Selecting a model and tuning hyperparameters using cross-validation
The challenges of machine learning, in particular underfitting and
overfitting (the bias/variance trade-off)
The most common learning algorithms: linear and polynomial
regression, logistic regression, k-nearest neighbors, support vector
machines, decision trees, random forests, and ensemble methods
Reducing the dimensionality of the training data to fight the “curse of
dimensionality”
Other unsupervised learning techniques, including clustering, density
estimation, and anomaly detection
Part II, “Neural Networks and Deep Learning”, covers the following topics:
What neural nets are and what they’re good for


Building and training neural nets using TensorFlow and Keras
The most important neural net architectures: feedforward neural nets
for tabular data, convolutional nets for computer vision, recurrent nets
and long short-term memory (LSTM) nets for sequence processing,
encoder–decoders and transformers for natural language processing
(and more!), autoencoders, generative adversarial networks (GANs),
and diffusion models for generative learning
Techniques for training deep neural nets
How to build an agent (e.g., a bot in a game) that can learn good
strategies through trial and error, using reinforcement learning
Loading and preprocessing large amounts of data efficiently
Training and deploying TensorFlow models at scale
The first part is based mostly on Scikit-Learn, while the second part uses
TensorFlow and Keras.
CAUTION
Don’t jump into deep waters too hastily: while deep learning is no doubt one of the most exciting areas in machine learning, you should master the fundamentals first. Moreover, most problems can be solved quite well using simpler techniques such as random forests and ensemble methods (discussed in Part I). deep learning is best suited for complex problems such as image recognition, speech recognition, or natural language processing, and it requires a lot of data, computing power, and patience (unless you can leverage a pretrained neural network, as you will see).
Changes Between the First and the Second
Edition
If you have already read the first edition, here are the main changes
between the first and the second edition:


All the code was migrated from TensorFlow 1.x to TensorFlow 2.x,
and I replaced most of the low-level TensorFlow code (graphs,
sessions, feature columns, estimators, and so on) with much simpler
Keras code.
The second edition introduced the Data API for loading and
preprocessing large datasets, the distribution strategies API to train and
deploy TF models at scale, TF Serving and Google Cloud AI Platform
to productionize models, and (briefly) TF Transform, TFLite, TF
Addons/Seq2Seq, TensorFlow.js, and TF Agents.
It also introduced many additional ML topics, including a new chapter
on unsupervised learning, computer vision techniques for object
detection and semantic segmentation, handling sequences using
convolutional neural networks (CNNs), natural language processing
(NLP) using recurrent neural networks (RNNs), CNNs and
transformers, GANs, and more.
See https://homl.info/changes2 for more details.
Changes Between the Second and the Third
Edition
If you read the second edition, here are the main changes between the
second and the third edition:
All the code was updated to the latest library versions. In particular,
this third edition introduces many new additions to Scikit-Learn (e.g.,
feature name tracking, histogram-based gradient boosting, label
propagation, and more). It also introduces the Keras Tuner library for
hyperparameter tuning, Hugging Face’s Transformers library for
natural language processing, and Keras’s new preprocessing and data
augmentation layers.
Several vision models were added (ResNeXt, DenseNet, MobileNet,
CSPNet, and EfficientNet), as well as guidelines for choosing the right


one.
Chapter 15 now analyzes the Chicago bus and rail ridership data
instead of generated time series, and it introduces the ARMA model
and its variants.
Chapter 16 on natural language processing now builds an English-to
Spanish translation model, first using an encoder–decoder RNN, then
using a transformer model. The chapter also covers language models
such as Switch Transformers, DistilBERT, T5, and PaLM (with chain
of-thought prompting). In addition, it introduces vision transformers
(ViTs) and gives an overview of a few transformer-based visual
models, such as data-efficient image transformers (DeiTs), Perceiver,
and DINO, as well as a brief overview of some large multimodal
models, including CLIP, DALL·E, Flamingo, and GATO.
Chapter 17 on generative learning now introduces diffusion models,
and shows how to implement a denoising diffusion probabilistic model
(DDPM) from scratch.
Chapter 19 migrated from Google Cloud AI Platform to Google Vertex
AI, and uses distributed Keras Tuner for large-scale hyperparameter
search. The chapter now includes TensorFlow.js code that you can
experiment with online. It also introduces additional distributed
training techniques, including PipeDream and Pathways.
In order to allow for all the new content, some sections were moved
online, including installation instructions, kernel principal component
analysis (PCA), mathematical details of Bayesian Gaussian mixtures,
TF Agents, and former appendices A (exercise solutions), C (support
vector machine math), and E (extra neural net architectures).
See https://homl.info/changes3 for more details.
Other Resources


Many excellent resources are available to learn about machine learning. For
example, Andrew Ng’s ML course on Coursera is amazing, although it
requires a significant time investment.
There are also many interesting websites about machine learning, including
Scikit-Learn’s exceptional User Guide. You may also enjoy Dataquest,
which provides very nice interactive tutorials, and ML blogs such as those
listed on Quora.
There are many other introductory books about machine learning. In
particular:
Joel Grus’s Data Science from Scratch, 2nd edition (O’Reilly),
presents the fundamentals of machine learning and implements some
of the main algorithms in pure Python (from scratch, as the name
suggests).
Stephen Marsland’s Machine Learning: An Algorithmic Perspective,
2nd edition (Chapman & Hall), is a great introduction to machine
learning, covering a wide range of topics in depth with code examples
in Python (also from scratch, but using NumPy).
Sebastian Raschka’s Python Machine Learning, 3rd edition (Packt
Publishing), is also a great introduction to machine learning and
leverages Python open source libraries (Pylearn 2 and Theano).
François Chollet’s Deep Learning with Python, 2nd edition (Manning),
is a very practical book that covers a large range of topics in a clear
and concise way, as you might expect from the author of the excellent
Keras library. It favors code examples over mathematical theory.
Andriy Burkov’s The Hundred-Page Machine Learning Book (self
published) is very short but covers an impressive range of topics,
introducing them in approachable terms without shying away from the
math equations.
Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin’s
Learning from Data (AMLBook) is a rather theoretical approach to


ML that provides deep insights, in particular on the bias/variance
trade-off (see Chapter 4).
Stuart Russell and Peter Norvig’s Artificial Intelligence: A Modern
Approach, 4th edition (Pearson), is a great (and huge) book covering
an incredible amount of topics, including machine learning. It helps
put ML into perspective.
Jeremy Howard and Sylvain Gugger’s Deep Learning for Coders with
fastai and PyTorch (O’Reilly) provides a wonderfully clear and
practical introduction to deep learning using the fastai and PyTorch
libraries.
Finally, joining ML competition websites such as Kaggle.com will allow
you to practice your skills on real-world problems, with help and insights
from some of the best ML professionals out there.
Conventions Used in This Book
The following typographical conventions are used in this book:
Italic
Indicates new terms, URLs, email addresses, filenames, and file
extensions.
Constant width
Used for program listings, as well as within paragraphs to refer to
program elements such as variable or function names, databases, data
types, environment variables, statements, and keywords.
Constant width bold
Shows commands or other text that should be typed literally by the user.
Constant width italic


Shows text that should be replaced with user-supplied values or by
values determined by context.
Punctuation
To avoid any confusion, punctutation appears outside of quotes
throughout the book. My apologies to the purists.
TIP
This element signifies a tip or suggestion.
NOTE
This element signifies a general note.
WARNING
This element indicates a warning or caution.
O’Reilly Online Learning
NOTE
For more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help companies succeed.
Our unique network of experts and innovators share their knowledge and
expertise through books, articles, and our online learning platform.
O’Reilly’s online learning platform gives you on-demand access to live
training courses, in-depth learning paths, interactive coding environments,


and a vast collection of text and video from O’Reilly and 200+ other
publishers. For more information, visit https://oreilly.com.
How to Contact Us
Please address comments and questions concerning this book to the
publisher:
O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-998-9938 (in the United States or Canada)
707-829-0515 (international or local)
707-829-0104 (fax)
We have a web page for this book, where we list errata, examples, and any
additional information. You can access this page at
https://homl.info/oreilly3.
Email bookquestions@oreilly.com to comment or ask technical questions
about this book.
For news and information about our books and courses, visit
https://oreilly.com.
Find us on LinkedIn: https://linkedin.com/company/oreilly-media
Follow us on Twitter: https://twitter.com/oreillymedia
Watch us on YouTube: https://youtube.com/oreillymedia
Acknowledgments


Never in my wildest dreams did I imagine that the first and second editions
of this book would get such a large audience. I received so many messages
from readers, many asking questions, some kindly pointing out errata, and
most sending me encouraging words. I cannot express how grateful I am to
all these readers for their tremendous support. Thank you all so very much!
Please do not hesitate to file issues on GitHub if you find errors in the code
examples (or just to ask questions), or to submit errata if you find errors in
the text. Some readers also shared how this book helped them get their first
job, or how it helped them solve a concrete problem they were working on.
I find such feedback incredibly motivating. If you find this book helpful, I
would love it if you could share your story with me, either privately (e.g.,
via LinkedIn) or publicly (e.g., tweet me at @aureliengeron or write an
Amazon review).
Huge thanks as well to all the wonderful people who offered their time and
expertise to review this third edition, correcting errors and making countless
suggestions. This edition is so much better thanks to them: Olzhas
Akpambetov, George Bonner, François Chollet, Siddha Gangju, Sam
Goodman, Matt Harrison, Sasha Sobran, Lewis Tunstall, Leandro von
Werra, and my dear brother Sylvain. You are all amazing!
I am also very grateful to the many people who supported me along the
way, by answering my questions, suggesting improvements, and
contributing to the code on GitHub: in particular, Yannick Assogba, Ian
Beauregard, Ulf Bissbort, Rick Chao, Peretz Cohen, Kyle Gallatin, Hannes
Hapke, Victor Khaustov, Soonson Kwon, Eric Lebigot, Jason Mayes,
Laurence Moroney, Sara Robinson, Joaquín Ruales, and Yuefeng Zhou.
This book wouldn’t exist without O’Reilly’s fantastic staff, in particular
Nicole Taché, who gave me insightful feedback and was always cheerful,
encouraging, and helpful: I could not dream of a better editor. Big thanks to
Michele Cronin as well, who cheered me on through the final chapters and
managed to get me past the finish line. Thanks to the whole production
team, in particular Elizabeth Kelly and Kristen Brown. Thanks as well to
Kim Cofer for the thorough copyediting, and to Johnny O’Toole, who
managed the relationship with Amazon and answered many of my


questions. Thanks to Kate Dullea for greatly improving my illustrations.
Thanks to Marie Beaugureau, Ben Lorica, Mike Loukides, and Laurel
Ruma for believing in this project and helping me define its scope. Thanks
to Matt Hacker and all of the Atlas team for answering all my technical
questions regarding formatting, AsciiDoc, MathML, and LaTeX, and thanks
to Nick Adams, Rebecca Demarest, Rachel Head, Judith McConville, Helen
Monroe, Karen Montgomery, Rachel Roumeliotis, and everyone else at
O’Reilly who contributed to this book.
I’ll never forget all the wonderful people who helped me with the first and
second editions of this book: friends, colleagues, experts, including many
members of the TensorFlow team. The list is long: Olzhas Akpambetov,
Karmel Allison, Martin Andrews, David Andrzejewski, Paige Bailey, Lukas
Biewald, Eugene Brevdo, William Chargin, François Chollet, Clément
Courbet, Robert Crowe, Mark Daoust, Daniel “Wolff” Dobson, Julien
Dubois, Mathias Kende, Daniel Kitachewsky, Nick Felt, Bruce Fontaine,
Justin Francis, Goldie Gadde, Irene Giannoumis, Ingrid von Glehn, Vincent
Guilbeau, Sandeep Gupta, Priya Gupta, Kevin Haas, Eddy Hung,
Konstantinos Katsiapis, Viacheslav Kovalevskyi, Jon Krohn, Allen Lavoie,
Karim Matrah, Grégoire Mesnil, Clemens Mewald, Dan Moldovan,
Dominic Monn, Sean Morgan, Tom O’Malley, James Pack, Alexander Pak,
Haesun Park, Alexandre Passos, Ankur Patel, Josh Patterson, André Susano
Pinto, Anthony Platanios, Anosh Raj, Oscar Ramirez, Anna Revinskaya,
Saurabh Saxena, Salim Sémaoune, Ryan Sepassi, Vitor Sessak, Jiri Simsa,
Iain Smears, Xiaodan Song, Christina Sorokin, Michel Tessier, Wiktor
Tomczak, Dustin Tran, Todd Wang, Pete Warden, Rich Washington, Martin
Wicke, Edd Wilder-James, Sam Witteveen, Jason Zaman, Yuefeng Zhou,
and my brother Sylvain.
Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle,
and to our three wonderful children, Alexandre, Rémi, and Gabrielle, for
encouraging me to work hard on this book. Their insatiable curiosity was
priceless: explaining some of the most difficult concepts in this book to my
wife and children helped me clarify my thoughts and directly improved


many parts of it. Plus, they keep bringing me cookies and coffee, who could
ask for more?
1 Geoffrey E. Hinton et al., “A Fast Learning Algorithm for Deep Belief Nets”, Neural Computation 18 (2006): 1527–1554.
2 Despite the fact that Yann LeCun’s deep convolutional neural networks had worked well for image recognition since the 1990s, although they were not as general-purpose.


Part I. The Fundamentals of
Machine Learning


Chapter 1. The Machine
Learning Landscape
Not so long ago, if you had picked up your phone and asked it the way
home, it would have ignored you—and people would have questioned your
sanity. But machine learning is no longer science fiction: billions of people
use it every day. And the truth is it has actually been around for decades in
some specialized applications, such as optical character recognition (OCR).
The first ML application that really became mainstream, improving the
lives of hundreds of millions of people, took over the world back in the
1990s: the spam filter. It’s not exactly a self-aware robot, but it does
technically qualify as machine learning: it has actually learned so well that
you seldom need to flag an email as spam anymore. It was followed by
hundreds of ML applications that now quietly power hundreds of products
and features that you use regularly: voice prompts, automatic translation,
image search, product recommendations, and many more.
Where does machine learning start and where does it end? What exactly
does it mean for a machine to learn something? If I download a copy of all
Wikipedia articles, has my computer really learned something? Is it
suddenly smarter? In this chapter I will start by clarifying what machine
learning is and why you may want to use it.
Then, before we set out to explore the machine learning continent, we will
take a look at the map and learn about the main regions and the most
notable landmarks: supervised versus unsupervised learning and their
variants, online versus batch learning, instance-based versus model-based
learning. Then we will look at the workflow of a typical ML project,
discuss the main challenges you may face, and cover how to evaluate and
fine-tune a machine learning system.
This chapter introduces a lot of fundamental concepts (and jargon) that
every data scientist should know by heart. It will be a high-level overview


(it’s the only chapter without much code), all rather simple, but my goal is
to ensure everything is crystal clear to you before we continue on to the rest
of the book. So grab a coffee and let’s get started!
TIP
If you are already familiar with machine learning basics, you may want to skip directly to Chapter 2. If you are not sure, try to answer all the questions listed at the end of the chapter before moving on.
What Is Machine Learning?
Machine learning is the science (and art) of programming computers so
they can learn from data.
Here is a slightly more general definition:
[Machine learning is the] field of study that gives computers the ability to learn without being explicitly programmed.
—Arthur Samuel, 1959
And a more engineering-oriented one:
A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
—Tom Mitchell, 1997
Your spam filter is a machine learning program that, given examples of
spam emails (flagged by users) and examples of regular emails (nonspam,
also called “ham”), can learn to flag spam. The examples that the system
uses to learn are called the training set. Each training example is called a
training instance (or sample). The part of a machine learning system that
learns and makes predictions is called a model. Neural networks and
random forests are examples of models.


In this case, the task T is to flag spam for new emails, the experience E is
the training data, and the performance measure P needs to be defined; for
example, you can use the ratio of correctly classified emails. This particular
performance measure is called accuracy, and it is often used in
classification tasks.
If you just download a copy of all Wikipedia articles, your computer has a
lot more data, but it is not suddenly better at any task. This is not machine
learning.
Why Use Machine Learning?
Consider how you would write a spam filter using traditional programming
techniques (Figure 1-1):
1. First you would examine what spam typically looks like. You might
notice that some words or phrases (such as “4U”, “credit card”, “free”,
and “amazing”) tend to come up a lot in the subject line. Perhaps you
would also notice a few other patterns in the sender’s name, the
email’s body, and other parts of the email.
2. You would write a detection algorithm for each of the patterns that you
noticed, and your program would flag emails as spam if a number of
these patterns were detected.
3. You would test your program and repeat steps 1 and 2 until it was good
enough to launch.


Figure 1-1. The traditional approach
Since the problem is difficult, your program will likely become a long list
of complex rules—pretty hard to maintain.
In contrast, a spam filter based on machine learning techniques
automatically learns which words and phrases are good predictors of spam
by detecting unusually frequent patterns of words in the spam examples
compared to the ham examples (Figure 1-2). The program is much shorter,
easier to maintain, and most likely more accurate.


Figure 1-2. The machine learning approach
What if spammers notice that all their emails containing “4U” are blocked?
They might start writing “For U” instead. A spam filter using traditional
programming techniques would need to be updated to flag “For U” emails.
If spammers keep working around your spam filter, you will need to keep
writing new rules forever.
In contrast, a spam filter based on machine learning techniques
automatically notices that “For U” has become unusually frequent in spam
flagged by users, and it starts flagging them without your intervention
(Figure 1-3).


Figure 1-3. Automatically adapting to change
Another area where machine learning shines is for problems that either are
too complex for traditional approaches or have no known algorithm. For
example, consider speech recognition. Say you want to start simple and
write a program capable of distinguishing the words “one” and “two”. You
might notice that the word “two” starts with a high-pitch sound (“T”), so
you could hardcode an algorithm that measures high-pitch sound intensity
and use that to distinguish ones and twos—but obviously this technique will
not scale to thousands of words spoken by millions of very different people
in noisy environments and in dozens of languages. The best solution (at
least today) is to write an algorithm that learns by itself, given many
example recordings for each word.
Finally, machine learning can help humans learn (Figure 1-4). ML models
can be inspected to see what they have learned (although for some models
this can be tricky). For instance, once a spam filter has been trained on
enough spam, it can easily be inspected to reveal the list of words and
combinations of words that it believes are the best predictors of spam.
Sometimes this will reveal unsuspected correlations or new trends, and
thereby lead to a better understanding of the problem. Digging into large
amounts of data to discover hidden patterns is called data mining, and
machine learning excels at it.


Figure 1-4. Machine learning can help humans learn
To summarize, machine learning is great for:
Problems for which existing solutions require a lot of fine-tuning or
long lists of rules (a machine learning model can often simplify code
and perform better than the traditional approach)
Complex problems for which using a traditional approach yields no
good solution (the best machine learning techniques can perhaps find a
solution)
Fluctuating environments (a machine learning system can easily be
retrained on new data, always keeping it up to date)
Getting insights about complex problems and large amounts of data
Examples of Applications


Let’s look at some concrete examples of machine learning tasks, along with
the techniques that can tackle them:
Analyzing images of products on a production line to automatically classify
them
This is image classification, typically performed using convolutional
neural networks (CNNs; see Chapter 14) or sometimes transformers
(see Chapter 16).
Detecting tumors in brain scans
This is semantic image segmentation, where each pixel in the image is
classified (as we want to determine the exact location and shape of
tumors), typically using CNNs or transformers.
Automatically classifying news articles
This is natural language processing (NLP), and more specifically text
classification, which can be tackled using recurrent neural networks
(RNNs) and CNNs, but transformers work even better (see Chapter 16).
Automatically flagging offensive comments on discussion forums
This is also text classification, using the same NLP tools.
Summarizing long documents automatically
This is a branch of NLP called text summarization, again using the same
tools.
Creating a chatbot or a personal assistant
This involves many NLP components, including natural language
understanding (NLU) and question-answering modules.
Forecasting your company’s revenue next year, based on many performance
metrics


This is a regression task (i.e., predicting values) that may be tackled
using any regression model, such as a linear regression or polynomial
regression model (see Chapter 4), a regression support vector machine
(see Chapter 5), a regression random forest (see Chapter 7), or an
artificial neural network (see Chapter 10). If you want to take into
account sequences of past performance metrics, you may want to use
RNNs, CNNs, or transformers (see Chapters 15 and 16).
Making your app react to voice commands
This is speech recognition, which requires processing audio samples:
since they are long and complex sequences, they are typically processed
using RNNs, CNNs, or transformers (see Chapters 15 and 16).
Detecting credit card fraud
This is anomaly detection, which can be tackled using isolation forests,
Gaussian mixture models (see Chapter 9), or autoencoders (see
Chapter 17).
Segmenting clients based on their purchases so that you can design a
different marketing strategy for each segment
This is clustering, which can be achieved using k-means, DBSCAN, and
more (see Chapter 9).
Representing a complex, high-dimensional dataset in a clear and insightful
diagram
This is data visualization, often involving dimensionality reduction
techniques (see Chapter 8).
Recommending a product that a client may be interested in, based on past
purchases
This is a recommender system. One approach is to feed past purchases
(and other information about the client) to an artificial neural network
(see Chapter 10), and get it to output the most likely next purchase. This


neural net would typically be trained on past sequences of purchases
across all clients.
Building an intelligent bot for a game
This is often tackled using reinforcement learning (RL; see Chapter 18),
which is a branch of machine learning that trains agents (such as bots)
to pick the actions that will maximize their rewards over time (e.g., a
bot may get a reward every time the player loses some life points),
within a given environment (such as the game). The famous AlphaGo
program that beat the world champion at the game of Go was built using
RL.
This list could go on and on, but hopefully it gives you a sense of the
incredible breadth and complexity of the tasks that machine learning can
tackle, and the types of techniques that you would use for each task.
Types of Machine Learning Systems
There are so many different types of machine learning systems that it is
useful to classify them in broad categories, based on the following criteria:
How they are supervised during training (supervised, unsupervised,
semi-supervised, self-supervised, and others)
Whether or not they can learn incrementally on the fly (online versus
batch learning)
Whether they work by simply comparing new data points to known
data points, or instead by detecting patterns in the training data and
building a predictive model, much like scientists do (instance-based
versus model-based learning)
These criteria are not exclusive; you can combine them in any way you like.
For example, a state-of-the-art spam filter may learn on the fly using a deep


neural network model trained using human-provided examples of spam and
ham; this makes it an online, model-based, supervised learning system.
Let’s look at each of these criteria a bit more closely.
Training Supervision
ML systems can be classified according to the amount and type of
supervision they get during training. There are many categories, but we’ll
discuss the main ones: supervised learning, unsupervised learning, self
supervised learning, semi-supervised learning, and reinforcement learning.
Supervised learning
In supervised learning, the training set you feed to the algorithm includes
the desired solutions, called labels (Figure 1-5).
Figure 1-5. A labeled training set for spam classification (an example of supervised learning)
A typical supervised learning task is classification. The spam filter is a
good example of this: it is trained with many example emails along with
their class (spam or ham), and it must learn how to classify new emails.
Another typical task is to predict a target numeric value, such as the price
of a car, given a set of features (mileage, age, brand, etc.). This sort of task
is called regression (Figure 1-6). To train the system, you need to give it
1


many examples of cars, including both their features and their targets (i.e.,
their prices).
Note that some regression models can be used for classification as well, and
vice versa. For example, logistic regression is commonly used for
classification, as it can output a value that corresponds to the probability of
belonging to a given class (e.g., 20% chance of being spam).
Figure 1-6. A regression problem: predict a value, given an input feature (there are usually multiple input features, and sometimes multiple output values)
NOTE
The words target and label are generally treated as synonyms in supervised learning, but target is more common in regression tasks and label is more common in classification tasks. Moreover, features are sometimes called predictors or attributes. These terms may refer to individual samples (e.g., “this car’s mileage feature is equal to 15,000”) or to all samples (e.g., “the mileage feature is strongly correlated with price”).
Unsupervised learning
In unsupervised learning, as you might guess, the training data is unlabeled
(Figure 1-7). The system tries to learn without a teacher.


For example, say you have a lot of data about your blog’s visitors. You may
want to run a clustering algorithm to try to detect groups of similar visitors
(Figure 1-8). At no point do you tell the algorithm which group a visitor
belongs to: it finds those connections without your help. For example, it
might notice that 40% of your visitors are teenagers who love comic books
and generally read your blog after school, while 20% are adults who enjoy
sci-fi and who visit during the weekends. If you use a hierarchical
clustering algorithm, it may also subdivide each group into smaller groups.
This may help you target your posts for each group.
Figure 1-7. An unlabeled training set for unsupervised learning


Figure 1-8. Clustering
Visualization algorithms are also good examples of unsupervised learning:
you feed them a lot of complex and unlabeled data, and they output a 2D or
3D representation of your data that can easily be plotted (Figure 1-9). These
algorithms try to preserve as much structure as they can (e.g., trying to keep
separate clusters in the input space from overlapping in the visualization) so
that you can understand how the data is organized and perhaps identify
unsuspected patterns.
A related task is dimensionality reduction, in which the goal is to simplify
the data without losing too much information. One way to do this is to
merge several correlated features into one. For example, a car’s mileage
may be strongly correlated with its age, so the dimensionality reduction
algorithm will merge them into one feature that represents the car’s wear
and tear. This is called feature extraction.


Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters
TIP
It is often a good idea to try to reduce the number of dimensions in your training data using a dimensionality reduction algorithm before you feed it to another machine learning algorithm (such as a supervised learning algorithm). It will run much faster, the data will take up less disk and memory space, and in some cases it may also perform better.
Yet another important unsupervised task is anomaly detection—for
example, detecting unusual credit card transactions to prevent fraud,
catching manufacturing defects, or automatically removing outliers from a
dataset before feeding it to another learning algorithm. The system is shown
mostly normal instances during training, so it learns to recognize them;
then, when it sees a new instance, it can tell whether it looks like a normal
one or whether it is likely an anomaly (see Figure 1-10). A very similar task
is novelty detection: it aims to detect new instances that look different from
all instances in the training set. This requires having a very “clean” training
2


set, devoid of any instance that you would like the algorithm to detect. For
example, if you have thousands of pictures of dogs, and 1% of these
pictures represent Chihuahuas, then a novelty detection algorithm should
not treat new pictures of Chihuahuas as novelties. On the other hand,
anomaly detection algorithms may consider these dogs as so rare and so
different from other dogs that they would likely classify them as anomalies
(no offense to Chihuahuas).
Figure 1-10. Anomaly detection
Finally, another common unsupervised task is association rule learning, in
which the goal is to dig into large amounts of data and discover interesting
relations between attributes. For example, suppose you own a supermarket.
Running an association rule on your sales logs may reveal that people who
purchase barbecue sauce and potato chips also tend to buy steak. Thus, you
may want to place these items close to one another.
Semi-supervised learning
Since labeling data is usually time-consuming and costly, you will often
have plenty of unlabeled instances, and few labeled instances. Some
algorithms can deal with data that’s partially labeled. This is called semi
supervised learning (Figure 1-11).


Figure 1-11. Semi-supervised learning with two classes (triangles and squares): the unlabeled examples (circles) help classify a new instance (the cross) into the triangle class rather than the square class, even though it is closer to the labeled squares
Some photo-hosting services, such as Google Photos, are good examples of
this. Once you upload all your family photos to the service, it automatically
recognizes that the same person A shows up in photos 1, 5, and 11, while
another person B shows up in photos 2, 5, and 7. This is the unsupervised
part of the algorithm (clustering). Now all the system needs is for you to tell
it who these people are. Just add one label per person and it is able to name
everyone in every photo, which is useful for searching photos.
Most semi-supervised learning algorithms are combinations of
unsupervised and supervised algorithms. For example, a clustering
algorithm may be used to group similar instances together, and then every
unlabeled instance can be labeled with the most common label in its cluster.
Once the whole dataset is labeled, it is possible to use any supervised
learning algorithm.
Self-supervised learning
Another approach to machine learning involves actually generating a fully
labeled dataset from a fully unlabeled one. Again, once the whole dataset is
labeled, any supervised learning algorithm can be used. This approach is
called self-supervised learning.
For example, if you have a large dataset of unlabeled images, you can
randomly mask a small part of each image and then train a model to recover
the original image (Figure 1-12). During training, the masked images are
3


used as the inputs to the model, and the original images are used as the
labels.
Figure 1-12. Self-supervised learning example: input (left) and target (right)
The resulting model may be quite useful in itself—for example, to repair
damaged images or to erase unwanted objects from pictures. But more often
than not, a model trained using self-supervised learning is not the final goal.
You’ll usually want to tweak and fine-tune the model for a slightly different
task—one that you actually care about.
For example, suppose that what you really want is to have a pet
classification model: given a picture of any pet, it will tell you what species
it belongs to. If you have a large dataset of unlabeled photos of pets, you
can start by training an image-repairing model using self-supervised
learning. Once it’s performing well, it should be able to distinguish different
pet species: when it repairs an image of a cat whose face is masked, it must
know not to add a dog’s face. Assuming your model’s architecture allows it
(and most neural network architectures do), it is then possible to tweak the
model so that it predicts pet species instead of repairing images. The final
step consists of fine-tuning the model on a labeled dataset: the model
already knows what cats, dogs, and other pet species look like, so this step


is only needed so the model can learn the mapping between the species it
already knows and the labels we expect from it.
NOTE
Transferring knowledge from one task to another is called transfer learning, and it’s one of the most important techniques in machine learning today, especially when using deep neural networks (i.e., neural networks composed of many layers of neurons). We will discuss this in detail in Part II.
Some people consider self-supervised learning to be a part of unsupervised
learning, since it deals with fully unlabeled datasets. But self-supervised
learning uses (generated) labels during training, so in that regard it’s closer
to supervised learning. And the term “unsupervised learning” is generally
used when dealing with tasks like clustering, dimensionality reduction, or
anomaly detection, whereas self-supervised learning focuses on the same
tasks as supervised learning: mainly classification and regression. In short,
it’s best to treat self-supervised learning as its own category.
Reinforcement learning
Reinforcement learning is a very different beast. The learning system,
called an agent in this context, can observe the environment, select and
perform actions, and get rewards in return (or penalties in the form of
negative rewards, as shown in Figure 1-13). It must then learn by itself what
is the best strategy, called a policy, to get the most reward over time. A
policy defines what action the agent should choose when it is in a given
situation.


Figure 1-13. Reinforcement learning
For example, many robots implement reinforcement learning algorithms to
learn how to walk. DeepMind’s AlphaGo program is also a good example
of reinforcement learning: it made the headlines in May 2017 when it beat
Ke Jie, the number one ranked player in the world at the time, at the game
of Go. It learned its winning policy by analyzing millions of games, and
then playing many games against itself. Note that learning was turned off
during the games against the champion; AlphaGo was just applying the
policy it had learned. As you will see in the next section, this is called
offline learning.
Batch Versus Online Learning


Another criterion used to classify machine learning systems is whether or
not the system can learn incrementally from a stream of incoming data.
Batch learning
In batch learning, the system is incapable of learning incrementally: it must
be trained using all the available data. This will generally take a lot of time
and computing resources, so it is typically done offline. First the system is
trained, and then it is launched into production and runs without learning
anymore; it just applies what it has learned. This is called offline learning.
Unfortunately, a model’s performance tends to decay slowly over time,
simply because the world continues to evolve while the model remains
unchanged. This phenomenon is often called model rot or data drift. The
solution is to regularly retrain the model on up-to-date data. How often you
need to do that depends on the use case: if the model classifies pictures of
cats and dogs, its performance will decay very slowly, but if the model
deals with fast-evolving systems, for example making predictions on the
financial market, then it is likely to decay quite fast.
WARNING
Even a model trained to classify pictures of cats and dogs may need to be retrained regularly, not because cats and dogs will mutate overnight, but because cameras keep changing, along with image formats, sharpness, brightness, and size ratios. Moreover, people may love different breeds next year, or they may decide to dress their pets with tiny hats—who knows?
If you want a batch learning system to know about new data (such as a new
type of spam), you need to train a new version of the system from scratch
on the full dataset (not just the new data, but also the old data), then replace
the old model with the new one. Fortunately, the whole process of training,
evaluating, and launching a machine learning system can be automated
fairly easily (as we saw in Figure 1-3), so even a batch learning system can
adapt to change. Simply update the data and train a new version of the
system from scratch as often as needed.


This solution is simple and often works fine, but training using the full set
of data can take many hours, so you would typically train a new system
only every 24 hours or even just weekly. If your system needs to adapt to
rapidly changing data (e.g., to predict stock prices), then you need a more
reactive solution.
Also, training on the full set of data requires a lot of computing resources
(CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a
lot of data and you automate your system to train from scratch every day, it
will end up costing you a lot of money. If the amount of data is huge, it may
even be impossible to use a batch learning algorithm.
Finally, if your system needs to be able to learn autonomously and it has
limited resources (e.g., a smartphone application or a rover on Mars), then
carrying around large amounts of training data and taking up a lot of
resources to train for hours every day is a showstopper.
A better option in all these cases is to use algorithms that are capable of
learning incrementally.
Online learning
In online learning, you train the system incrementally by feeding it data
instances sequentially, either individually or in small groups called mini
batches. Each learning step is fast and cheap, so the system can learn about
new data on the fly, as it arrives (see Figure 1-14).


Figure 1-14. In online learning, a model is trained and launched into production, and then it keeps learning as new data comes in
Online learning is useful for systems that need to adapt to change extremely
rapidly (e.g., to detect new patterns in the stock market). It is also a good
option if you have limited computing resources; for example, if the model is
trained on a mobile device.
Additionally, online learning algorithms can be used to train models on
huge datasets that cannot fit in one machine’s main memory (this is called
out-of-core learning). The algorithm loads part of the data, runs a training
step on that data, and repeats the process until it has run on all of the data
(see Figure 1-15).


Figure 1-15. Using online learning to handle huge datasets
One important parameter of online learning systems is how fast they should
adapt to changing data: this is called the learning rate. If you set a high
learning rate, then your system will rapidly adapt to new data, but it will
also tend to quickly forget the old data (and you don’t want a spam filter to
flag only the latest kinds of spam it was shown). Conversely, if you set a
low learning rate, the system will have more inertia; that is, it will learn
more slowly, but it will also be less sensitive to noise in the new data or to
sequences of nonrepresentative data points (outliers).
WARNING
Out-of-core learning is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning.
A big challenge with online learning is that if bad data is fed to the system,
the system’s performance will decline, possibly quickly (depending on the


data quality and learning rate). If it’s a live system, your clients will notice.
For example, bad data could come from a bug (e.g., a malfunctioning
sensor on a robot), or it could come from someone trying to game the
system (e.g., spamming a search engine to try to rank high in search
results). To reduce this risk, you need to monitor your system closely and
promptly switch learning off (and possibly revert to a previously working
state) if you detect a drop in performance. You may also want to monitor
the input data and react to abnormal data; for example, using an anomaly
detection algorithm (see Chapter 9).
Instance-Based Versus Model-Based Learning
One more way to categorize machine learning systems is by how they
generalize. Most machine learning tasks are about making predictions. This
means that given a number of training examples, the system needs to be
able to make good predictions for (generalize to) examples it has never seen
before. Having a good performance measure on the training data is good,
but insufficient; the true goal is to perform well on new instances.
There are two main approaches to generalization: instance-based learning
and model-based learning.
Instance-based learning
Possibly the most trivial form of learning is simply to learn by heart. If you
were to create a spam filter this way, it would just flag all emails that are
identical to emails that have already been flagged by users—not the worst
solution, but certainly not the best.
Instead of just flagging emails that are identical to known spam emails,
your spam filter could be programmed to also flag emails that are very
similar to known spam emails. This requires a measure of similarity
between two emails. A (very basic) similarity measure between two emails
could be to count the number of words they have in common. The system
would flag an email as spam if it has many words in common with a known
spam email.


This is called instance-based learning: the system learns the examples by
heart, then generalizes to new cases by using a similarity measure to
compare them to the learned examples (or a subset of them). For example,
in Figure 1-16 the new instance would be classified as a triangle because
the majority of the most similar instances belong to that class.
Figure 1-16. Instance-based learning
Model-based learning and a typical machine learning workflow
Another way to generalize from a set of examples is to build a model of
these examples and then use that model to make predictions. This is called
model-based learning (Figure 1-17).


Figure 1-17. Model-based learning
For example, suppose you want to know if money makes people happy, so
you download the Better Life Index data from the OECD’s website and
World Bank stats about gross domestic product (GDP) per capita. Then you
join the tables and sort by GDP per capita. Table 1-1 shows an excerpt of
what you get.
Table 1-1. Does money make people happier?
Country GDP per capita (USD) Life satisfaction
Turkey 28,384 5.5
Hungary 31,008 5.6
France 42,026 6.5
United States 60,236 6.9
New Zealand 42,404 7.3
Australia 48,698 7.3
Denmark 55,938 7.6
Let’s plot the data for these countries (Figure 1-18).


Figure 1-18. Do you see a trend here?
There does seem to be a trend here! Although the data is noisy (i.e., partly
random), it looks like life satisfaction goes up more or less linearly as the
country’s GDP per capita increases. So you decide to model life satisfaction
as a linear function of GDP per capita. This step is called model selection:
you selected a linear model of life satisfaction with just one attribute, GDP
per capita (Equation 1-1).
Equation 1-1. A simple linear model
life_satisfaction = θ0 + θ1 × GDP_per_capita
This model has two model parameters, θ and θ . By tweaking these
parameters, you can make your model represent any linear function, as
shown in Figure 1-19.
0 14


Figure 1-19. A few possible linear models
Before you can use your model, you need to define the parameter values θ
and θ . How can you know which values will make your model perform
best? To answer this question, you need to specify a performance measure.
You can either define a utility function (or fitness function) that measures
how good your model is, or you can define a cost function that measures
how bad it is. For linear regression problems, people typically use a cost
function that measures the distance between the linear model’s predictions
and the training examples; the objective is to minimize this distance.
This is where the linear regression algorithm comes in: you feed it your
training examples, and it finds the parameters that make the linear model fit
best to your data. This is called training the model. In our case, the
algorithm finds that the optimal parameter values are θ = 3.75 and θ =
6.78 × 10 .
0
1
01
–5


WARNING
Confusingly, the word “model” can refer to a type of model (e.g., linear regression), to a fully specified model architecture (e.g., linear regression with one input and one output), or to the final trained model ready to be used for predictions (e.g., linear regression with one input and one output, using θ = 3.75 and θ = 6.78 × 10 ). Model selection consists in choosing the type of model and fully specifying its architecture. Training a model means running an algorithm to find the model parameters that will make it best fit the training data, and hopefully make good predictions on new data.
Now the model fits the training data as closely as possible (for a linear
model), as you can see in Figure 1-20.
Figure 1-20. The linear model that fits the training data best
You are finally ready to run the model to make predictions. For example,
say you want to know how happy Cypriots are, and the OECD data does not
have the answer. Fortunately, you can use your model to make a good
prediction: you look up Cyprus’s GDP per capita, find $37,655, and then
apply your model and find that life satisfaction is likely to be somewhere
around 3.75 + 37,655 × 6.78 × 10 = 6.30.
0 1 –5
–5


To whet your appetite, Example 1-1 shows the Python code that loads the
data, separates the inputs X from the labels y, creates a scatterplot for
visualization, and then trains a linear model and makes a prediction.
Example 1-1. Training and running a linear model using Scikit-Learn
import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression
# Download and prepare the data
data_root = "https://github.com/ageron/data/raw/main/" lifesat = pd.read_csv(data_root + "lifesat/lifesat.csv") X = lifesat[["GDP per capita (USD)"]].values y = lifesat[["Life satisfaction"]].values
# Visualize the data
lifesat.plot(kind='scatter', grid=True, x="GDP per capita (USD)", y="Life satisfaction") plt.axis([23_500, 62_500, 4, 9]) plt.show()
# Select a linear model
model = LinearRegression()
# Train the model model.fit(X, y)
# Make a prediction for Cyprus X_new = [[37_655.2]] # Cyprus' GDP per capita in 2020 print(model.predict(X_new)) # output: [[6.30165767]]
5


NOTE
If you had used an instance-based learning algorithm instead, you would have found that Israel has the closest GDP per capita to that of Cyprus ($38,341), and since the OECD data tells us that Israelis’ life satisfaction is 7.2, you would have predicted a life satisfaction of 7.2 for Cyprus. If you zoom out a bit and look at the two next-closest countries, you will find Lithuania and Slovenia, both with a life satisfaction of 5.9. Averaging these three values, you get 6.33, which is pretty close to your model-based prediction. This simple algorithm is called k-nearest neighbors regression (in this example, k = 3).
Replacing the linear regression model with k-nearest neighbors regression in the previous code is as easy as replacing these lines:
from sklearn.linear_model import LinearRegression model = LinearRegression()
with these two:
from sklearn.neighbors import KNeighborsRegressor model = KNeighborsRegressor(n_neighbors=3)
If all went well, your model will make good predictions. If not, you may
need to use more attributes (employment rate, health, air pollution, etc.), get
more or better-quality training data, or perhaps select a more powerful
model (e.g., a polynomial regression model).
In summary:
You studied the data.
You selected a model.
You trained it on the training data (i.e., the learning algorithm searched
for the model parameter values that minimize a cost function).
Finally, you applied the model to make predictions on new cases (this
is called inference), hoping that this model will generalize well.


This is what a typical machine learning project looks like. In Chapter 2 you
will experience this firsthand by going through a project end to end.
We have covered a lot of ground so far: you now know what machine
learning is really about, why it is useful, what some of the most common
categories of ML systems are, and what a typical project workflow looks
like. Now let’s look at what can go wrong in learning and prevent you from
making accurate predictions.
Main Challenges of Machine Learning
In short, since your main task is to select a model and train it on some data,
the two things that can go wrong are “bad model” and “bad data”. Let’s
start with examples of bad data.
Insufficient Quantity of Training Data
For a toddler to learn what an apple is, all it takes is for you to point to an
apple and say “apple” (possibly repeating this procedure a few times). Now
the child is able to recognize apples in all sorts of colors and shapes.
Genius.
Machine learning is not quite there yet; it takes a lot of data for most
machine learning algorithms to work properly. Even for very simple
problems you typically need thousands of examples, and for complex
problems such as image or speech recognition you may need millions of
examples (unless you can reuse parts of an existing model).


THE UNREASONABLE EFFECTIVENESS OF DATA
In a famous paper published in 2001, Microsoft researchers Michele
Banko and Eric Brill showed that very different machine learning
algorithms, including fairly simple ones, performed almost identically
well on a complex problem of natural language disambiguation once
they were given enough data (as you can see in Figure 1-21).
As the authors put it, “these results suggest that we may want to
reconsider the trade-off between spending time and money on algorithm
development versus spending it on corpus development”.
The idea that data matters more than algorithms for complex problems
was further popularized by Peter Norvig et al. in a paper titled “The
Unreasonable Effectiveness of Data”, published in 2009. It should be
noted, however, that small and medium-sized datasets are still very
common, and it is not always easy or cheap to get extra training data
so don’t abandon algorithms just yet.
6
7


Figure 1-21. The importance of data versus algorithms
Nonrepresentative Training Data
In order to generalize well, it is crucial that your training data be
representative of the new cases you want to generalize to. This is true
whether you use instance-based learning or model-based learning.
For example, the set of countries you used earlier for training the linear
model was not perfectly representative; it did not contain any country with
8


a GDP per capita lower than $23,500 or higher than $62,500. Figure 1-22
shows what the data looks like when you add such countries.
If you train a linear model on this data, you get the solid line, while the old
model is represented by the dotted line. As you can see, not only does
adding a few missing countries significantly alter the model, but it makes it
clear that such a simple linear model is probably never going to work well.
It seems that very rich countries are not happier than moderately rich
countries (in fact, they seem slightly unhappier!), and conversely some poor
countries seem happier than many rich countries.
By using a nonrepresentative training set, you trained a model that is
unlikely to make accurate predictions, especially for very poor and very
rich countries.
Figure 1-22. A more representative training sample
It is crucial to use a training set that is representative of the cases you want
to generalize to. This is often harder than it sounds: if the sample is too
small, you will have sampling noise (i.e., nonrepresentative data as a result
of chance), but even very large samples can be nonrepresentative if the
sampling method is flawed. This is called sampling bias.


EXAMPLES OF SAMPLING BIAS
Perhaps the most famous example of sampling bias happened during
the US presidential election in 1936, which pitted Landon against
Roosevelt: the Literary Digest conducted a very large poll, sending mail
to about 10 million people. It got 2.4 million answers, and predicted
with high confidence that Landon would get 57% of the votes. Instead,
Roosevelt won with 62% of the votes. The flaw was in the Literary
Digest’s sampling method:
First, to obtain the addresses to send the polls to, the Literary
Digest used telephone directories, lists of magazine subscribers,
club membership lists, and the like. All of these lists tended to
favor wealthier people, who were more likely to vote Republican
(hence Landon).
Second, less than 25% of the people who were polled answered.
Again this introduced a sampling bias, by potentially ruling out
people who didn’t care much about politics, people who didn’t like
the Literary Digest, and other key groups. This is a special type of
sampling bias called nonresponse bias.
Here is another example: say you want to build a system to recognize
funk music videos. One way to build your training set is to search for
“funk music” on YouTube and use the resulting videos. But this
assumes that YouTube’s search engine returns a set of videos that are
representative of all the funk music videos on YouTube. In reality, the
search results are likely to be biased toward popular artists (and if you
live in Brazil you will get a lot of “funk carioca” videos, which sound
nothing like James Brown). On the other hand, how else can you get a
large training set?
Poor-Quality Data


Obviously, if your training data is full of errors, outliers, and noise (e.g.,
due to poor-quality measurements), it will make it harder for the system to
detect the underlying patterns, so your system is less likely to perform well.
It is often well worth the effort to spend time cleaning up your training data.
The truth is, most data scientists spend a significant part of their time doing
just that. The following are a couple examples of when you’d want to clean
up training data:
If some instances are clearly outliers, it may help to simply discard
them or try to fix the errors manually.
If some instances are missing a few features (e.g., 5% of your
customers did not specify their age), you must decide whether you
want to ignore this attribute altogether, ignore these instances, fill in
the missing values (e.g., with the median age), or train one model with
the feature and one model without it.
Irrelevant Features
As the saying goes: garbage in, garbage out. Your system will only be
capable of learning if the training data contains enough relevant features
and not too many irrelevant ones. A critical part of the success of a machine
learning project is coming up with a good set of features to train on. This
process, called feature engineering, involves the following steps:
Feature selection (selecting the most useful features to train on among
existing features)
Feature extraction (combining existing features to produce a more
useful one—as we saw earlier, dimensionality reduction algorithms
can help)
Creating new features by gathering new data
Now that we have looked at many examples of bad data, let’s look at a
couple examples of bad algorithms.


Overfitting the Training Data
Say you are visiting a foreign country and the taxi driver rips you off. You
might be tempted to say that all taxi drivers in that country are thieves.
Overgeneralizing is something that we humans do all too often, and
unfortunately machines can fall into the same trap if we are not careful. In
machine learning this is called overfitting: it means that the model performs
well on the training data, but it does not generalize well.
Figure 1-23 shows an example of a high-degree polynomial life satisfaction
model that strongly overfits the training data. Even though it performs
much better on the training data than the simple linear model, would you
really trust its predictions?
Figure 1-23. Overfitting the training data
Complex models such as deep neural networks can detect subtle patterns in
the data, but if the training set is noisy, or if it is too small, which introduces
sampling noise, then the model is likely to detect patterns in the noise itself
(as in the taxi driver example). Obviously these patterns will not generalize
to new instances. For example, say you feed your life satisfaction model
many more attributes, including uninformative ones such as the country’s
name. In that case, a complex model may detect patterns like the fact that
all countries in the training data with a w in their name have a life
satisfaction greater than 7: New Zealand (7.3), Norway (7.6), Sweden (7.3),
and Switzerland (7.5). How confident are you that the w-satisfaction rule
generalizes to Rwanda or Zimbabwe? Obviously this pattern occurred in the


training data by pure chance, but the model has no way to tell whether a
pattern is real or simply the result of noise in the data.
WARNING
Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:
Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data, or by constraining the model.
Gather more training data.
Reduce the noise in the training data (e.g., fix data errors and remove outliers).
Constraining a model to make it simpler and reduce the risk of overfitting is
called regularization. For example, the linear model we defined earlier has
two parameters, θ and θ . This gives the learning algorithm two degrees of
freedom to adapt the model to the training data: it can tweak both the height
(θ ) and the slope (θ ) of the line. If we forced θ = 0, the algorithm would
have only one degree of freedom and would have a much harder time fitting
the data properly: all it could do is move the line up or down to get as close
as possible to the training instances, so it would end up around the mean. A
very simple model indeed! If we allow the algorithm to modify θ but we
force it to keep it small, then the learning algorithm will effectively have
somewhere in between one and two degrees of freedom. It will produce a
model that’s simpler than one with two degrees of freedom, but more
complex than one with just one. You want to find the right balance between
fitting the training data perfectly and keeping the model simple enough to
ensure that it will generalize well.
Figure 1-24 shows three models. The dotted line represents the original
model that was trained on the countries represented as circles (without the
countries represented as squares), the solid line is our second model trained
with all countries (circles and squares), and the dashed line is a model
trained with the same data as the first model but with a regularization
01
01 1
1


constraint. You can see that regularization forced the model to have a
smaller slope: this model does not fit the training data (circles) as well as
the first model, but it actually generalizes better to new examples that it did
not see during training (squares).
Figure 1-24. Regularization reduces the risk of overfitting
The amount of regularization to apply during learning can be controlled by
a hyperparameter. A hyperparameter is a parameter of a learning algorithm
(not of the model). As such, it is not affected by the learning algorithm
itself; it must be set prior to training and remains constant during training. If
you set the regularization hyperparameter to a very large value, you will get
an almost flat model (a slope close to zero); the learning algorithm will
almost certainly not overfit the training data, but it will be less likely to find
a good solution. Tuning hyperparameters is an important part of building a
machine learning system (you will see a detailed example in the next
chapter).
Underfitting the Training Data
As you might guess, underfitting is the opposite of overfitting: it occurs
when your model is too simple to learn the underlying structure of the data.
For example, a linear model of life satisfaction is prone to underfit; reality
is just more complex than the model, so its predictions are bound to be
inaccurate, even on the training examples.
Here are the main options for fixing this problem:


Select a more powerful model, with more parameters.
Feed better features to the learning algorithm (feature engineering).
Reduce the constraints on the model (for example by reducing the
regularization hyperparameter).
Stepping Back
By now you know a lot about machine learning. However, we went through
so many concepts that you may be feeling a little lost, so let’s step back and
look at the big picture:
Machine learning is about making machines get better at some task by
learning from data, instead of having to explicitly code rules.
There are many different types of ML systems: supervised or not,
batch or online, instance-based or model-based.
In an ML project you gather data in a training set, and you feed the
training set to a learning algorithm. If the algorithm is model-based, it
tunes some parameters to fit the model to the training set (i.e., to make
good predictions on the training set itself), and then hopefully it will be
able to make good predictions on new cases as well. If the algorithm is
instance-based, it just learns the examples by heart and generalizes to
new instances by using a similarity measure to compare them to the
learned instances.
The system will not perform well if your training set is too small, or if
the data is not representative, is noisy, or is polluted with irrelevant
features (garbage in, garbage out). Lastly, your model needs to be
neither too simple (in which case it will underfit) nor too complex (in
which case it will overfit).
There’s just one last important topic to cover: once you have trained a
model, you don’t want to just “hope” it generalizes to new cases. You want
to evaluate it and fine-tune it if necessary. Let’s see how to do that.


Testing and Validating
The only way to know how well a model will generalize to new cases is to
actually try it out on new cases. One way to do that is to put your model in
production and monitor how well it performs. This works well, but if your
model is horribly bad, your users will complain—not the best idea.
A better option is to split your data into two sets: the training set and the
test set. As these names imply, you train your model using the training set,
and you test it using the test set. The error rate on new cases is called the
generalization error (or out-of-sample error), and by evaluating your model
on the test set, you get an estimate of this error. This value tells you how
well your model will perform on instances it has never seen before.
If the training error is low (i.e., your model makes few mistakes on the
training set) but the generalization error is high, it means that your model is
overfitting the training data.
TIP
It is common to use 80% of the data for training and hold out 20% for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instances, probably more than enough to get a good estimate of the generalization error.
Hyperparameter Tuning and Model Selection
Evaluating a model is simple enough: just use a test set. But suppose you
are hesitating between two types of models (say, a linear model and a
polynomial model): how can you decide between them? One option is to
train both and compare how well they generalize using the test set.
Now suppose that the linear model generalizes better, but you want to apply
some regularization to avoid overfitting. The question is, how do you
choose the value of the regularization hyperparameter? One option is to
train 100 different models using 100 different values for this
hyperparameter. Suppose you find the best hyperparameter value that


produces a model with the lowest generalization error—say, just 5% error.
You launch this model into production, but unfortunately it does not
perform as well as expected and produces 15% errors. What just happened?
The problem is that you measured the generalization error multiple times on
the test set, and you adapted the model and hyperparameters to produce the
best model for that particular set. This means the model is unlikely to
perform as well on new data.
A common solution to this problem is called holdout validation (Figure 1
25): you simply hold out part of the training set to evaluate several
candidate models and select the best one. The new held-out set is called the
validation set (or the development set, or dev set). More specifically, you
train multiple models with various hyperparameters on the reduced training
set (i.e., the full training set minus the validation set), and you select the
model that performs best on the validation set. After this holdout validation
process, you train the best model on the full training set (including the
validation set), and this gives you the final model. Lastly, you evaluate this
final model on the test set to get an estimate of the generalization error.
Figure 1-25. Model selection using holdout validation
This solution usually works quite well. However, if the validation set is too
small, then the model evaluations will be imprecise: you may end up


selecting a suboptimal model by mistake. Conversely, if the validation set is
too large, then the remaining training set will be much smaller than the full
training set. Why is this bad? Well, since the final model will be trained on
the full training set, it is not ideal to compare candidate models trained on a
much smaller training set. It would be like selecting the fastest sprinter to
participate in a marathon. One way to solve this problem is to perform
repeated cross-validation, using many small validation sets. Each model is
evaluated once per validation set after it is trained on the rest of the data. By
averaging out all the evaluations of a model, you get a much more accurate
measure of its performance. There is a drawback, however: the training
time is multiplied by the number of validation sets.
Data Mismatch
In some cases, it’s easy to get a large amount of data for training, but this
data probably won’t be perfectly representative of the data that will be used
in production. For example, suppose you want to create a mobile app to
take pictures of flowers and automatically determine their species. You can
easily download millions of pictures of flowers on the web, but they won’t
be perfectly representative of the pictures that will actually be taken using
the app on a mobile device. Perhaps you only have 1,000 representative
pictures (i.e., actually taken with the app).
In this case, the most important rule to remember is that both the validation
set and the test set must be as representative as possible of the data you
expect to use in production, so they should be composed exclusively of
representative pictures: you can shuffle them and put half in the validation
set and half in the test set (making sure that no duplicates or near-duplicates
end up in both sets). After training your model on the web pictures, if you
observe that the performance of the model on the validation set is
disappointing, you will not know whether this is because your model has
overfit the training set, or whether this is just due to the mismatch between
the web pictures and the mobile app pictures.
One solution is to hold out some of the training pictures (from the web) in
yet another set that Andrew Ng dubbed the train-dev set (Figure 1-26).


After the model is trained (on the training set, not on the train-dev set), you
can evaluate it on the train-dev set. If the model performs poorly, then it
must have overfit the training set, so you should try to simplify or
regularize the model, get more training data, and clean up the training data.
But if it performs well on the train-dev set, then you can evaluate the model
on the dev set. If it performs poorly, then the problem must be coming from
the data mismatch. You can try to tackle this problem by preprocessing the
web images to make them look more like the pictures that will be taken by
the mobile app, and then retraining the model. Once you have a model that
performs well on both the train-dev set and the dev set, you can evaluate it
one last time on the test set to know how well it is likely to perform in
production.
Figure 1-26. When real data is scarce (right), you may use similar abundant data (left) for training and hold out some of it in a train-dev set to evaluate overfitting; the real data is then used to evaluate data mismatch (dev set) and to evaluate the final model’s performance (test set)


NO FREE LUNCH THEOREM
A model is a simplified representation of the data. The simplifications
are meant to discard the superfluous details that are unlikely to
generalize to new instances. When you select a particular type of
model, you are implicitly making assumptions about the data. For
example, if you choose a linear model, you are implicitly assuming that
the data is fundamentally linear and that the distance between the
instances and the straight line is just noise, which can safely be ignored.
In a famous 1996 paper, David Wolpert demonstrated that if you make
absolutely no assumption about the data, then there is no reason to
prefer one model over any other. This is called the No Free Lunch
(NFL) theorem. For some datasets the best model is a linear model,
while for other datasets it is a neural network. There is no model that is
a priori guaranteed to work better (hence the name of the theorem). The
only way to know for sure which model is best is to evaluate them all.
Since this is not possible, in practice you make some reasonable
assumptions about the data and evaluate only a few reasonable models.
For example, for simple tasks you may evaluate linear models with
various levels of regularization, and for a complex problem you may
evaluate various neural networks.
Exercises
In this chapter we have covered some of the most important concepts in
machine learning. In the next chapters we will dive deeper and write more
code, but before we do, make sure you can answer the following questions:
1. How would you define machine learning?
2. Can you name four types of applications where it shines?
3. What is a labeled training set?
9


4. What are the two most common supervised tasks?
5. Can you name four common unsupervised tasks?
6. What type of algorithm would you use to allow a robot to walk in
various unknown terrains?
7. What type of algorithm would you use to segment your customers into
multiple groups?
8. Would you frame the problem of spam detection as a supervised
learning problem or an unsupervised learning problem?
9. What is an online learning system?
10. What is out-of-core learning?
11. What type of algorithm relies on a similarity measure to make
predictions?
12. What is the difference between a model parameter and a model
hyperparameter?
13. What do model-based algorithms search for? What is the most
common strategy they use to succeed? How do they make predictions?
14. Can you name four of the main challenges in machine learning?
15. If your model performs great on the training data but generalizes
poorly to new instances, what is happening? Can you name three
possible solutions?
16. What is a test set, and why would you want to use it?
17. What is the purpose of a validation set?
18. What is the train-dev set, when do you need it, and how do you use it?
19. What can go wrong if you tune hyperparameters using the test set?
Solutions to these exercises are available at the end of this chapter’s
notebook, at https://homl.info/colab3.


1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the fact that the children of tall people tend to be shorter than their parents. Since the children were shorter, he called this regression to the mean. This name was then applied to the methods he used to analyze correlations between variables.
2 Notice how animals are rather well separated from vehicles and how horses are close to deer but far from birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-Modal Transfer”, Proceedings of the 26th International Conference on Neural Information Processing Systems 1 (2013): 935–943.
3 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you may need to provide a few labels per person and manually clean up some clusters.
4 By convention, the Greek letter θ (theta) is frequently used to represent model parameters.
5 It’s OK if you don’t understand all the code yet; I will present Scikit-Learn in the following chapters.
6 For example, knowing whether to write “to”, “two”, or “too”, depending on the context.
7 Peter Norvig et al., “The Unreasonable Effectiveness of Data”, IEEE Intelligent Systems 24, no. 2 (2009): 8–12.
8 Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very Large Corpora for Natural Language Disambiguation”, Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (2001): 26–33.
9 David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms”, Neural Computation 8, no. 7 (1996): 1341–1390.


Chapter 2. End-to-End Machine
Learning Project
In this chapter you will work through an example project end to end,
pretending to be a recently hired data scientist at a real estate company. This
example is fictitious; the goal is to illustrate the main steps of a machine
learning project, not to learn anything about the real estate business. Here
are the main steps we will walk through:
1. Look at the big picture.
2. Get the data.
3. Explore and visualize the data to gain insights.
4. Prepare the data for machine learning algorithms.
5. Select a model and train it.
6. Fine-tune your model.
7. Present your solution.
8. Launch, monitor, and maintain your system.
Working with Real Data
When you are learning about machine learning, it is best to experiment with
real-world data, not artificial datasets. Fortunately, there are thousands of
open datasets to choose from, ranging across all sorts of domains. Here are
a few places you can look to get data:
Popular open data repositories:
OpenML.org


Kaggle.com
PapersWithCode.com
UC Irvine Machine Learning Repository
Amazon’s AWS datasets
TensorFlow datasets
Meta portals (they list open data repositories):
DataPortals.org
OpenDataMonitor.eu
Other pages listing many popular open data repositories:
Wikipedia’s list of machine learning datasets
Quora.com
The datasets subreddit
In this chapter we’ll use the California Housing Prices dataset from the
StatLib repository (see Figure 2-1). This dataset is based on data from the
1990 California census. It is not exactly recent (a nice house in the Bay
Area was still affordable at the time), but it has many qualities for learning,
so we will pretend it is recent data. For teaching purposes I’ve added a
categorical attribute and removed a few features.
1


Figure 2-1. California housing prices
Look at the Big Picture
Welcome to the Machine Learning Housing Corporation! Your first task is
to use California census data to build a model of housing prices in the state.
This data includes metrics such as the population, median income, and
median housing price for each block group in California. Block groups are
the smallest geographical unit for which the US Census Bureau publishes
sample data (a block group typically has a population of 600 to 3,000
people). I will call them “districts” for short.
Your model should learn from this data and be able to predict the median
housing price in any district, given all the other metrics.


TIP
Since you are a well-organized data scientist, the first thing you should do is pull out your machine learning project checklist. You can start with the one in Appendix A; it should work reasonably well for most machine learning projects, but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self-explanatory or because they will be discussed in later chapters.
Frame the Problem
The first question to ask your boss is what exactly the business objective is.
Building a model is probably not the end goal. How does the company
expect to use and benefit from this model? Knowing the objective is
important because it will determine how you frame the problem, which
algorithms you will select, which performance measure you will use to
evaluate your model, and how much effort you will spend tweaking it.
Your boss answers that your model’s output (a prediction of a district’s
median housing price) will be fed to another machine learning system (see
Figure 2-2), along with many other signals. This downstream system will
determine whether it is worth investing in a given area. Getting this right is
critical, as it directly affects revenue.
The next question to ask your boss is what the current solution looks like (if
any). The current situation will often give you a reference for performance,
as well as insights on how to solve the problem. Your boss answers that the
district housing prices are currently estimated manually by experts: a team
gathers up-to-date information about a district, and when they cannot get
the median housing price, they estimate it using complex rules.
2


Figure 2-2. A machine learning pipeline for real estate investments
This is costly and time-consuming, and their estimates are not great; in
cases where they manage to find out the actual median housing price, they
often realize that their estimates were off by more than 30%. This is why
the company thinks that it would be useful to train a model to predict a
district’s median housing price, given other data about that district. The
census data looks like a great dataset to exploit for this purpose, since it
includes the median housing prices of thousands of districts, as well as
other data.


PIPELINES
A sequence of data processing components is called a data pipeline.
Pipelines are very common in machine learning systems, since there is
a lot of data to manipulate and many data transformations to apply.
Components typically run asynchronously. Each component pulls in a
large amount of data, processes it, and spits out the result in another
data store. Then, some time later, the next component in the pipeline
pulls in this data and spits out its own output. Each component is fairly
self-contained: the interface between components is simply the data
store. This makes the system simple to grasp (with the help of a data
flow graph), and different teams can focus on different components.
Moreover, if a component breaks down, the downstream components
can often continue to run normally (at least for a while) by just using
the last output from the broken component. This makes the architecture
quite robust.
On the other hand, a broken component can go unnoticed for some time
if proper monitoring is not implemented. The data gets stale and the
overall system’s performance drops.
With all this information, you are now ready to start designing your system.
First, determine what kind of training supervision the model will need: is it
a supervised, unsupervised, semi-supervised, self-supervised, or
reinforcement learning task? And is it a classification task, a regression
task, or something else? Should you use batch learning or online learning
techniques? Before you read on, pause and try to answer these questions for
yourself.
Have you found the answers? Let’s see. This is clearly a typical supervised
learning task, since the model can be trained with labeled examples (each
instance comes with the expected output, i.e., the district’s median housing
price). It is a typical regression task, since the model will be asked to
predict a value. More specifically, this is a multiple regression problem,
since the system will use multiple features to make a prediction (the


district’s population, the median income, etc.). It is also a univariate
regression problem, since we are only trying to predict a single value for
each district. If we were trying to predict multiple values per district, it
would be a multivariate regression problem. Finally, there is no continuous
flow of data coming into the system, there is no particular need to adjust to
changing data rapidly, and the data is small enough to fit in memory, so
plain batch learning should do just fine.
TIP
If the data were huge, you could either split your batch learning work across multiple servers (using the MapReduce technique) or use an online learning technique.
Select a Performance Measure
Your next step is to select a performance measure. A typical performance
measure for regression problems is the root mean square error (RMSE). It
gives an idea of how much error the system typically makes in its
predictions, with a higher weight given to large errors. Equation 2-1 shows
the mathematical formula to compute the RMSE.
Equation 2-1. Root mean square error (RMSE)
RMSE (X, h) =
  
⎷
m
∑
i=1
(h (x(i)) − y(i))
2
1
m


NOTATIONS
This equation introduces several very common machine learning
notations that I will use throughout this book:
m is the number of instances in the dataset you are measuring the
RMSE on.
For example, if you are evaluating the RMSE on a validation
set of 2,000 districts, then m = 2,000.
x is a vector of all the feature values (excluding the label) of the
i instance in the dataset, and y is its label (the desired output
value for that instance).
For example, if the first district in the dataset is located at
longitude –118.29°, latitude 33.91°, and it has 1,416
inhabitants with a median income of $38,372, and the median
house value is $156,400 (ignoring other features for now),
then:
x(1) =
⎛
⎜⎜⎜⎜
⎝
−118.29
33.91
1,416
38,372
⎞
⎟⎟⎟⎟
⎠
and:
y(1) = 156,400
X is a matrix containing all the feature values (excluding labels) of
all instances in the dataset. There is one row per instance, and the
i row is equal to the transpose of x , noted (x ) .
For example, if the first district is as just described, then the
matrix X looks like this:
(i)
th (i)
th (i) (i) ⊺ 3


X=
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜
⎝
(x(1))⊺
(x(2))⊺
⋮
(x(1999))⊺
(x(2000))⊺
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟
⎠
=(
−118.29 33.91 1,416 38,372
⋮ ⋮⋮ ⋮
)
h is your system’s prediction function, also called a hypothesis.
When your system is given an instance’s feature vector x , it
outputs a predicted value ŷ = h(x ) for that instance (ŷ is
pronounced “y-hat”).
For example, if your system predicts that the median housing
price in the first district is $158,400, then ŷ = h(x ) =
158,400. The prediction error for this district is ŷ – y =
2,000.
RMSE(X,h) is the cost function measured on the set of examples
using your hypothesis h.
We use lowercase italic font for scalar values (such as m or y ) and
function names (such as h), lowercase bold font for vectors (such as
x ), and uppercase bold font for matrices (such as X).
Although the RMSE is generally the preferred performance measure for
regression tasks, in some contexts you may prefer to use another function.
For example, if there are many outlier districts. In that case, you may
consider using the mean absolute error (MAE, also called the average
absolute deviation), shown in Equation 2-2:
Equation 2-2. Mean absolute error (MAE)
MAE (X, h) =
m
∑
i=1
∣∣h (x(i)) − y(i)∣∣
(i)
(i) (i)
(1) (1)
(1) (1)
(i)
(i)
1
m


Both the RMSE and the MAE are ways to measure the distance between
two vectors: the vector of predictions and the vector of target values.
Various distance measures, or norms, are possible:
Computing the root of a sum of squares (RMSE) corresponds to the
Euclidean norm: this is the notion of distance we are all familiar with.
It is also called the l norm, noted ∥ · ∥ (or just ∥ · ∥).
Computing the sum of absolutes (MAE) corresponds to the l norm,
noted ∥ · ∥ . This is sometimes called the Manhattan norm because it
measures the distance between two points in a city if you can only
travel along orthogonal city blocks.
More generally, the l norm of a vector v containing n elements is
defined as ∥v∥ = (|v | + |v | + ... + |v | ) . l gives the number of
nonzero elements in the vector, and l gives the maximum absolute
value in the vector.
The higher the norm index, the more it focuses on large values and neglects
small ones. This is why the RMSE is more sensitive to outliers than the
MAE. But when outliers are exponentially rare (like in a bell-shaped curve),
the RMSE performs very well and is generally preferred.
Check the Assumptions
Lastly, it is good practice to list and verify the assumptions that have been
made so far (by you or others); this can help you catch serious issues early
on. For example, the district prices that your system outputs are going to be
fed into a downstream machine learning system, and you assume that these
prices are going to be used as such. But what if the downstream system
converts the prices into categories (e.g., “cheap”, “medium”, or
“expensive”) and then uses those categories instead of the prices
themselves? In this case, getting the price perfectly right is not important at
all; your system just needs to get the category right. If that’s so, then the
problem should have been framed as a classification task, not a regression
22
1
1
k
k 1 k 2 k n k 1/k 0
∞


task. You don’t want to find this out after working on a regression system
for months.
Fortunately, after talking with the team in charge of the downstream system,
you are confident that they do indeed need the actual prices, not just
categories. Great! You’re all set, the lights are green, and you can start
coding now!
Get the Data
It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and
walk through the code examples. As I mentioned in the preface, all the code
examples in this book are open source and available online as Jupyter
notebooks, which are interactive documents containing text, images, and
executable code snippets (Python in our case). In this book I will assume
you are running these notebooks on Google Colab, a free service that lets
you run any Jupyter notebook directly online, without having to install
anything on your machine. If you want to use another online platform (e.g.,
Kaggle) or if you want to install everything locally on your own machine,
please see the instructions on the book’s GitHub page.
Running the Code Examples Using Google Colab
First, open a web browser and visit https://homl.info/colab3: this will lead
you to Google Colab, and it will display the list of Jupyter notebooks for
this book (see Figure 2-3). You will find one notebook per chapter, plus a
few extra notebooks and tutorials for NumPy, Matplotlib, Pandas, linear
algebra, and differential calculus. For example, if you click
02_end_to_end_machine_learning_project.ipynb, the notebook from
Chapter 2 will open up in Google Colab (see Figure 2-4).
A Jupyter notebook is composed of a list of cells. Each cell contains either
executable code or text. Try double-clicking the first text cell (which
contains the sentence “Welcome to Machine Learning Housing Corp.!”).
This will open the cell for editing. Notice that Jupyter notebooks use


Markdown syntax for formatting (e.g., **bold**, *italics*, #
Title, [url](link text), and so on). Try modifying this text, then
press Shift-Enter to see the result.
Figure 2-3. List of notebooks in Google Colab


Figure 2-4. Your notebook in Google Colab
Next, create a new code cell by selecting Insert → “Code cell” from the
menu. Alternatively, you can click the + Code button in the toolbar, or
hover your mouse over the bottom of a cell until you see + Code and + Text
appear, then click + Code. In the new code cell, type some Python code,
such as print("Hello World"), then press Shift-Enter to run this
code (or click the ▷ button on the left side of the cell).
If you’re not logged in to your Google account, you’ll be asked to log in
now (if you don’t already have a Google account, you’ll need to create
one). Once you are logged in, when you try to run the code you’ll see a
security warning telling you that this notebook was not authored by Google.
A malicious person could create a notebook that tries to trick you into
entering your Google credentials so they can access your personal data, so
before you run a notebook, always make sure you trust its author (or
double-check what each code cell will do before running it). Assuming you


trust me (or you plan to check every code cell), you can now click “Run
anyway”.
Colab will then allocate a new runtime for you: this is a free virtual
machine located on Google’s servers that contains a bunch of tools and
Python libraries, including everything you’ll need for most chapters (in
some chapters, you’ll need to run a command to install additional libraries).
This will take a few seconds. Next, Colab will automatically connect to this
runtime and use it to execute your new code cell. Importantly, the code runs
on the runtime, not on your machine. The code’s output will be displayed
under the cell. Congrats, you’ve run some Python code on Colab!
TIP
To insert a new code cell, you can also type Ctrl-M (or Cmd-M on macOS) followed by A (to insert above the active cell) or B (to insert below). There are many other keyboard shortcuts available: you can view and edit them by typing Ctrl-M (or Cmd-M) then H. If you choose to run the notebooks on Kaggle or on your own machine using JupyterLab or an IDE such as Visual Studio Code with the Jupyter extension, you will see some minor differences—runtimes are called kernels, the user interface and keyboard shortcuts are slightly different, etc.—but switching from one Jupyter environment to another is not too hard.
Saving Your Code Changes and Your Data
You can make changes to a Colab notebook, and they will persist for as
long as you keep your browser tab open. But once you close it, the changes
will be lost. To avoid this, make sure you save a copy of the notebook to
your Google Drive by selecting File → “Save a copy in Drive”.
Alternatively, you can download the notebook to your computer by
selecting File → Download → “Download .ipynb”. Then you can later visit
https://colab.research.google.com and open the notebook again (either from
Google Drive or by uploading it from your computer).


WARNING
Google Colab is meant only for interactive use: you can play around in the notebooks and tweak the code as you like, but you cannot let the notebooks run unattended for a long period of time, or else the runtime will be shut down and all of its data will be lost.
If the notebook generates data that you care about, make sure you download
this data before the runtime shuts down. To do this, click the Files icon (see
step 1 in Figure 2-5), find the file you want to download, click the vertical
dots next to it (step 2), and click Download (step 3). Alternatively, you can
mount your Google Drive on the runtime, allowing the notebook to read
and write files directly to Google Drive as if it were a local directory. For
this, click the Files icon (step 1), then click the Google Drive icon (circled
in Figure 2-5) and follow the on-screen instructions.
Figure 2-5. Downloading a file from a Google Colab runtime (steps 1 to 3), or mounting your Google Drive (circled icon)
By default, your Google Drive will be mounted at /content/drive/MyDrive.
If you want to back up a data file, simply copy it to this directory by
running !cp /content/my_great_model
/content/drive/MyDrive. Any command starting with a bang (!) is
treated as a shell command, not as Python code: cp is the Linux shell
command to copy a file from one path to another. Note that Colab runtimes
run on Linux (specifically, Ubuntu).


The Power and Danger of Interactivity
Jupyter notebooks are interactive, and that’s a great thing: you can run each
cell one by one, stop at any point, insert a cell, play with the code, go back
and run the same cell again, etc., and I highly encourage you to do so. If
you just run the cells one by one without ever playing around with them,
you won’t learn as fast. However, this flexibility comes at a price: it’s very
easy to run cells in the wrong order, or to forget to run a cell. If this
happens, the subsequent code cells are likely to fail. For example, the very
first code cell in each notebook contains setup code (such as imports), so
make sure you run it first, or else nothing will work.
TIP
If you ever run into a weird error, try restarting the runtime (by selecting Runtime → “Restart runtime” from the menu) and then run all the cells again from the beginning of the notebook. This often solves the problem. If not, it’s likely that one of the changes you made broke the notebook: just revert to the original notebook and try again. If it still fails, please file an issue on GitHub.


Book Code Versus Notebook Code
You may sometimes notice some little differences between the code in this
book and the code in the notebooks. This may happen for several reasons:
A library may have changed slightly by the time you read these lines,
or perhaps despite my best efforts I made an error in the book. Sadly, I
cannot magically fix the code in your copy of this book (unless you are
reading an electronic copy and you can download the latest version),
but I can fix the notebooks. So, if you run into an error after copying
code from this book, please look for the fixed code in the notebooks: I
will strive to keep them error-free and up-to-date with the latest library
versions.
The notebooks contain some extra code to beautify the figures (adding
labels, setting font sizes, etc.) and to save them in high resolution for
this book. You can safely ignore this extra code if you want.
I optimized the code for readability and simplicity: I made it as linear and
flat as possible, defining very few functions or classes. The goal is to ensure
that the code you are running is generally right in front of you, and not
nested within several layers of abstractions that you have to search through.
This also makes it easier for you to play with the code. For simplicity,
there’s limited error handling, and I placed some of the least common
imports right where they are needed (instead of placing them at the top of
the file, as is recommended by the PEP 8 Python style guide). That said,
your production code will not be very different: just a bit more modular,
and with additional tests and error handling.
OK! Once you’re comfortable with Colab, you’re ready to download the
data.
Download the Data
In typical environments your data would be available in a relational
database or some other common data store, and spread across multiple
tables/documents/files. To access it, you would first need to get your


credentials and access authorizations and familiarize yourself with the data
schema. In this project, however, things are much simpler: you will just
download a single compressed file, housing.tgz, which contains a comma
separated values (CSV) file called housing.csv with all the data.
Rather than manually downloading and decompressing the data, it’s usually
preferable to write a function that does it for you. This is useful in particular
if the data changes regularly: you can write a small script that uses the
function to fetch the latest data (or you can set up a scheduled job to do that
automatically at regular intervals). Automating the process of fetching the
data is also useful if you need to install the dataset on multiple machines.
Here is the function to fetch and load the data:
from pathlib import Path import pandas as pd import tarfile import urllib.request
def load_housing_data(): tarball_path = Path("datasets/housing.tgz") if not tarball_path.is_file(): Path("datasets").mkdir(parents=True, exist_ok=True) url = "https://github.com/ageron/data/raw/main/housing.tgz" urllib.request.urlretrieve(url, tarball_path) with tarfile.open(tarball_path) as housing_tarball: housing_tarball.extractall(path="datasets") return pd.read_csv(Path("datasets/housing/housing.csv"))
housing = load_housing_data()
When load_housing_data() is called, it looks for the
datasets/housing.tgz file. If it does not find it, it creates the datasets
directory inside the current directory (which is /content by default, in
Colab), downloads the housing.tgz file from the ageron/data GitHub
repository, and extracts its content into the datasets directory; this creates
the datasets/housing directory with the housing.csv file inside it. Lastly, the
function loads this CSV file into a Pandas DataFrame object containing all
the data, and returns it.
4


Take a Quick Look at the Data Structure
You start by looking at the top five rows of data using the DataFrame’s
head() method (see Figure 2-6).
Figure 2-6. Top five rows in the dataset
Each row represents one district. There are 10 attributes (they are not all
shown in the screenshot): longitude, latitude,
housing_median_age, total_rooms, total_bedrooms,
population, households, median_income,
median_house_value, and ocean_proximity.
The info() method is useful to get a quick description of the data, in
particular the total number of rows, each attribute’s type, and the number of
non-null values:
>>> housing.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object


dtypes: float64(9), object(1) memory usage: 1.6+ MB
NOTE
In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like in the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented blocks), and the outputs have no prefix.
There are 20,640 instances in the dataset, which means that it is fairly small
by machine learning standards, but it’s perfect to get started. You notice that
the total_bedrooms attribute has only 20,433 non-null values, meaning
that 207 districts are missing this feature. You will need to take care of this
later.
All attributes are numerical, except for ocean_proximity. Its type is
object, so it could hold any kind of Python object. But since you loaded
this data from a CSV file, you know that it must be a text attribute. When
you looked at the top five rows, you probably noticed that the values in the
ocean_proximity column were repetitive, which means that it is
probably a categorical attribute. You can find out what categories exist and
how many districts belong to each category by using the
value_counts() method:
>>> housing["ocean_proximity"].value_counts() <1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64
Let’s look at the other fields. The describe() method shows a summary
of the numerical attributes (Figure 2-7).


Figure 2-7. Summary of each numerical attribute
The count, mean, min, and max rows are self-explanatory. Note that the
null values are ignored (so, for example, the count of
total_bedrooms is 20,433, not 20,640). The std row shows the
standard deviation, which measures how dispersed the values are. The
25%, 50%, and 75% rows show the corresponding percentiles: a percentile
indicates the value below which a given percentage of observations in a
group of observations fall. For example, 25% of the districts have a
housing_median_age lower than 18, while 50% are lower than 29 and
75% are lower than 37. These are often called the 25th percentile (or first
quartile), the median, and the 75th percentile (or third quartile).
Another quick way to get a feel of the type of data you are dealing with is to
plot a histogram for each numerical attribute. A histogram shows the
number of instances (on the vertical axis) that have a given value range (on
the horizontal axis). You can either plot this one attribute at a time, or you
can call the hist() method on the whole dataset (as shown in the
following code example), and it will plot a histogram for each numerical
attribute (see Figure 2-8):
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(12, 8)) plt.show()
5


Figure 2-8. A histogram for each numerical attribute
Looking at these histograms, you notice a few things:
First, the median income attribute does not look like it is expressed in
US dollars (USD). After checking with the team that collected the
data, you are told that the data has been scaled and capped at 15
(actually, 15.0001) for higher median incomes, and at 0.5 (actually,
0.4999) for lower median incomes. The numbers represent roughly
tens of thousands of dollars (e.g., 3 actually means about $30,000).
Working with preprocessed attributes is common in machine learning,
and it is not necessarily a problem, but you should try to understand
how the data was computed.
The housing median age and the median house value were also
capped. The latter may be a serious problem since it is your target
attribute (your labels). Your machine learning algorithms may learn
that prices never go beyond that limit. You need to check with your
client team (the team that will use your system’s output) to see if this is


a problem or not. If they tell you that they need precise predictions
even beyond $500,000, then you have two options:
Collect proper labels for the districts whose labels were capped.
Remove those districts from the training set (and also from the
test set, since your system should not be evaluated poorly if it
predicts values beyond $500,000).
These attributes have very different scales. We will discuss this later in
this chapter, when we explore feature scaling.
Finally, many histograms are skewed right: they extend much farther to
the right of the median than to the left. This may make it a bit harder
for some machine learning algorithms to detect patterns. Later, you’ll
try transforming these attributes to have more symmetrical and bell
shaped distributions.
You should now have a better understanding of the kind of data you’re
dealing with.
WARNING
Wait! Before you look at the data any further, you need to create a test set, put it aside, and never look at it.
Create a Test Set
It may seem strange to voluntarily set aside part of the data at this stage.
After all, you have only taken a quick glance at the data, and surely you
should learn a whole lot more about it before you decide what algorithms to
use, right? This is true, but your brain is an amazing pattern detection
system, which also means that it is highly prone to overfitting: if you look
at the test set, you may stumble upon some seemingly interesting pattern in
the test data that leads you to select a particular kind of machine learning
model. When you estimate the generalization error using the test set, your


estimate will be too optimistic, and you will launch a system that will not
perform as well as expected. This is called data snooping bias.
Creating a test set is theoretically simple; pick some instances randomly,
typically 20% of the dataset (or less if your dataset is very large), and set
them aside:
import numpy as np
def shuffle_and_split_data(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]
You can then use this function like this:
>>> train_set, test_set = shuffle_and_split_data(housing, 0.2) >>> len(train_set) 16512 >>> len(test_set) 4128
Well, this works, but it is not perfect: if you run the program again, it will
generate a different test set! Over time, you (or your machine learning
algorithms) will get to see the whole dataset, which is what you want to
avoid.
One solution is to save the test set on the first run and then load it in
subsequent runs. Another option is to set the random number generator’s
seed (e.g., with np.random.seed(42)) before calling
np.random.permutation() so that it always generates the same
shuffled indices.
However, both these solutions will break the next time you fetch an updated
dataset. To have a stable train/test split even after updating the dataset, a
common solution is to use each instance’s identifier to decide whether or
not it should go in the test set (assuming instances have unique and
6


immutable identifiers). For example, you could compute a hash of each
instance’s identifier and put that instance in the test set if the hash is lower
than or equal to 20% of the maximum hash value. This ensures that the test
set will remain consistent across multiple runs, even if you refresh the
dataset. The new test set will contain 20% of the new instances, but it will
not contain any instance that was previously in the training set.
Here is a possible implementation:
from zlib import crc32
def is_id_in_test_set(identifier, test_ratio): return crc32(np.int64(identifier)) < test_ratio * 2**32
def split_data_with_id_hash(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]
Unfortunately, the housing dataset does not have an identifier column. The
simplest solution is to use the row index as the ID:
housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")
If you use the row index as a unique identifier, you need to make sure that
new data gets appended to the end of the dataset and that no row ever gets
deleted. If this is not possible, then you can try to use the most stable
features to build a unique identifier. For example, a district’s latitude and
longitude are guaranteed to be stable for a few million years, so you could
combine them into an ID like so:
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"] train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")
7


Scikit-Learn provides a few functions to split datasets into multiple subsets
in various ways. The simplest function is train_test_split(), which
does pretty much the same thing as the shuffle_and_split_data()
function we defined earlier, with a couple of additional features. First, there
is a random_state parameter that allows you to set the random
generator seed. Second, you can pass it multiple datasets with an identical
number of rows, and it will split them on the same indices (this is very
useful, for example, if you have a separate DataFrame for labels):
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
So far we have considered purely random sampling methods. This is
generally fine if your dataset is large enough (especially relative to the
number of attributes), but if it is not, you run the risk of introducing a
significant sampling bias. When employees at a survey company decides to
call 1,000 people to ask them a few questions, they don’t just pick 1,000
people randomly in a phone book. They try to ensure that these 1,000
people are representative of the whole population, with regard to the
questions they want to ask. For example, the US population is 51.1%
females and 48.9% males, so a well-conducted survey in the US would try
to maintain this ratio in the sample: 511 females and 489 males (at least if it
seems possible that the answers may vary across genders). This is called
stratified sampling: the population is divided into homogeneous subgroups
called strata, and the right number of instances are sampled from each
stratum to guarantee that the test set is representative of the overall
population. If the people running the survey used purely random sampling,
there would be about a 10.7% chance of sampling a skewed test set with
less than 48.5% female or more than 53.5% female participants. Either way,
the survey results would likely be quite biased.
Suppose you’ve chatted with some experts who told you that the median
income is a very important attribute to predict median housing prices. You
may want to ensure that the test set is representative of the various


categories of incomes in the whole dataset. Since the median income is a
continuous numerical attribute, you first need to create an income category
attribute. Let’s look at the median income histogram more closely (back in
Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,
$15,000–$60,000), but some median incomes go far beyond 6. It is
important to have a sufficient number of instances in your dataset for each
stratum, or else the estimate of a stratum’s importance may be biased. This
means that you should not have too many strata, and each stratum should be
large enough. The following code uses the pd.cut() function to create an
income category attribute with five categories (labeled from 1 to 5);
category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5
to 3, and so on:
housing["income_cat"] = pd.cut(housing["median_income"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])
These income categories are represented in Figure 2-9:
housing["income_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True) plt.xlabel("Income category") plt.ylabel("Number of districts") plt.show()
Now you are ready to do stratified sampling based on the income category.
Scikit-Learn provides a number of splitter classes in the
sklearn.model_selection package that implement various
strategies to split your dataset into a training set and a test set. Each splitter
has a split() method that returns an iterator over different training/test
splits of the same data.


Figure 2-9. Histogram of income categories
To be precise, the split() method yields the training and test indices, not
the data itself. Having multiple splits can be useful if you want to better
estimate the performance of your model, as you will see when we discuss
cross-validation later in this chapter. For example, the following code
generates 10 different stratified splits of the same dataset:
from sklearn.model_selection import StratifiedShuffleSplit
splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42) strat_splits = [] for train_index, test_index in splitter.split(housing, housing["income_cat"]): strat_train_set_n = housing.iloc[train_index] strat_test_set_n = housing.iloc[test_index] strat_splits.append([strat_train_set_n, strat_test_set_n])
For now, you can just use the first split:
strat_train_set, strat_test_set = strat_splits[0]
Or, since stratified sampling is fairly common, there’s a shorter way to get a
single split using the train_test_split() function with the
stratify argument:


strat_train_set, strat_test_set = train_test_split( housing, test_size=0.2, stratify=housing["income_cat"], random_state=42)
Let’s see if this worked as expected. You can start by looking at the income
category proportions in the test set:
>>> strat_test_set["income_cat"].value_counts() / len(strat_test_set) 3 0.350533 2 0.318798 4 0.176357 5 0.114341 1 0.039971 Name: income_cat, dtype: float64
With similar code you can measure the income category proportions in the
full dataset. Figure 2-10 compares the income category proportions in the
overall dataset, in the test set generated with stratified sampling, and in a
test set generated using purely random sampling. As you can see, the test
set generated using stratified sampling has income category proportions
almost identical to those in the full dataset, whereas the test set generated
using purely random sampling is skewed.
Figure 2-10. Sampling bias comparison of stratified versus purely random sampling
You won’t use the income_cat column again, so you might as well drop
it, reverting the data back to its original state:
for set_ in (strat_train_set, strat_test_set): set_.drop("income_cat", axis=1, inplace=True)


We spent quite a bit of time on test set generation for a good reason: this is
an often neglected but critical part of a machine learning project. Moreover,
many of these ideas will be useful later when we discuss cross-validation.
Now it’s time to move on to the next stage: exploring the data.
Explore and Visualize the Data to Gain
Insights
So far you have only taken a quick glance at the data to get a general
understanding of the kind of data you are manipulating. Now the goal is to
go into a little more depth.
First, make sure you have put the test set aside and you are only exploring
the training set. Also, if the training set is very large, you may want to
sample an exploration set, to make manipulations easy and fast during the
exploration phase. In this case, the training set is quite small, so you can
just work directly on the full set. Since you’re going to experiment with
various transformations of the full training set, you should make a copy of
the original so you can revert to it afterwards:
housing = strat_train_set.copy()
Visualizing Geographical Data
Because the dataset includes geographical information (latitude and
longitude), it is a good idea to create a scatterplot of all the districts to
visualize the data (Figure 2-11):
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True) plt.show()


Figure 2-11. A geographical scatterplot of the data
This looks like California all right, but other than that it is hard to see any
particular pattern. Setting the alpha option to 0.2 makes it much easier to
visualize the places where there is a high density of data points (Figure 2
12):
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.2) plt.show()
Now that’s much better: you can clearly see the high-density areas, namely
the Bay Area and around Los Angeles and San Diego, plus a long line of
fairly high-density areas in the Central Valley (in particular, around
Sacramento and Fresno).
Our brains are very good at spotting patterns in pictures, but you may need
to play around with visualization parameters to make the patterns stand out.