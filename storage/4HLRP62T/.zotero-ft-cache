目录
版权信息
版权
内容提要
序
前言
主要符号表
第1章 绪论
第2章 模型评估与选择
第3章 线性模型
第4章 决策树
第5章 神经网络
第6章 支持向量机


第7章 贝叶斯分类器
第8章 集成学习
第9章 聚类
第10章 降维与度量学习
第11章 特征选择与稀疏学习
第12章 计算学习理论
第13章 半监督学习
第14章 概率图模型
第15章 规则学习
第16章 强化学习


版权信息
书名:机器学习公式详解
ISBN:978-7-115-55910-4
本书由人民邮电出版社发行数字版。版权所有,侵权必究。
您购买的人民邮电出版社电子书仅供您个人使用,未经授权,不得
以任何方式复制和传播本书内容。
我们愿意相信读者具有这样的良知和觉悟,与我们共同保护知识产
权。
如果购买者有侵权行为,我们可能对该用户实施包括但不限于关闭
该帐号等维权措施,并可能追究法律责任。


版权
编 著 谢文睿 秦 州
译 郭媛
责任编辑
人民邮电出版社出版发行 北京市丰台区成寿寺路11号
邮编 100164 电子邮件 315@ptpress.com.cn
网址 http://www.ptpress.com.cn
读者服务热线:(010)81055410
反盗版热线:(010)81055315


内容提要
周志华老师的《机器学习》(俗称“西瓜书”)是机器学习领域的经
典入门教材之一. 本书(俗称“南瓜书”)基于Datawhale成员自学“西瓜
书”时记下的笔记编著而成,旨在对“西瓜书”中重、难点公式加以解
析,以及对部分公式补充具体的推导细节.
全书共16章,与“西瓜书”章节、公式对应,每个公式的推导和解析
都以本科数学基础的视角进行讲解,希望能够帮助读者达到“理工科数
学基础扎实点的大二下学期学生”水平. 每章都附有相关阅读材料,以便
有兴趣的读者进一步钻研探索.
本书思路清晰,视角独特,结构合理,可作为高等院校计算机及相
关专业的本科生或研究生教材,也可供对机器学习感兴趣的研究人员和
工程技术人员阅读参考.


序
虽然与本书的编著者素不相识、从未谋面,但是看过书稿之后,我
便很乐意也感觉很荣幸有机会给这本书写序.
这是一本与众不同的书.
首先,确切地说,这是一本“伴侣书”. 类似于咖啡伴侣一样,这本
书是周志华教授的“西瓜书”——《机器学习》的伴侣书, 它也有一个可
爱的名字——“南瓜书”. “南瓜书”对“西瓜书”中的公式进行了解析,并补
充了必要的推导过程;在推导公式的过程中有时候会需要一些先验知
识,编著者也进行了必要的补充. 上述做法对学习机器学习时“知其
然”并“知其所以然”非常重要. 现在能用一些机器学习工具来实现某个任
务的人越来越多了,但是具有机器学习思维且了解其原理从而能够解决
实际问题的能力在工作中更重要,具有这种能力的人也更具有竞争力.
其次,这是一本通过开源方式多人协作写成的书. 这种多人分工合
作、互相校验、开放监督的方式,既保证了书的质量,也保证了写作的
效率. 在我看来,这是一种站在读者角度且非常先进的生产方式,容易
给读者带来很好的体验.
最后,我想说这是一本完全根据学习经历编著而成的书. 也就是
说,这本书完全从读者学习的角度出发,分享编著者在学习中遇到
的“坑”以及跳过这个“坑”的方法,这对初学者来说是非常宝贵的经验,


也特别能够引起他们的共鸣. 其实,每个人在学习一门新的课程时,都
会有自己独特的经验和方法. 这种经验和方法的共享非常难能可贵. 在这
里,理解公式便是编著者认为了解机器学习原理的最好方法,其实对于
这一点我也深表赞同,因为在学习中我就是那种喜欢推导公式的典型代
表,只有公式推导成功,才觉得对知识的原理理解得更深刻,否则总是
觉得心里不踏实.
对于本书,我有几点阅读建议,供大家参考.
首先,这本 “南瓜书”要和“西瓜书”配套阅读,如果在阅读“西瓜
书”时对公式疑惑或对概念理解不畅,可以通过“南瓜书”快速定位公式
并进行推导,从而深入理解. 从这个意义来说,“南瓜书”可以看成是“西
瓜书”的公式字典.
其次,阅读时一定要克服对公式的排斥或者畏惧心理. 公式是通过
符号对原理本质的高度概括,是一种精简而美丽的数学语言. 推几个公
式之后,相信读者会从中感觉到没有体验过的乐趣.
最后,这本书非常偏技术原理,看上去也有点儿枯燥,阅读时读者
还是要事先做好克服困难的准备. 有时,即使编著者给出了推导过程,
读者也不一定一眼就能理解,这就需要自己静下心来仔细研读. 只有这
样,才有可能成为具有机器学习思维而不只是会用机器学习工具的人.
祝大家阅读愉快!
王斌


小米AI实验室主任、NLP首席科学家


前言
由于国内相关资料的匮乏,机器学习算法的公式推导历来都被认为
是初学者的“噩梦”. 笔者两年前也受到了相同的困扰,但是在笔者师兄
的鼓励下,笔者开始尝试做读书笔记, 经年累月遂有了编著本书的基
本素材. 本书就是以笔者拜读周志华老师的《机器学习》 (俗称“西瓜
书”)时记下的笔 记为蓝本编著的. “西瓜书”作为机器学习领域的经典中
文著作, 已经成为相关从业人员和学习者的必读 书目. 周老师为了兼顾
更多读者,在“西瓜书”中尽可能少地使用数学知识. 然而这对笔者这 类
对公式推导感兴趣的读者来说就颇费思量. 为此,本书便在“西瓜书”的
基础上,对其中的 重难点公式进行一些补充. 具体地说,本书会对“西瓜
书”中缺少推导细节的公式补充了详细的推导过程, 对不太易懂的公式
补充解析.
全书的章节编排和“西瓜书”保持一致,共16章,各章中的内容都对
应“西瓜书”中相应章节与公 式. 为了尽可能地降低阅读门槛,本书以本
科数学视角编写,所以有本科数学基础的读者 基本都能畅读本书. 对于
超过本科数学范围的数学知识,本书都会在相应章节附上详细讲 解的
附注,以及具体的参考文献,读者可以按图索骥,拓展阅读. 由于本书
主要是对“西瓜 书”进行的补充,所以在编写具体章节内容时,默认读者
已经阅读过“西瓜书”相应章节.
本书需要搭配“西瓜书”一起阅读. 在阅读“西瓜书”的过程中,当遇


到推导不明白的公 式时再来查阅本书,效果最佳.
本书是由开源组织Datawhale的成员采用开源协作的方式完成,参
与者包括2位主要编著 者(谢文睿和秦州)、6位编委会成员(贾彬
彬、居凤霞、马晶敏、胡风范、周天烁和叶梁)、 12位特别贡献成员
(awyd234、feijuan、Ggmatch、Heitao5200、huaqing89、LongJH、
LilRachel、LeoLRH、 Nono17、spareribs、\linebreak sunchaothu和
StevenLzq).
本书可作为《机器学习》一书的配套读物,读者也可以 将其视
为“一份现学现卖的读书笔记”. 由于编者水平有限,书中难免有所纰漏
和表述不当的地方,还望各位读 者批评指正.
谢文睿
2020年12月27日


主要符号表
标量
向量
变量集
矩阵
单位阵
样本空间或状态空间
概率分布
数据样本(数据集)
假设空间
假设集
学习算法
行向量
列向量


向量或矩阵转置
集合
集合 中元素个数
范数, 缺省时为L 范数
概率质量函数, 条件概率质量函数
概率密度函数, 条件概率密度函数
函数 )对 在分布 下的数学期望; 意义明确时将省略 和
(或).
上确界
指示函数, 在 为真和假时分别取值为1, 0
符号函数, 在 时分别取值为


第1章 绪论
式(1.1)
参见式 (1.2)
式(1.2)
1
2
3
4
5
3 5显然成立
解析


1 2:
2 3:首先要知道此时我们假设 是任何能将样本映射到{0,1}的
函数.存在不止一个 时, 服从均匀分布,即每个 出现的概率相等.例如
样本空间只有两个样本时, .那么所有可能的真实目
标函数 如下:
一共 个可能的真实目标函数.所以此时通过算法 学习
出来的模型 对每个样本无论预测值为0还是1,都必然有一半的 与之
预测值相等.例如,现在学出来的模型 对 的预测值为1,即
,那么有且只有 和 与 的预测值相等,也就是有且只有一半的 与
它预测值相等,所以 .
值得一提的是,在这里我们假设真实的目标函数 服从均匀分布,
但是实际情形并非如此,通常我们只认为能高度拟合已有样本数据的函
数才是真实目标函数,例如,现在已有的样本数据为 ,
那么此时 才是我们认为的真实目标函数,由于没有收集到或者压根不


存在 这类样本,所以
都不算是真实目标函数.这也就是“西瓜书”式(1.3)下面的第3段
中“骑自行车”的例子所想表达的内容.


第2章 模型评估与选择
式(2.20)
解析
在解释 式之前,需要先弄清楚 曲线的具体绘制过程.下面我
们就举个例子,按照“西瓜书”图2.4下方给出的绘制方法来讲解一下
曲线的具体绘制过程.
假设我们已经训练得到一个学习器 ,现在用该学习器来对8个测
试样本(4个正例,4个反例,即 )进行预测,预测结果
为:
此处用 表示样本, 以和坐标 作出区分
其中, 和 分别表示样本为正例和为反例,数字表示学习器 预测
该样本为正例的概率,例如对于反例 来说, 当前学习器 预测它是
正例的概率为 .
上面给出的预测结果已经按照预测值从大到小排序


根据“西瓜书”上给出的绘制方法,首先需要对所有测试样本按照学
习器给出的预测结果进行排序,接着将分类阈值设为一个不可能取到的
最大值.显然,此时所有样本预测为正例的概率都一定小于分类阈值,
那么预测为正例的样本个数为0,相应的真正例率和假正例率也都为0,
所以我们可以在坐标 处标记一个点. 接下来需要把分类阈值从大到
小依次设为每个样本的预测值,也就是依次设为0.77, 0.62, 0.58, 0.47,
0.33, 0.23,0.15,然后分别计算真正例率和假正例率,再在相应的坐标上
标记点,最后再将各个点用直线连接, 即可得到 曲线.需要注意的
是,在统计预测结果时,预测值等于分类阈值的样本也被算作预测为正
例. 例如,当分类阈值为 时,测试样本 被预测为正例,由于它的真
实标记也是正例,所以此时 是一个真正例.为了便于绘图,我们将 轴
(假正例率轴)的“步长”定为 , 轴(真正例率轴)的“步长”定为 .
根据真正例率和假正例率的定义可知,每次变动分类阈值时,若新增
个假正例,那么相应的 轴坐标也就增加 ;若新增 个真正例,那么
相应的 轴坐标也就增加 .按照以上讲述的绘制流程,最终我们可以绘
制出如图2-1所示的 曲线.


图2-1 ROC曲线示意
注: 表示红色线段; 表示蓝色线段; 表示绿色线段
在这里,为了能在解析式(2.21)时复用此图,我们没有写上具体的
数值,转而用其数学符号代替.其中绿色线段表示在分类阈值变动的过
程中只新增了真正例,红色线段表示只新增了假正例,蓝色线段表示既
新增了真正例也新增了假正例.根据 值的定义可知,此时的 值其
实就是所有红色线段和蓝色线段与 轴围成的面积之和.观察图2-1可知,
红色线段与 轴围成的图形恒为矩形,蓝色线段与 轴围成的图形恒为梯
形.由于梯形面积式既能算梯形面积,也能算矩形面积,所以无论是红
色线段还是蓝色线段,其与 轴围成的面积都能用梯形公式来计算:
其中, 为“高”, 为“上底”, 为“下底”.那么对所有红色
线段和蓝色线段与 轴围成的面积进行求和,则有


此即 .
式(2.21)
解析
按照我们上述对式(2.20)的解析思路, 可以看作是所有绿色线
段和蓝色线段与 轴围成的面积之和,但从式(2.21)中很难一眼看出其面
积的具体计算方式,因此我们进行恒等变形如下:


在变动分类阈值的过程当中,如果有新增真正例,那么图2-1就会
相应地增加一条绿色线段或蓝色线段,所以上式中的 可以看作是
在累加所有绿色和蓝色线段,相应地, 后面的内容便是在求绿色
线段或者蓝色线段与 轴围成的面积,即:
与式(2.20)中的求解思路相同,不论是绿色线段还是蓝色线段,其
与 轴围成的图形面积都可以用梯形公式来进行计算,所以上式表示的
依旧是一个梯形的面积公式.其中 即梯形的“高”,中括号内便是“上底
+下底”,下面我们来分别推导一下“上底”(较短的底)和“下底”(较长
的底).
由于在绘制 曲线的过程中,每新增一个假正例时 坐标也就新
增一个步长,所以对于“上底”,也就是绿色或者蓝色线段的下端点到
轴的距离,长度就等于 乘以预测值大于 的假正例的个数,即
而对于“下底”,长度就等于 乘以预测值大于等于 的假正例
的个数,即


式(2.27)
解析
截至2018年12月“西瓜书”第1版第30次印刷,式(2.27)应当勘误为
具体推导过程如下:由“西瓜书”中的上下文可知,对 进行假设
检验,等价于本章附注中所述的对 进行假设检验,所以在“西瓜
书”中求解最大错误率 等价于在附注中求解事件最大发生频率 . 由附注
可知
所以
将上式中的 等价替换为 可得


式(2.41)
1
2
3
4
5
6
7
1 2:减一个 再加一个 ,属于简单的恒等变形
4 5:同1 2一样,减一个 再加一个 ,属于简单的恒等变形
5 6:同2 3一样, 将最后一项利用期望的运算性质进行展开
解析
2 3:首先将中括号内的式子展开,有


然后根据期望的运算性质 可将上式化为
3 4:再次利用期望的运算性质将第3步得到的式子的最后一项
展开,有
首先计算展开后得到的第1项,有
由于 是常量,所以由期望的运算性质:
(其中 均为常量)可得
由式(2.37)可知 ,所以
接着计算展开后得到的第二项
由于噪声和 无关,所以 和 是两个相互独立的随机变量. 根
据期望的运算性质 (其中 和 为相互独立的随机变


量)可得
所以
6 7:因为 和 均为常量,根据期望的运算性质,有6中的第
2项
同理有6中的最后一项
由于此时假定噪声的期望为零,即 ,所以


附注
二项分布参数 的检验[1]
设某事件发生的概率为 , 未知.做 次独立试验,每次观察该事件
是否发生,以 记该事件发生的次数,则 服从二项分布 ,现根
据 检验如下假设:
由二项分布本身的特性可知: 越小, 取到较小值的概率越大.因
此,对于上述假设,一个直观上合理的检验为
:当 时接受 ,否则就拒绝
其中, 表示事件最大发生次数.此检验对应的功效函数为
由于“ 越小, 取到较小值的概率越大”可以等价表示为:
是关于 的减函数,所以 是关
于 的增函数,那么当 时, 即 的上确界.又根据参考文献


[1]中5.1.3的定义1.2可知,检验水平 默认取最小可能的水平,所以在给
定检验水平 时,可以通过如下方程解得满足检验水平 的整数 :
更为严格的数学证明参见参考文献[1]中第二章习题7
显然,当 时有
对于此方程,通常不一定正好解得一个使得方程成立的整数 ,较
常见的情况是存在这样一个 使得
;
此时, 只能取 或者 .若 取 ,则相当于升高了检验水平 ;
若 取 则相当于降低了检验水平 .具体如何取舍需要结合实际情
况,但是通常为了减小犯第一类错误的概率,会倾向于令 取 .
下面考虑如何求解 .易证 是关于 的减函数,再结合上述关于
的两个不等式易推得




参考文献
[1]陈希孺.概率论与数理统计[M].合肥:中国科学技术大学出版
社,2009.


第3章 线性模型
式 (3.5)
解析
已知 ,所以


式(3.6)
解析
已知 ,所以


式(3.7)
解析
令式(3.5)等于0,有
由于令式(3.6)等于0可得 ,又因为 且
,则 ,代入上式可得


将和
代入上式,即可得式(3.7):
如果要想用Python来实现上式的话,上式中的求和运算只能用循环
来实现.但是如果能将上式向量化,也就是转换成矩阵(即向量)运算
的话,我们就可以利用诸如NumPy这种专门加速矩阵运算的类库来进行
编写. 下面我们就尝试将上式进行向量化.
将 代入分母可得
又因为 且


,则有
若令 , 为去均值
后的 ; , 为去均值后
的 ,代入上式可得
、 、 、 均为 行1列的列向量
式(3.10)
解析
将 展开可得
对 求导可得


矩阵微分式 ,
式(3.27)
解析
将式(3.26)代入式(3.25)可得
其中 , ,代入上式可得
由于 =0或1,则
两式综合可得


由于此式仍为极大似然估计的似然函数,所以最大化似然函数等价
于最小化似然函数的相反数,即在似然函数前添加负号即可得式(3.27).
值得一提的是,若将式(3.26)改写为
,再代入式(3.25)可得
显然,此种方式更易推导出式(3.27).
式(3.30)


解析
此式可以进行向量化. 令 ,代入上式得
式(3.32)
解析


式(3.37)
解析
由式(3.36),可定义拉格朗日函数为
对 求偏导可得
.
这是由于 、
令上式等于0即可得
由于我们想要求解的只有 ,而拉格朗日乘子 具体取值多少都无
所谓,因此可以任意设定 来配合我们求解 . 注意到


如果我们令 ,那么上式即可改写为
将其代入 即可解得
式(3.38)
参见式(3.37)
式(3.39)
参见式(3.37)
式(3.43)
解析
由式(3.40)、式(3.41)、式(3.42)可得:


式(3.44)


解析
此式是式(3.35)的推广形式,证明如下.
设 ,其中 为 行1
列的列向量,则
所以式(3.44)可变形为
对比式(3.35)易知,上式即式(3.35)的推广形式.
式(3.45)
解析
同式(3.35),此处也固定式(3.44)的分母为1,那么式(3.44)此时等价
于如下优化问题


根据拉格朗日乘子法,可定义上述优化问题的拉格朗日函数
根据矩阵微分式 对上式关于 求偏导
可得
.
这是由于 且
令上式等于 即可得


第4章 决策树
式(4.1)
解析
下面证明 .
已知集合 的信息熵的定义为
其中, 表示样本类别总数, 表示第 类样本所占的比例,有
.若令 ,那么信息熵 就可以看作
一个 元实值函数,即
其中 .
下面考虑求该多元函数的最值. 首先我们先来求最大值,如果不考


虑约束 而仅考虑 ,则对 求最大值等价于
如下最小化问题:
显然,在 时,此问题为凸优化问题.对于凸优化问题来
说,使其拉格朗日函数的一阶偏导数等于0的点即最优解.根据拉格朗日
乘子法可知,该优化问题的拉格朗日函数为
其中, 为拉格朗日乘子.对 分别关于 求一
阶偏导数,并令偏导数等于0可得
;


;
...
;
.
整理一下可得
由以上两个方程可以解得
又因为 还需满足约束 ,显然 ,所以
是满足所有约束的最优解,即当前最小化问题的最


小值点,同时也是 的最大值点.将 代入
中可得
所以 在满足约束 时的最大值为
.
下面求最小值. 如果不考虑约束 而仅考虑 ,则
可以看作 个互不相关的一元函数的和,即
其中, .那么当 分别
取到其最小值时, 也就取到了最小值.所以接下来考虑分别
求 各自的最小值,由于 的定义
域和函数表达式均相同,所以只需求出 的最小值也就求出了
的最小值.下面考虑求 的最小值,首先对 关于
求一阶和二阶导数,有


显然,当 时 恒小于0,所以 是一个在
其定义域范围内开口向下的凹函数,那么其最小值必然在边界取.分别
取 和 ,代入 可得
计算信息熵时约定:若 ,则
所以, 的最小值为0,同理可得 的最小值也都为
0,即 的最小值为0. 但是, 此时仅考虑约束 ,而未
考虑 .若考虑约束 ,那么 的最小值一定大
于等于0.如果令某个 ,那么根据约束 可知
,将其代入 可得
所以 一定是
在满足约束 和 的条件下的最小值点,此
时 取到最小值0.
综上可知,当 取到最大值时: ,
此时样本集合纯度最低;当 取到最小值时:


,此时样本集合纯度最
高.
式(4.2)
解析
此为信息增益的定义式.在信息论中信息增益也称为互信息(参见
本章附注1),表示已知一个随机变量的信息后另一个随机变量的不确
定性减少的程度.所以此式可以理解为,在已知属性 的取值后,样本类
别这个随机变量的不确定性减小的程度.若根据某个属性计算得到的信
息增益越大,则说明在知道其取值后样本集的不确定性减小的程度越
大,即“西瓜书”上所说的“纯度提升”越大.
式(4.6)
解析
此为数据集 中属性 的基尼指数的定义,表示在属性 的取值已知
的条件下,数据集 按照属性 的所有可能取值划分后的纯度.不过在构
造CART决策树时并不会严格按照此式来选择最优划分属性,主要是因
为CART决策树是一棵二叉树,如果用上面的式去选出最优划分属性,


无法进一步选出最优划分属性的最优划分点.常用的CART决策树的构造
算法如下:
(1) 考虑每个属性 的每个可能取值 ,将数据集 分为 和
两部分来计算基尼指数,即
(2) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最
优划分点;
(3) 重复以上两步,直至满足停止条件.
下面以“西瓜书”中表4.2中西瓜数据集2.0为例来构造CART决策树,
其中第一个最优划分属性和最优划分点的计算过程如下:以属性“色
泽”为例,它有3个可能的取值:{青绿},{乌黑},{浅白},若使用该属
性的属性值是否等于“青绿”对数据集 进行划分,则可得到2个子集,分
别记为 (色泽=青绿), (色泽 青绿).子集 包含编号 共
6个样例,其中正例占 ,反例占 ;子集 包含编号
共11个样例,其中正例占 ,反例占
,根据式(4.5)可计算出用“色泽=青绿”划分之后得到基尼指数为
( ,色泽=青绿)


类似地,可以计算出不同属性取不同值的基尼指数如下:
Gini_index( ,色泽=乌黑)=0.456
Gini_index( ,色泽=浅白)=0.426
Gini_index( ,根蒂=蜷缩) =0.456
Gini_index( ,根蒂=稍蜷) =0.496
Gini_index( ,根蒂=硬挺) =0.439
Gini_index( ,敲声=浊响) =0.450
Gini_index( ,敲声=沉闷) =0.494
Gini_index( ,敲声=清脆) =0.439
Gini_index( ,纹理=清晰) =0.286
Gini_index( ,纹理=稍稀) =0.437
Gini_index( ,纹理=模糊) =0.403
Gini_index( ,脐部=凹陷) =0.415
Gini_index( ,脐部=稍凹) =0.497
Gini_index( ,脐部=平坦) =0.362


Gini_index( ,触感=硬挺) =0.494
Gini_index( ,触感=软粘) = 0.494.
特别地,对于属性“触感”,由于它的可取值个数为2,所以其实只
需计算其中一个取值的基尼指数即可.根据上面的计算结果可知,
Gini_index( ,纹理=清晰) =0.286最小,所以选择属性“纹理”为最优划分
属性并生成根节点,接着以“纹理=清晰”为最优划分点生成 (纹理=清
晰)、 (纹理 清晰)两个子节点,对两个子节点分别重复上述步骤继续
生成下一层子节点,直至满足停止条件.
以上便是CART决策树的构建过程,从构建过程可以看出,CART
决策树最终构造出来的是一棵二叉树.CART除了决策树能处理分类问题
以外,回归树还可以处理回归问题,附注2中给出了CART回归树的构
造算法.
式(4.7)
解析
此式所表达的思想很简单,就是以每两个相邻取值的中点作为划分
点.
下面以“西瓜书”中表4.3中西瓜数据集3.0为例来说明此式的用法.对
于“密度”这个连续属性,已观测到的可能取值为


{0.243,0.245,0.343,0.360,0.403,0.437,
0.481,0.556,0.593,0.608,0.634,0.639,0.657,0.666,0.697,0.719,0.774}共17个
值,根据式(4.7)可知,此时 依次取1到16,那么“密度”这个属性的候选
划分点集合为
式(4.8)
解析
此式是式(4.2)用于离散化后的连续属性的版本,其中 由式(4.7)计
算得来, 表示属性 的取值分别小于等于和大于候选划分点 时
的情形,即当 时有 ,当 时有 .


附注
1互信息[1]
在解释互信息之前,需要先解释一下什么是条件熵.条件熵表示的
是在已知一个随机变量的条件下,另一个随机变量的不确定性.具体
地,假设有随机变量 和 ,且它们服从以下联合概率分布
那么在已知 的条件下,随机变量 的条件熵为
其中, .互信息定义为信息熵和条件熵的
差,它表示的是已知一个随机变量的信息后使得另一个随机变量的不确
定性减少的程度.具体地,假设有随机变量 和 ,那么在已知 的信息
后, 的不确定性减少的程度为
此即互信息的数学定义.
2CART回归树[1]
假设给定数据集


其中 为 维特征向量, 是连续型随机变量. 这是一个标准
的回归问题的数据集,若把每个属性视为坐标空间中的一个坐标轴,则
个属性就构成了一个 维的特征空间,而每个 维特征向量 就对应了 维
的特征空间中的一个数据点.CART回归树的目标是将特征空间划分成若
干个子空间,每个子空间都有一个固定的输出值,也就是凡是落在同一
个子空间内的数据点 ,它们所对应的输出值 恒相等,且都为该子空
间的输出值.那么如何划分出若干个子空间呢?这里采用一种启发式的
方法.
(1) 任意选择一个属性 ,遍历其所有可能取值,根据下式找出属性
最优划分点 :
其中, , 和 分别
为集合 和 中的样本 对应的输出值 的均值,即
(2) 遍历所有属性,找到最优划分属性 ,然后根据 的最优划分点
将特征空间划分为两个子空间,接着对每个子空间重复上述步骤,直
至满足停止条件.这样就生成了一棵CART回归树,假设最终将特征空间
划分为 个子空间 ,那么CART回归树的模型式可以表示


为
同理,其中的 表示的也是集合 中的样本 对应的输出值 的均
值.此式直观上的理解就是,对于一个给定的样本 ,首先判断其属于哪
个子空间,然后将其所属的子空间对应的输出值作为该样本的预测值 .


参考文献
[1] 李航. 统计学习方法[M]. 北京:清华大学出版社, 2012.


第5章 神经网络
式(5.2)
解析
此式是感知机学习算法中的参数更新式,下面依次给出感知机模
型、学习策略和学习算法的具体介绍[1].
感知机模型
已知感知机由两层神经元组成,故感知机模型的式可表示为
其中, ,为样本的特征向量,是感知机模型的输入; 是
感知机模型的参数, ,为权重, 为阈值.假定 为阶跃函数,那么
感知机模型的式可进一步表示为
用 代表阶跃函数
由于 维空间中的超平面方程为


所以此时感知机模型式中的 可以看作是 维空间中的一个超
平面,将 维空间划分为 和 两个子空间,落在前
一个子空间的样本对应的模型输出值为1,落在后一个子空间的样本对
应的模型输出值为0,如此便实现了分类功能.
感知机学习策略
给定一个线性可分的数据集 (参见本章附注),感知机的学习目
标是求得能对数据集 中的正负样本完全正确划分的分离超平面
假设此时误分类样本集合为 ,对任意一个误分类样本
来说,当 时,模型输出值为 ,样本真实标记
为 ;反之,当 时,模型输出值为 ,样本真实标记
为 .综合两种情形可知,以下式恒成立:
所以,给定数据集 ,其损失函数可以定义为
显然,此损失函数是非负的. 如果没有误分类点,则损失函数值为
0.而且,误分类点越少,误分类点离超平面越近,损失函数值就越小.因
此,给定数据集 ,损失函数 是关于 的连续可导函数.


感知机学习算法
感知机模型的学习问题可以转化为求解损失函数的最优化问题,具
体地,给定数据集
其中 ,求参数 ,使其为极小化损失函数的解:
其中 为误分类样本集合.若将阈值 看作一个固定输入为
的“哑节点”,即
那么 可化简为
其中 . 根据该式,可将要求解的极小化问题进一
步简化为


假设误分类样本集合 固定,那么可以求得损失函数 的梯度
感知机的学习算法具体采用的是随机梯度下降法,即在极小化过程
中,不是一次使 中所有误分类点的梯度下降,而是一次随机选取一个
误分类点并使其梯度下降.所以权重 的更新式为
相应地, 中的某个分量 的更新式即式(5.2).
式(5.10)
参见式(5.12)
式(5.12)
解析
因为


又
所以
式(5.13)


解析
因为
又
所以


式(5.14)
解析
因为
又
所以


式(5.15)
参见式(5.13)
式(5.20)
解析
Boltzmann机(Restricted Boltzmann Machin e,简称RBM)本质上
是一个引入了隐变量的无向图模型,其能量可理解为
其中, 表示图的能量, 表示图中边的能量, 表示图
中结点的能量.边能量由两连接结点的值及其权重的乘积确定, 即


;结点能量由结点的值及其阈值的乘积确定,即
.图中边的能量为所有边能量之和为
图中结点的能量为所有结点能量之和
故状态向量 所对应的Boltzmann机能量
式(5.22)
解析
受限Boltzmann机仅保留显层与隐层之间的连接.显层状态向量
,隐层状态向量 .显层状态向量 中的
变量 仅与隐层状态向量 有关,所以给定隐层状态向量 ,有
相互独立.
式(5.23)


解析
由式(5.22)的解析同理可得,给定显层状态向量 ,有 相
互独立.
式(5.24)
解析
由式(5.20)可推导出受限Boltzmann机的能量函数
其中
再由式(5.21)可知,RBM的联合概率分布


其中 为规范化因子
给定含 个独立同分布数据的数据集 ,记
.学习RBM的策略是求出参数 的值,使得如下对数似然函
数最大化:
具体地,采用梯度上升法来求解参数 ,因此考虑求对数似然函数
的梯度.对于 中的任意一个样本 ,其对应似然函数


,
对 求导有
.
由于
,
,
故


.
包含三个参数,在这里我们仅以 中的任意一个分量
为例进行详细推导.首先将上式中的 替换为 可得
根据式(5.23)可知


同理可推得
将以上两式代回 中可得
观察此式可知,通过枚举所有可能的 来计算
的复杂度太高,因此可以考虑求其近似值来简化计算.具体地,RBM通
常采用的是“西瓜书”上所说的“对比散度”(Contrastive Divergence,简
称CD)算法.CD算法的核心思想是:用步长为 (通常设为1)的CD函
数
读者可参阅“皮果提”发布于CSDN的文章《受限玻尔兹曼机
(RBM)学习笔记(六)对比散度算法》
近似代替
对于 来说,即用


近似代替
令 , 表示参数为 的RBM网络,则
的具体算法如图5-1所示.
输入:步长 ;
数据集 ;
参数为 的RBM网络RBM .
过程:
1:初始化
2:for do
3:
4: for do
5:
6:
7: end for


8: for do
9:
10: end for
11:end for
输出:
图5-1 CD算法
图5-1中函数 表示在给定 的条件下,从
中采样生成 ,同理,函数 表示在给定 的条件下,
从 中采样生成 .由于两个函数的算法可以互相类比推得,因此仅
给出函数 的具体算法,如图5-2所示.
综上可知,式(5.24)其实就是带有学习率 的 的一种形式化表示.
输入:显层状态向量 ;
参数为 的 网络RBM .
过程:
1: for do
2: 随机生成


3:
4: end for
输出:
图5-2 算法


附注
数据集的线性可分[1]
给定一个数据集
其中 . 如果存在某个超平面
能将数据集 中的正样本和负样本完全正确地划分到超平面两侧,
即对所有 的样本 有 ,对所有 的样本 有
,则称数据集 线性可分,否则称数据集 线性不可分.


参考文献
[1] 李航. 统计学习方法[M]. 北京:清华大学出版社, 2012.


第6章 支持向量机
式(6.9)
解析
式(6.8)可作如下展开:
对 和 分别求偏导数并分别令其等于0和0,有
值得一提的是,上述求解过程遵循的是“西瓜书”附录B中式(B.7)左
侧的那段话:“在推导对偶问题时,常通过将拉格朗日函数 对


求导并令导数为0,来获得对偶函数的表达形式.”那么这段话背后的缘由
是什么呢?在这里我们认为有两种说法可以进行解释:
(1) 对于强对偶性成立的优化问题,其主问题的最优解 一定满足
本章附注给出的KKT条件,而KKT条件中的条件(1)就要求最优解 能使
得拉格朗日函数 关于 的一阶导数等于0;
证明参见参考文献[1]的5.5节
(2) 对于任意优化问题,若拉格朗日函数 是关于 的凸函
数,那么此时对 关于 求导并令导数等于0解出来的点一定是最
小值点. 根据对偶函数的定义,将最小值点代回 即可得到对偶
函数.
显然,对于SVM来说,从以上任意一种说法都能解释得通.
式(6.10)
参见式(6.9)
式(6.11)


s.t. ,
解析
将式(6.9)和式(6.10)代入式(6.8)即可将 中的 和 消去,再
考虑式(6.10)的约束,就得到了式(6.6)的对偶问题:
由于 ,所以上式最后一项可化为0,于是得
,


所以
式(6.13)
参见式(6.9)中给出的第1点理由
式(6.35)
s.t. ,
解析
令
显然 . 且当 时有
当 时有


综上可得
式(6.37)
参见式(6.9)
式(6.38)
参见式(6.10)
式(6.39)
解析
式(6.36)关于 求偏导并令其等于0可得
式(6.40)


s.t. ,
解析
将式(6.37) (6.39)代入式(6.36)可以得到式(6.35)的对偶问题,有
,
所以


.
又因为 , , ,消去 可得等价约束条件
式(6.41)
参见式(6.13)
式(6.52)
解析
将式(6.45)的约束条件全部恒等变形为小于等于0的形式可得
由于以上四个约束条件的拉格朗日乘子分别为 ,由本章


附注可知,以上四个约束条件可相应转化为KKT条件
又由式(6.49)和式(6.50)有
所以上述KKT条件可以进一步变形为
又因为样本 只可能处在间隔带的某一侧,即约束条件
和 不可能同时成立,所以 和
中至少有一个为0,即 .
在此基础上再进一步分析可知,如果 ,则根据约束
可知此时 .同理,如果 ,则根据约束
可知此时 .所以 和 中也是至少有一个为0,即 .
将 整合进上述KKT条件中即可得到式(6.52).
式(6.60)


类似于第3章的式(3.35)
式(6.62)
类似于第3章的式(3.34)
式(6.63)
类似于第3章的式(3.33)
式(6.65)
解析
由表示定理可知,此时二分类KLDA最终求得的投影直线方程总可
以写成如下形式:
又因为直线方程的固定形式为


所以
将 代入可得
.
由于 的计算结果为标量,而标量的转置等于其本身,所以
即
式(6.66)
解析
为了详细地说明此式的计算原理,下面首先举例说明,然后再在例


子的基础上延展出其一般形式.
假设此时仅有4个样本,其中第1和第3个样本的标记为0,第2和第4
个样本的标记为1,那么此时有
;
;
所以
,


根据此结果易得 的一般形式为
式(6.67)
参见式(6.66)的解析
式(6.70)
解析


此式是将式(6.65)代入式(6.60)后推得而来的,下面给出详细地推导
过程.
首先将式(6.65)代入式(6.60)的分子可得
,
其中
.
将其代入上式可得


.
由于 为标量,所以其转置等于本身,即
. 将其代入
上式可得
.
设 ,同时结合式(6.66)的解析可得到
的一般形式,上式可以化简为
以上便是式(6.70)分子部分的推导,下面继续推导式(6.70)的分母部
分.
将式(6.65)代入式(6.60)的分母可得:


其中
由于
且


所以
再将此式代回 可得


其中,第1项
第2项
同理,有第3项


将上述三项的化简结果代回再将此式代回 可得


附注
KKT条件[2]
考虑一般的约束优化问题
其中,自变量 . 设 具有连续的一阶偏导数,
是优化问题的局部可行解.若该优化问题满足任意一个约束限制条件
(constrain t qualifications or regularity conditions),则一定存在
,使得:
参见维基百科页面“Karush-kuhn-tucker conditions”
(1)
(2)
(3)
(4)
(5) ;
其中 为拉格朗日函数


以上5条即KKT条件,严格数学证明参见参考文献[2]的4.2.1小节.


参考文献
[1] 王书宁. 凸优化[M].北京:清华大学出版社, 2013.
[2] 王燕军. 最优化基础理论与方法[M]. 上海:复旦大学出版社,
2011.


第7章 贝叶斯分类器
式(7.5)
解析
由式(7.1)和式(7.4)可得
又 ,则
此即式(7.5).
式(7.6)
将式(7.5)代入式(7.3)即可推得此式
式(7.12)


参见式(7.13)
式(7.13)
解析
根据式(7.11)和式(7.10)可知参数求解式为
由“西瓜书”上下文可知,此时假设概率密度函数
,其等价于假设
其中, 表示 的维数, 为对称正定协方差矩阵, 表示
的行列式.将其代入参数求解式可得


.
假设此时数据集 中的样本个数为 ,即 ,则上式可以改写
为
.
为了便于分别求解 和 ,在这里我们根据式 和
将上式中的最后一项作如下恒等变形:


.
所以
.
观察上式可知,由于此时 和 一样均为正定矩阵,所以当
时,上式最后一项为正定二次型.根据正定二次型的性质可
知,此时上式最后一项的取值仅与 相关,并有当且仅当