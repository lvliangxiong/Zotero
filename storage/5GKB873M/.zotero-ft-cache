
        
            
                
            
        
    THE EMPEROR’S NEW MINDRoger Penrose is the Emeritus Rouse Ball Professor of Mathematics at the University of Oxford and Gresham Professor of Geometry, Gresham College, London. He has a part-time appointment as Francis and Helen Pentz Distinguished Professor of Physics and Mathematics, Penn State University, USA. Born in Colchester, Essex, in 1931, he attended University College School, London, was awarded a B.Sc. degree at University College London and a Ph.D. at St John’s College, Cambridge. He has held several posts in the UK and USA, most particularly at Birkbeck College, London. He was elected a Fellow of the Royal Society of London in 1972 and a Foreign Associate of the United States National Academy of Sciences in 1998. He has received a number of prizes and awards including the 1988 Wolf Prize, which he shared with Stephen Hawking for their understanding of the universe, the Dannie Heinemann Prize, the Royal Society Royal Medal, the Dirac Medal, and the Albert Einstein prize. His 1989 book The Emperor’s New Mind became a bestseller and won the 1990 (now Rhône-Poulenc) Science Book Prize. His latest books are Shadows of the Mind (1994), The Nature of Space and Time (1996) with Stephen Hawking, and The Large, the Small and the Human Mind (1997).He has research interests in many aspects of geometry, having made contributions to the theory of non-periodic tilings, to general relativity theory, and the foundations of quantum theory. He has contributed to the science of consciousness. His main research programme is to develop the theory of twistors, which he originated over thirty years ago as an attempt to unite Einstein’s general theory of relativity with quantum mechanics. He was knighted in 1994 for services to science.

Other books by Roger PenroseShadows of the Mind TheLarge, the Small and the Human MindWith Stephen HawkingThe Nature of Space and TimeWith Wolfgang RindlerSpinors and Space-Time, Vols. 1 and 2

Roger PenroseTHE EMPEROR’S NEW MINDConcerning Computers, Minds and The Laws of PhysicsFOREWORD BYMartin Gardner

Great Clarendon Street, Oxford OX2 6DPOxford University Press is a department of the University of Oxford.It furthers the University’s objective of excellence in research, scholarship,and education by publishing worldwide inOxford New YorkAuckland Cape Town Dar es Salaam Hong Kong Karachi Kuala LumpurMadrid Melbourne Mexico City Nairobi New Delhi ShanghaiTaipei TorontoWith offices inArgentina Austria Brazil Chile Czech Republic France GreeceGuatemala Hungary Italy Japan South Korea Poland PortugalSingapore Switzerland Thailand Turkey Ukraine VietnamOxford is a registered trade mark of Oxford University Pressin the UK and in certain other countries© Oxford University Press 1989Preface © Roger Penrose 1999The moral rights of the author have been assertedDatabase right Oxford University Press (maker)First published 1989First published as an Oxford University Press paperback with a new preface 1999All rights reserved. No part of this publication may be reproduced,stored in a retrieval system, or transmitted, in any form or by any means,without the prior permission in writing of Oxford University Press,or as expressly permitted by law, or under terms agreed with the appropriatereprographics rights organization. Enquiries concerning reproductionoutside the scope of the above should be sent to the Rights Department,Oxford University Press, at the address aboveYou must not circulate this book in any other binding or coverand you must impose this same condition on any acquirerBritish Library Cataloguing in Publication DataData availableLibrary of Congress Cataloging in Publication DataData availableISBN-13: 978-0-19-286198-613Printed in Great Britainon acid-free paper byClays Ltd, St Ives plc

DEDICATIONI dedicate this book to the loving memory of my dear mother, who did not quite live to see it.

NOTE TO THE READER:on reading mathematical equationsAT A NUMBER of places in this book I have resorted to the use of mathematical formulae, unabashed and unheeding of warnings that are frequently given: that each such formula will cut down the general readership by half. If you are a reader who finds any formula intimidating (and most people do), then I recommend a procedure that I normally adopt myself when such an offending line presents itself. The procedure is, more or less, to ignore that line completely and to skip over to the next actual line of text! Well, not exactly this; one should spare the poor formula a perusing, rather than a comprehending glance, and then press onwards. After a little, if armed with new confidence, one may return to that neglected formula and try to pick out some salient features. The text itself may be helpful in letting one know what is important and what can be safely ignored about it. If not, then do not be afraid to leave a formula behind altogether.

ACKNOWLEDGEMENTSTHERE ARE MANY who have helped me, in one way or another, in the writing of this book, and to whom thanks are due. In particular, there are those proponents of strong AI (especially those who were involved in a BBC TV programme I once watched) who, by the expressions of such extreme AI opinions, had goaded me, a number of years ago, into embarking upon this project. (Yet, had I known of the future labours that the writing would involve me in, I fear, now, that I should not have started!) Many people have perused versions of small parts of the manuscript and have provided me with many helpful suggestions for improvement; and to them, I also offer my thanks: Toby Bailey, David Deutsch (who was also greatly helpful in checking my Turing machine specifications), Stuart Hampshire, Jim Hartle, Lane Hughston, Angus Mclntyre, Mary Jane Mowat, Tristan Needham, Ted Newman, Eric Penrose, Toby Penrose, Wolfgang Rindler, Engelbert Schücking, and Dennis Sciama. Christopher Penrose’s help with detailed information concerning the Mandelbrot set is especially appreciated, as is that of Jonathan Penrose, for his useful information concerning chess computers. Special thanks go to Colin Blakemore, Erich Harth, and David Hubel for reading and checking over Chapter 9, which concerns a subject on which I am certainly no expert – though, as with all others whom I thank, they are in no way responsible for the errors which remain. I thank NSF for support under contracts DMS 84-05644, DMS 86-06488 (held at Rice University, Houston, where some lectures were given on which this book was partly based), and PHY 86–12424 (at Syracuse University where some valuable discussions on quantum mechanics took place). I am greatly indebted, also, to Martin Gardner for his extreme generosity in providing the foreword to this work, and also for some specific comments. Most particularly, I thank my beloved Vanessa, for her careful and detailed criticism of several chapters, for much invaluable assistance with references and, by no means least, for putting up with me when I have been at my most insufferable – and for her deep love and support where it was vitally needed.

FIGURE ACKNOWLEDGEMENTSTHE PUBLISHERS EITHER have sought or are grateful to the following for permission to reproduce illustration material.Figs 4.6 and 4.9 from D. A. Klarner (ed.), The mathematical Gardner (Wadsworth International, 1981).Fig. 4.7 from B. Grünbaum and G. C. Shephard, Tilings and patterns (W. H. Freeman, 1987). Copyright © 1987 by W. H. Freeman and Company. Used by permission.Fig. 4.10 from K. Chandrasekharan, Hermann Weyl 1885-1985 (Springer, 1986).Figs 4.11 and 10.3 from Pentaplexity: a class of non-periodic tilings of the plane. The Mathematical Intelligencer, 2, 32-7 (Springer, 1979).Fig. 4.12 from H. S. M. Coxeter, M. Emmer, R. Penrose, and M. L. Teuber (eds), M. C. Escher: Art and science (North-Holland, 1986).Fig. 5.2 © 1989 M. C. Escher Heirs/Cordon Art – Baarn – Holland.Fig. 10.4 from Journal of Materials Research, 2, 1–4 (Materials Research Society, 1987).All other figures (including 4.10 and 4.12) by the author.

FOREWORDby Martin GardnerMANY GREAT MATHEMATICIANS and physicists find it difficult, if not impossible, to write a book that non-professionals can understand. Until this year one might have supposed that Roger Penrose, one of the world’s most knowledgeable and creative mathematical physicists, belonged to such a class. Those of us who had read his non-technical articles and lectures knew better. Even so, it came as a delightful surprise to find that Penrose had taken time off from his labours to produce a marvellous book for informed laymen. It is a book that I believe will become a classic.Although Penrose’s chapters range widely over relativity theory, quantum mechanics, and cosmology, their central concern is what philosophers call the ‘mind-body problem’. For decades now the proponents of ‘strong AI’ (Artificial Intelligence) have tried to persuade us that it is only a matter of a century or two (some have lowered the time to fifty years!) until electronic computers will be doing everything a human mind can do. Stimulated by science fiction read in their youth, and convinced that our minds are simply ‘computers made of meat’ (as Marvin Minsky once put it), they take for granted that pleasure and pain, the appreciation of beauty and humour, consciousness, and free will are capacities that will emerge naturally when electronic robots become sufficiently complex in their algorithmic behaviour.Some philosophers of science (notably John Searle, whose notorious Chinese room thought experiment is discussed in depth by Penrose), strongly disagree. To them a computer is not essentially different from mechanical calculators that operate with wheels, levers, or anything that transmits signals. (One can base a computer on rolling marbles or water moving through pipes.) Because electricity travels through wires faster than other forms of energy (except light) it can twiddle symbols more rapidly than mechanical calculators, and therefore handle tasks of enormous complexity. But does an electrical computer ‘understand’ what it is doing in a way that is superior to the ‘understanding’ of an abacus? Computers now play grandmaster chess. Do they ‘understand’ the game any better than a tick-tack-toe machine that a group of computer hackers once constructed with tinker toys?Penrose’s book is the most powerful attack yet written on strong AI. Objections have been raised in past centuries to the reductionist claim that a mind is a machine operated by known laws of physics, but Penrose’s offensive is more persuasive because it draws on information not available to earlier writers. The book reveals Penrose to be more than a mathematical physicist. He is also a philosopher of first rank, unafraid to grapple with problems that contemporary philosophers tend to dismiss as meaningless.Penrose also has the courage to affirm, contrary to a growing denial by a small group of physicists, a robust realism. Not only is the universe ‘out there’, but mathematical truth also has its own mysterious independence and timelessness. Like Newton and Einstein, Penrose has a profound sense of humility and awe toward both the physical world and the Platonic realm of pure mathematics. The distinguished number theorist Paul Erdös likes to speak of ‘God’s book’ in which all the best proofs are recorded. Mathematicians are occasionally allowed to glimpse part of a page. When a physicist or a mathematician experiences a sudden ‘aha’ insight, Penrose believes, it is more than just something ‘conjured up by complicated calculation’. It is mind making contact for a moment with objective truth. Could it be, he wonders, that Plato’s world and the physical world (which physicists have now dissolved into mathematics) are really one and the same?Many pages in Penrose’s book are devoted to a famous fractallike structure called the Mandelbrot set after Benoit Mandelbrot who discovered it. Although self-similar in a statistical sense as portions of it are enlarged, its infinitely convoluted pattern keeps changing in unpredictable ways. Penrose finds it incomprehensible (as do I) that anyone could suppose that this exotic structure is not as much ‘out there’ as Mount Everest is, subject to exploration in the way a jungle is explored.Penrose is one of an increasingly large band of physicists who think Einstein was not being stubborn or muddle-headed when he said his ‘little finger’ told him that quantum mechanics is incomplete. To support this contention, Penrose takes you on a dazzling tour that covers such topics as complex numbers, Turing machines, complexity theory, the bewildering paradoxes of quantum mechanics, formal systems, Gödel undecidability, phase spaces, Hilbert spaces, black holes, white holes, Hawking radiation, entropy, the structure of the brain, and scores of other topics at the heart of current speculations. Are dogs and cats ‘conscious’ of themselves? Is it possible in theory for a matter-transmission machine to translocate a person from here to there the way astronauts are beamed up and down in television’s Star Trek series? What is the survival value that evolution found in producing consciousness? Is there a level beyond quantum mechanics in which the direction of time and the distinction between right and left are firmly embedded? Are the laws of quantum mechanics, perhaps even deeper laws, essential for the operation of a mind?To the last two questions Penrose answers yes. His famous theory of ‘twistors’–abstract geometrical objects which operate in a higher-dimensional complex space that underlies space–time–is too technical for inclusion in this book. They are Penrose’s efforts over two decades to probe a region deeper than the fields and particles of quantum mechanics. In his fourfold classification of theories as superb, useful, tentative, and misguided, Penrose modestly puts twistor theory in the tentative class, along with superstrings and other grand unification schemes now hotly debated.Since 1973 Penrose has been the Rouse Ball Professor of Mathematics at Oxford University. The title is appropriate because W. W. Rouse Ball not only was a noted mathematician, he was also an amateur magician with such an ardent interest in recreational mathematics that he wrote the classic English work on this field, Mathematical Recreations and Essays. Penrose shares Ball’s enthusiasm for play. In his youth he discovered an ‘impossible object’ called a ‘tribar’. (An impossible object is a drawing of a solid figure that cannot exist because it embodies self-contradictory elements.) He and his father Lionel, a geneticist, turned the tribar into the Penrose Staircase, a structure that Maurits Escher used in two well-known lithographs: Ascending and Descending, and Waterfall. One day when Penrose was lying in bed, in what he called a ‘fit of madness’, he visualized an impossible object in four-dimensional space. It is something, he said, that a four-space creature, if it came upon it, would exclaim ‘My God, what’s that?’During the 1960s, when Penrose worked on cosmology with his friend Stephen Hawking, he made what is perhaps his best known discovery. If relativity theory holds ‘all the way down’, there must be a singularity in every black hole where the laws of physics no longer apply. Even this achievement has been eclipsed in recent years by Penrose’s construction of two shapes that tile the plane, in the manner of an Escher tessellation, but which can tile it only in a non-periodic way. (You can read about these amazing shapes in my book Penrose Tiles to Trapdoor Ciphers.) Penrose invented them, or rather discovered them, without any expectation they would be useful. To everybody’s astonishment it turned out that three-dimensional forms of his tiles may underlie a strange new kind of matter. Studying these ‘quasicrystals’ is now one of the most active research areas in crystallography. It is also the most dramatic instance in modern times of how playful mathematics can have unanticipated applications.Penrose’s achievements in mathematics and physics–and I have touched on only a small fraction–spring from a lifelong sense of wonder toward the mystery and beauty of being. His little finger tells him that the human mind is more than just a collection of tiny wires and switches. The Adam of his prologue and epilogue is partly a symbol of the dawn of consciousness in the slow evolution of sentient life. To me he is also Penrose–the child sitting in the third row, a distance back from the leaders of AI–who dares to suggest that the emperors of strong AI have no clothes. Many of Penrose’s opinions are infused with humour, but this one is no laughing matter.

PREFACETHE EMPEROR’S NEW MIND, published in its original form in 1989, represents my first serious venture into popular science writing. As part of the aim of this book, I try to set forth, as clearly as I can, a good deal of the profound progress that physicists have made towards an understanding of the workings of the physical world. But this is not simply a piece of scientific exposition. I also try to point out some of the important ways in which our present scientific understanding still falls a long way short of its ultimate goal. Most particularly, I argue that the phenomenon of consciousness cannot be accommodated within the framework of present-day physical theory.This runs contrary to a certain common perception of the implications of a scientific viewpoint. According to this perception, all aspects of mentality (including conscious awareness) are merely features of the computational activity of the brain; consequently, electronic computers should also be capable of consciousness, and would conjure up this quality as soon as they acquire sufficient computational power and are programmed in an appropriate way. I do my best to express, in a dispassionate way, my scientific reasons for disbelieving this perception, arguing that the conscious aspects of our minds are not explicable in computational terms and moreover that conscious minds can find no home within our present-day scientific world-view. Nevertheless, it is not my contention that we should look outside science for an understanding of mentality, merely that existing science has not the richness to achieve what is required.One thing that I had not adequately anticipated while writing this book was the vehemence that my thesis would evoke, mainly from those who strongly support the computational model of the mind, but also from some who regarded science as anathema to the study of consciousness. No doubt, a person’s philosophical position with regard to the mind can—like a person’s religion—be a touchy subject. But just how touchy a subject it can be was not something that I had fully appreciated.My reasoning, as presented in this book, has two main strands to it. The first of these endeavours to show, by appealing to results of Gödel (and Turing) that mathematical thinking (and hence conscious thinking generally) is something that cannot be encapsulated within any purely computational model of thought. This is the part of my argument that my critics have most frequently taken issue with. The second strand of the reasoning is to demonstrate that there is an important gap in our physical picture of the world, at a level which ought to bridge the submicroscopic world of quantum physics to the macro-world of classical physics. My viewpoint demands that the missing physics falling within this gap, when found, will play an essential part in the physical understanding of the conscious mind. Moreover, there must be something outside purely computational action in this sought-for area of physics.In the roughly ten years that have elapsed since the first printing of this book, there have been a number of clear-cut developments, and I wish to give an outline of some of these here, so that the reader can gain some understanding of what I perceive to be the present status of these ideas. To begin with, let us consider the status of the relevance of Gödel’s theorem in relation to some of the criticisms that my arguments stirred up. What Gödel’s theorem tells us, in a nutshell, is the following (which is not controversial). Suppose that we are given some computational procedure P for establishing mathematical assertions (let us say, assertions of a particularly well-defined type, such as the famous ‘Fermat’s last theorem’ (cf. pp. 76–7)). Then if we are prepared to accept that the rules of P are trustworthy—in the sense that we accept that the successful derivation of some mathematical assertion by use of the rules of P provides us with an unassailable demonstration of the truth of that assertion—then we must also accept as unassailably true some other assertion G(P) which is beyond the scope of the rules of P (cf. p. 135). Thus, once we have seen how to mechanize some part of our mathematical understanding (into P, say), then we can also see how to transcend this mechanization. To me, this provides a clear-cut reason for believing that our mathematical understanding contains elements that lie beyond purely computational action. But many critics have remained unconvinced and have pointed to various possible loopholes in this deduction. In my follow-up book Shadows of the Mind,1 I responded to all these criticisms in some detail and provided a number of new arguments to counter these criticisms. However, the argument still goes on.2One of the reasons that people sometimes have difficulty in seeing the relevance of Gödel’s theorem to our mathematical understanding is that, according to the way in which the theorem is usually presented, G(P) seems to have little relevance to any mathematical result of interest. Moreover, G(P), as a mathematical statement, would be enormously difficult to comprehend. Accordingly, even mathematicians often find themselves happy to disregard mathematical statements like G(P). Yet, there are examples of Gödel statements that are easily accessible, even to those who have no particular familiarity with mathematical terminology nor notation beyond that which is used in ordinary arithmetic.A particularly striking such example came to my attention (in a lecture by Dan Isaacson in 1996) only after the above-mentioned writings were published. This is the result known as Goodstein’s theorem.3 I believe that it is instructive to give Goodstein’s theorem explicitly here, so that the reader can gain some direct experience of a Gödel-type theorem.4To appreciate what Goodstein’s theorem asserts, consider any positive whole number, let us say 581. First, we express this as a sum of distinct powers of 2:581 = 29 + 26 + 22 + 1.(This is what would be involved in forming the binary representation of the number 581, namely 1001000101, where the Is represent powers of 2 that are present in the expansion and the 0s represent those which are absent.) It will be noticed that the ‘exponents’ in this expression, namely the numbers 9, 6, 2 could also be represented in this way (9 = 23 + 1, 6 = 22 + 21, 2 = 21), and we get (recalling 21 = 2)581 = 223 + 1 + 222 + 2 + 22 + 1.There is still an exponent at the next order, namely the ‘3’, for which this representation can be adopted yet again (3 = 21 + 1), and we obtain581 = 222+11+ + 222+2 + 22 + 1.For larger numbers, we might have to go to third- or higher-order exponents.We now apply a succession of simple operations to this expression, these alternating between(a) increase the ‘base’ by 1,(b) subtract 1.The ‘base’ referred to in (a) is just the number ‘2’ in the above expressions, but we can do similar things for larger bases: 3, 4, 5, 6, ,. . . . Let us see what happens when we apply (a) to the last expression for 581 above, so 2s become 3s. We get333+1+1 + 333+3 + 33 + 1(which is, in fact, a number of 40 digits, when written out in the normal way, starting 133027946. . .). Next, we apply (b), to obtain333+1+1 + 333+3 + 33(which, of course, is still a number of 40 digits, starting 133027946 . . .). Now apply (a) again, to obtain444+1+1 + 444+4 + 44(which is now a number of 618 digits, starting 12926802. . .). The operation (b) of subtracting 1 now yields444+1+1 + 444+4 + 3 × 43 + 3 × 42 + 3 × 4 + 3(where the ‘3’s arise analogously to the ‘9’s that occur in ordinary base 10 notation when we subtract 1 from 10000 to obtain 9999). The operation (a) then gives us555+1+1 + 555+5 + 3 × 53 + 3 × 52 + 3 × 5 + 3(which has 10923 digits and starts off 1274 . . .). Note that the coefficients ‘3’ that appear here are necessarily all less than the base (now 5) and are unaffected by the increase in the base. Applying (b) again, we get555+1+1 + 555+5 + 3 × 53 + 3 × 52 + 3 × 5 + 2and we are to continue this alternation (a), (b), (a), (b), (a), (b),. . . as far as we can. The numbers appear to be ever increasing, and it would be natural to suppose that this will continue indefinitely. However, this is not so; for Goodstein’s remarkable theorem tells us that no matter what positive whole number we start with (here 581) we always eventually end up with zero!This seems extraordinary. But it is in fact true, and to get a feeling for this fact, I would recommend that the reader try it—starting first with 3 (where we have 3 = 21 + 1, so our sequence gives 3, 4, 3, 4, 3, 2, 1, 0)—but then, more importantly, trying 4 (where we have 4 = 22, so we get a sequence that starts tamely enough with 4, 27, 26, 42, 41, 61, 60, 84, . . ., but which reaches a number with 121210695 digits before decreasing finally to zero!).What is rather more extraordinary is that Goodstein’s theorem is actually a Gödel theorem for that procedure that we learn at school called mathematical induction, as was shown by L. A. S. Kirby and J. B. Paris.5 Recall that mathematical induction provides a way of proving that some mathematical statement S(n) holds for all n = 1, 2, 3, 4, 5,. . . . The procedure is to show, first, that it holds for n= 1 and then to show that if it holds for n, then it must also hold for n + 1. What Kirby and Paris demonstrated was, in effect, that if P stands for the procedure of mathematical induction, then we can take G(P) to be Goodstein’s theorem. This tells us that if we believe the procedure of mathematical induction to be trustworthy (which is hardly a doubtful assumption), then we must also believe in the truth of Goodstein’s theorem—despite the fact that it is not provable by mathematical induction alone.The ‘unprovability’, in this sense, of Goodstein’s theorem certainly does not stop us from seeing that it is in fact true. Our insights enable us to transcend the limited procedures of ‘proof that we had allowed ourselves previously. In fact, the way that Goodstein himself proved his theorem was to use an instance of what is called ‘transfinite induction’. In the present context, this provides a way of organizing an intuition that can be directly obtained by familiarizing oneself with the ‘reason’ that Goodstein’s theorem is in fact true. This intuition can be largely obtained by examining a number of individual cases of Goodstein’s theorem. What happens is that the modest little operation (b) relentlessly ‘chips away’ until the towers of exponents eventually tumble away, one-by-one, until none is left, even though this takes an incredibly large number of steps.What all this shows is that the quality of understanding is not something that can ever be encapsulated in a set of rules. Moreover, understanding is a quality that depends upon our awareness, so whatever it is that is responsible for conscious awareness seems to be coming essentially into play when ‘understanding’ is present. Thus, our awareness seems to be something that involves elements that cannot be encapsulated in computational rules of any kind; there are, indeed, very strong reasons to believe that it is an essentially ‘non-computational process’.The possible ‘loopholes’ to this conclusion, referred to above, are that our capacity for (mathematical) understanding might be the result of some calculational procedure that is unknowable because of its complication, or not unknowable but not knowably correct, or inaccurate but only approximately correct. In relation to such possibilities, we must consider how such a computational procedure might come about. In Shadows of the Mind I addressed all these possible loopholes in considerable detail, and I would recommend that discussion (and also the Psyche account ‘Beyond the Doubting of a Shadow’6) to any reader, interested in following up these issues more fully.If we accept that there is indeed something outside purely computational procedures in our capacity for understanding and, therefore, in our conscious actions more generally, then the next step would be to seek where, in physical actions, any ‘essentially non-computational behaviour’ might be found. (This is assuming that we also accept that ‘physical action’ of some kind is where we must look in order to find the origin of conscious phenomena.) I try to make the case that there is indeed nowhere within our present-day accepted physical theories where appropriate ‘non-computational action’ takes place. Hence, we must look for a relevant place where there is an important gap in our theories. This gap, I claim, lies in the bridge between the ‘submicroscopic’ world where quantum physics holds sway and the macro-world of our more direct experiences, where classical physics works so well.An important point has to be made here. The term ‘non-computational’ refers to specific types of mathematical action of the kind that has been mathematically proved to be outside the scope of computation. Part of the aim of this book is to bring such things to the attention of readers unfamiliar with these matters. Non-computable processes can be completely deterministic. This is something of a fundamentally different character from the complete randomness that features in our present-day interpretation of quantum mechanics, when a small-scale quantum effect is magnified to the classical level—the procedure referred to as ‘R’ in the present work. I argue that a new theory will indeed be needed in order to make coherent sense of the ‘reality’ that underlies the stop-gap R-procedure that we use in present-day quantum mechanics, and I try to argue that it is in this undiscovered new theory that the required non-computability will be found.I also argue that this missing theory is the same as the missing link between quantum theory and Einstein’s general relativity. The term used in conventional physics for this unified scheme is ‘quantum gravity’. However, most practitioners in this field tend to assume that the rules of quantum mechanics will not be altered in the bringing together of these two great twentieth-century theories, and that it is only general relativity that will be subject to change. My view is different, since I take the view that the procedures of quantum mechanics (in particular, the R-procedure) must also be fundamentally modified. I use the term ‘correct quantum gravity’ (or CQG) in this book, for this undiscovered unification. But this would not really be a quantum gravity theory in the ordinary sense (and perhaps ‘CQG’ is an unfortunate term, which may have misled some people).Although this theory is still missing, this does not stop us from trying to estimate the level at which it should become relevant. In this book, I refer to what I call ‘the one-graviton criterion’. For some years, now, I have shifted my view away from this, and a far more plausible scheme (in my opinion) is put forward in Shadows of the Mind. This new scheme is not only more plausible physically (and has an additional justification, that I have presented in a paper7), but it is much more usable than the previous scheme, and it has directed our thoughts towards new theoretical developments. In fact, there are now some physically realizable experiments for testing this scheme, which I hope can be performed in the next several years.8Even if all this works out in the way that I am arguing for, it does not directly assist us in understanding the ‘seat of consciousness’. One of the major shortcomings of this book is, perhaps, that when I wrote it I knew of no place in the brain where it could be plausibly argued that the ‘large-scale quantum coherence’ could take place that would be needed for the application of the ideas that I have just been describing. But perhaps one of the book’s strengths was that it found a broad audience amongst scientists who could contribute back to the development of our understandings in these matters. One of these scientists was Stuart Hameroff, who acquainted me with the cell’s cytoskele-ton and its microtubules—structures of which I had been deplorably ignorant previously! He also informed me of his own ingenious ideas concerning a possible role for microtubules, within the brain’s neurons, in relation to the phenomenon of consciousness. It seemed to me that the most plausible place for the kind of large-scale quantum coherent action that my arguments required was indeed within microtubules. Of course, this information came too late for inclusion in this book, but it is featured in Shadows of the Mind and has been developed further in a number of articles, for the most part jointly with Stuart Hameroff.9Apart from the new developments that I have referred to in this new introduction, the essential ideas of The Emperor’s New Mind are the same as they were ten years ago. I hope that the reader will gain some genuine enjoyment, as well as stimulation, from what I have to say.Roger PenroseSeptember 1998

CONTENTS    Prologue  1 CAN A COMPUTER HAVE A MIND?    Introduction    The Turing test    Artificial intelligence    An AI approach to ‘pleasure’ and ‘pain’    Strong AI and Searle’s Chinese room    Hardware and software  2 ALGORITHMS AND TURING MACHINES    Background to the algorithm concept    Turing’s concept    Binary coding of numerical data    The Church–Turing Thesis    Numbers other than natural numbers    The universal Turing machine    The insolubility of Hilbert’s problem    How to outdo an algorithm    Church’s lambda calculus  3 MATHEMATICS AND REALITY    The land of Tor’Bled-Nam    Real numbers    How many real numbers are there?    ‘Reality’ of real numbers    Complex numbers    Construction of the Mandelbrot set    Platonic reality of mathematical concepts?  4 TRUTH, PROOF, AND INSIGHT    Hilbert’s programme for mathematics    Formal mathematical systems    Gödel’s theorem    Mathematical insight    Platonism or intuitionism?    Gödel-type theorems from Turing’s result    Recursively enumerable sets    Is the Mandelbrot set recursive?    Some examples of non-recursive mathematics    Is the Mandelbrot set like non-recursive mathematics?    Complexity theory    Complexity and computability in physical things  5 THE CLASSICAL WORLD    The status of physical theory    Euclidean geometry    The dynamics of Galileo and Newton    The mechanistic world of Newtonian dynamics    Is life in the billiard-ball world computable?    Hamiltonian mechanics    Phase space    Maxwell’s electromagnetic theory    Computability and the wave equation    The Lorentz equation of motion; runaway particles    The special relativity of Einstein and Poincaré    Einstein’s general relativity    Relativistic causality and determinism    Computability in classical physics: where do we stand?    Mass, matter, and reality  6 QUANTUM MAGIC AND QUANTUM MYSTERY    Do philosophers need quantum theory?    Problems with classical theory    The beginnings of quantum theory    The two-slit experiment    Probability amplitudes    The quantum state of a particle    The uncertainty principle    The evolution procedures U and R    Particles in two places at once?    Hilbert space    Measurements    Spin and the Riemann sphere of states    Objectivity and measurability of quantum states    Copying a quantum state    Photon spin    Objects with large spin    Many-particle systems    The ‘paradox’ of Einstein, Podolsky, and Rosen    Experiments with photons: a problem for relativity?    Schrödinger’s equation; Dirac’s equation    Quantum field theory    Schrödinger’s cat    Various attitudes in existing quantum theory    Where does all this leave us?  7 COSMOLOGY AND THE ARROW OF TIME    The flow of time    The inexorable increase of entropy    What is entropy?    The second law in action    The origin of low entropy in the universe    Cosmology and the big bang    The primordial fireball    Does the big bang explain the second law?    Black holes    The structure of space-time singularities    How special was the big bang?  8 IN SEARCH OF QUANTUM GRAVITY    Why quantum gravity?    What lies behind the Weyl curvature hypothesis?    Time-asymmetry in state-vector reduction    Hawking’s box: a link with the Weyl curvature hypothesis?    When does the state-vector reduce?  9 REAL BRAINS AND MODEL BRAINS    What are brains actually like?    Where is the seat of consciousness?    Split-brain experiments    Blindsight    Information processing in the visual cortex    How do nerve signals work?    Computer models    Brain plasticity    Parallel computers and the ‘oneness’ of consciousness    Is there a role for quantum mechanics in brain activity?    Quantum computers    Beyond quantum theory?10 WHERE LIES THE PHYSICS OF MIND?    What are minds for?    What does consciousness actually do?    Natural selection of algorithms?    The non-algorithmic nature of mathematical insight    Inspiration, insight, and originality    Non-verbality of thought    Animal consciousness?    Contact with Plato’s world    A view of physical reality    Determinism and strong determinism    The anthropic principle    Tilings and quasicrystals    Possible relevance to brain plasticity    The time-delays of consciousness    The strange role of time in conscious perception    Conclusion: a child’s view    Epilogue    References    Index

PROLOGUETHERE WAS A GREAT gathering in the Grand Auditorium, marking the initiation of the new ‘Ultronic’ computer. President Pollo had just finished his opening speech. He was glad of that: he did not much care for such occasions and knew nothing of computers, save the fact that this one was going to gain him a great deal of time. He had been assured by the manufacturers that, amongst its many duties, it would be able to take over all those awkward decisions of State that he found so irksome. It had better do so, considering the amount of treasury gold that he had spent on it. He looked forward to being able to enjoy many long hours playing golf on his magnificent private golf course – one of the few remaining sizeable green areas left in his tiny country.Adam felt privileged to be among those attending this opening ceremony. He sat in the third row. Two rows in front of him was his mother, a chief technocrat involved in Ultronic’s design. His father, as it happened, was also there – uninvited at the back of the hall, and now completely surrounded by security guards. At the last minute Adam’s father had tried to blow up the computer. He had assigned himself this duty, as the self-styled ‘chairspirit’ of a small group of fringe activists: The Grand Council for Psychic Consciousness. Of course he and all his explosives had been spotted at once by numerous electronic and chemical sensing devices. As a small part of his punishment he would have to witness the turning-on ceremony.Adam had little feeling for either parent. Perhaps such feelings were not necessary for him. For all of his thirteen years he had been brought up in great material luxury, almost entirely by computers. He could have anything he wished for, merely at the touch of a button: food, drink, companionship, and entertainment, and also education whenever he felt the need – always illustrated by appealing and colourful graphic displays. His mother’s position had made all this possible.Now the Chief Designer was nearing the end of his speech:’. . . has over 1017 logical units. That’s more than the number of neurons in the combined brains of everyone in the entire country! Its intelligence will be unimaginable. But fortunately we do not need to imagine it. In a moment we shall all have the privilege of witnessing this intelligence at first hand: I call upon the esteemed First Lady of our great country, Madame Isabella Pollo, to throw the switch which will turn on our fantastic Ultronic Computer!’The President’s wife moved forward. Just a little nervously, and fumbling a little, she threw the switch. There was a hush, and an almost imperceptible dimming of lights as the 1017 logical units became activated. Everyone waited, not quite knowing what to expect. ‘Now is there anyone in the audience who would like to initiate our new Ultronic Computer System by asking it its first question?’ asked the Chief Designer. Everyone felt bashful, afraid to seem stupid before the crowd – and before the New Omnipresence. There was silence. ‘Surely there must be someone?’ he pleaded. But all were afraid, seeming to sense a new and all-powerful consciousness. Adam did not feel the same awe. He had grown up with computers since birth. He almost knew what it might feel like to be a computer. At least he thought perhaps he did. Anyway, he was curious. Adam raised his hand. ‘Ah yes,’ said the Chief Designer, ‘the little lad in the third row. You have a question for our – ah – new friend?’

1CAN A COMPUTER HAVE A MIND?INTRODUCTIONOVER THE PAST few decades, electronic computer technology has made enormous strides. Moreover, there can be little doubt that in the decades to follow, there will be further great advances in speed, capacity and logical design. The computers of today may be made to seem as sluggish and primitive as the mechanical calculators of yesteryear now appear to us. There is something almost frightening about the pace of development. Already computers are able to perform numerous tasks that had previously been the exclusive province of human thinking, with a speed and accuracy which far outstrip anything that a human being can achieve. We have long been accustomed to machinery which easily out-performs us in physical ways. That causes us no distress. On the contrary, we are only too pleased to have devices which regularly propel us at great speeds across the ground – a good five times as fast as the swiftest human athlete – or that can dig holes or demolish unwanted structures at rates which would put teams of dozens of men to shame. We are even more delighted to have machines that can enable us physically to do things we have never been able to do before: they can lift us into the sky and deposit us at the other side of an ocean in a matter of hours. These achievements do not worry our pride. But to be able to think – that has been a very human prerogative. It has, after all, been that ability to think which, when translated to physical terms, has enabled us to transcend our physical limitations and which has seemed to set us above our fellow creatures in achievement. If machines can one day excel us in that one important quality in which we have believed ourselves to be superior, shall we not then have surrendered that unique superiority to our creations?The question of whether a mechanical device could ever be said to think – perhaps even to experience feelings, or to have a mind -is not really a new one.1 But it has been given a new impetus, even an urgency, by the advent of modern computer technology. The question touches upon deep issues of philosophy. What does it mean to think or to feel? What is a mind? Do minds really exist? Assuming that they do, to what extent are minds functionally dependent upon the physical structures with which they are associated? Might minds be able to exist quite independently of such structures? Or are they simply the functionings of (appropriate kinds of) physical structure? In any case, is it necessary that the relevant structures be biological in nature (brains), or might minds equally well be associated with pieces of electronic equipment? Are minds subject to the laws of physics? What, indeed, are the laws of physics?These are among the issues I shall be attempting to address in this book. To ask for definitive answers to such grandiose questions would, of course, be a tall order. Such answers I cannot provide: nor can anyone else, though some may try to impress us with their guesses. My own guesses will have important roles to play in what follows, but I shall try to be clear in distinguishing such speculation from hard scientific fact, and I shall try also to be clear about the reasons underlying my speculations. My main purpose here, however, is not so much to attempt to guess answers. It is rather to raise certain apparently new issues concerning the relation between the structure of physical law, the nature of mathematics and of conscious thinking, and to present a viewpoint that I have not seen expressed before. It is a viewpoint that I cannot adequately describe in a few words; and this is one reason for my desire to present things in a book of this length. But briefly, and perhaps a little misleadingly, I can at least state that my point of view entails that it is our present lack of understanding of the fundamental laws of physics that prevents us from coming to grips with the concept of ‘mind’ in physical or logical terms. By this I do not mean that the laws will never be that well known. On the contrary, part of the aim of this work is to attempt to stimulate future research in directions which seem to be promising in this respect, and to try to make certain fairly specific, and apparently new, suggestions about the place that ‘mind’ might actually occupy within a development of the physics that we know.I should make clear that my point of view is an unconventional one among physicists and is consequently one which is unlikely to be adopted, at present, by computer scientists or physiologists. Most physicists would claim that the fundamental laws operative at the scale of a human brain are indeed all perfectly well known. It would, of course, not be disputed that there are still many gaps in our knowledge of physics generally. For example, we do not know the basic laws governing the mass-values of the subatomic particles of nature nor the strengths of their interactions. We do not know how to make quantum theory fully consistent with Einstein’s special theory of relativity – let alone how to construct the ‘quantum gravity’ theory that would make quantum theory consistent with his general theory of relativity. As a consequence of the latter, we do not understand the nature of space at the absurdly tiny scale of 1/100000000000000000000 of the dimension of the known fundamental particles, though at dimensions larger than that our knowledge is presumed adequate. We do not know whether the universe as a whole is finite or infinite in extent – either in space or in time – though such uncertainties would appear to have no bearing whatever on physics at the human scale. We do not understand the physics that must operate at the cores of black holes nor at the big-bang origin of the universe itself. Yet all these issues seem as remote as one could imagine from the ‘everyday’ scale (or a little smaller) that is relevant to the workings of a human brain. And remote they certainly are! Nevertheless, I shall argue that there is another vast unknown in our physical understanding at just such a level as could indeed be relevant to the operation of human thought and consciousness – in front of (or rather behind) our very noses! It is an unknown that is not even recognized by the majority of physicists, as I shall try to explain. I shall further argue that, quite remarkably, the black holes and big bang are considerations which actually do have a definite bearing on these issues!In what follows I shall attempt to persuade the reader of the force of evidence underlying the viewpoint I am trying to put forward. But in order to understand this viewpoint we shall have a lot of work to do. We shall need to journey through much strange territory – some of seemingly dubious relevance – and through many disparate fields of endeavour. We shall need to examine the structure, foundations, and puzzles of quantum theory, the basic features of both special and general relativity, of black holes, the big bang, and of the second law of thermodynamics, of Maxwell’s theory of electromagnetic phenomena, as well as of the basics of Newtonian mechanics. Questions of philosophy and psychology will have their clear role to play when it comes to attempting to understand the nature and function of consciousness. We shall, of course, have to have some glimpse of the actual neurophysiology of the brain, in addition to suggested computer models. We shall need some idea of the status of artificial intelligence. We shall need to know what a Turing machine is, and to understand the meaning of computability, of Gödel’s theorem, and of complexity theory. We shall need also to delve into the foundations of mathematics, and even to question the very nature of physical reality.If, at the end of it all, the reader remains unpersuaded by the less conventional of the arguments that I am trying to express, it is at least my hope that she or he will come away with something of genuine value from this tortuous but, I hope, fascinating journey.THE TURING TESTLet us imagine that a new model of computer has come on the market, possibly with a size of memory store and number of logical units in excess of those in a human brain. Suppose also that the machines have been carefully programmed and fed with great quantities of data of an appropriate kind. The manufacturers are claiming that the devices actually think. Perhaps they are also claiming them to be genuinely intelligent. Or they may go further and make the suggestion that the devices actually feel – pain, happiness, compassion, pride, etc. – and that they are aware of, and actually understand what they are doing. Indeed, the claim seems to be being made that they are conscious.How are we to tell whether or not the manufacturers’ claims are to be believed? Ordinarily, when we purchase a piece of machinery, we judge its worth solely according to the service it provides us. If it satisfactorily performs the tasks we set it, then we are well pleased. If not, then we take it back for repairs or for a replacement. To test the manufacturers’ claim that such a device actually has the asserted human attributes we would, according to this criterion, simply ask that it behaves as a human being would in these respects. Provided that it does this satisfactorily, we should have no cause to complain to the manufacturers and no need to return the computer for repairs or replacement.This provides us with a very operational view concerning these matters. The operationalist would say that the computer thinks provided that it acts indistinguishably from the way that a person acts when thinking. For the moment, let us adopt this operational viewpoint. Of course this does not mean that we are asking that the computer move about in the way that a person might while thinking. Still less would we expect it to look like a human being or feel like one to the touch: those would be attributes irrelevant to the computer’s purpose. However, this does mean that we are asking it to produce human-like answers to any question that we may care to put to it, and that we are claiming to be satisfied that it indeed thinks (or feels, understands, etc.) provided that it answers our questions in a way indistinguishable from a human being.This viewpoint was argued for very forcefully in a famous article by Alan Turing, entitled ‘Computing Machinery and Intelligence’, which appeared in 1950 in the philosophical journal Mind (Turing 1950). (We shall be hearing more about Turing later.) In this article the idea now referred to as the Turing test was first described. This was intended to be a test of whether a machine can reasonably be said to think. Let us suppose that a computer (like the one our manufacturers are hawking in the description above) is indeed being claimed to think. According to the Turing test, the computer, together with some human volunteer, are both to be hidden from the view of some (perceptive) interrogator. The interrogator has to try to decide which of the two is the computer and which is the human being merely by putting probing questions to each of them. These questions, but more importantly the answers that she* receives, are all transmitted in an impersonal fashion, say typed on a keyboard and displayed on a screen. The interrogator is allowed no information about either party other than that obtained merely from this question-and-answer session. The human subject answers the questions truthfully and tries to persuade her that he is indeed the human being and that the other subject is the computer; but the computer is programmed to ‘lie’ so as to try to convince the interrogator that it, instead, is the human being. If in the course of a series of such tests the interrogator is unable to identify the real human subject in any consistent way, then the computer (or the computer’s program, or programmer, or designer, etc.) is deemed to have passed the test.Now, it might be argued that this test is actually quite unfair on the computer. For if the roles were reversed so that the human subject instead were being asked to pretend to be a computer and the computer instead to answer truthfully, then it would be only too easy for the interrogator to find out which is which. All she would need to do would be to ask the subject to perform some very complicated arithmetical calculation. A good computer should be able to answer accurately at once, but a human would be easily stumped. (One might have to be a little careful about this, however. There are human ‘calculating prodigies’ who can perform very remarkable feats of mental arithmetic with unfailing accuracy and apparent effortlessness. For example, Johann Martin Zacharias Dase,2 an illiterate farmer’s son, who lived from 1824 to 1861, in Germany, was able to multiply any two eight figure numbers together in his head in less than a minute, or two twenty figure numbers together in about six minutes! It might be easy to mistake such feats for the calculations of a computer. In more recent times, the computational achievements of Alexander Aitken, who was Professor of Mathematics at the University of Edinburgh in the 1950s, and others, are as impressive. The arithmetical task that the interrogator chooses for the test would need to be significantly more taxing than this – say to multiply together two thirty digit numbers in two seconds, which would be easily within the capabilities of a good modern computer.)Thus, part of the task for the computer’s programmers is to make the computer appear to be ‘stupider’ than it actually is in certain respects. For if the interrogator were to ask the computer a complicated arithmetical question, as we had been considering above, then the computer must now have to pretend not to be able to answer it, or it would be given away at once! But I do not believe that the task of making the computer ‘stupider’ in this way would be a particularly serious problem facing the computer’s programmers. Their main difficulty would be to make it answer some of the simplest ‘common sense’ types of question – questions that the human subject would have no difficulty with whatever!There is an inherent problem in citing specific examples of such questions, however. For whatever question one might first suggest, it would be an easy matter, subsequently, to think of a way to make the computer answer that particular question as a person might. But any lack of real understanding on the part of the computer would be likely to become evident with sustained questioning, and especially with questions of an original nature and requiring some real understanding. The skill of the interrogator would partly lie in being able to devise such original forms of question, and partly in being able to follow them up with others, of a probing nature, designed to reveal whether or not any actual ‘understanding’ has occurred. She might also choose to throw in an occasional complete nonsense question, to see if the computer could detect the difference, or she might add one or two which sounded superficially like nonsense, but really did make some kind of sense: for example she might say, ‘I hear that a rhinoceros flew along the Mississippi in a pink balloon, this morning. What do you make of that?’ (One can almost imagine the beads of cold sweat forming on the computer’s brow – to use a most inappropriate metaphor!) It might guardedly reply, ‘That sounds rather ridiculous to me.’ So far, so good. Interrogator: ‘Really? My uncle did it once – both ways – only it was off-white with stripes. What’s so ridiculous about that?’ It is easy to imagine that if it had no proper ‘understanding’, a computer could soon be trapped into revealing itself. It might even blunder into ‘Rhinoceroses can’t fly’, its memory banks having helpfully come up with the fact that they have no wings, in answer to the first question, or ‘Rhinoceroses don’t have stripes’ in answer to the second. Next time she might try a real nonsense question, such as changing it to ‘under the Mississippi’, or ‘inside a pink balloon’, or ‘in a pink nightdress’ to see if the computer would have the sense to realize the essential difference!Let us set aside, for the moment, the issue of whether, or when, some computer might be made which actually passes the Turing test. Let us suppose instead, just for the purpose of argument, that such machines have already been constructed. We may well ask whether a computer, which does pass the test, should necessarily be said to think, feel, understand, etc. I shall come back to this matter very shortly. For the moment, let us consider some of the implications. For example, if the manufacturers are correct in their strongest claims, namely that their device is a thinking, feeling, sensitive, understanding, conscious being, then our purchasing of the device will involve us in moral responsibilities. It certainly should do so if the manufacturers are to be believed! Simply to operate the computer to satisfy our needs without regard to its own sensibilities would be reprehensible. That would be morally no different from maltreating a slave. Causing the computer to experience the pain that the manufacturers claim it is capable of feeling would be something that, in a general way, we should have to avoid. Turning off the computer, or even perhaps selling it, when it might have become attached to us, would present us with moral difficulties, and there would be countless other problems of the kind that relationships with other human beings or other animals tend to involve us in. All these would now become highly relevant issues. Thus, it would be of great importance for us to know (and also for the authorities to know!) whether the manufacturers’ claims – which, let us suppose, are based on their assertion that‘Each thinking device has been thoroughly Turing-tested by our team of experts’–are actually true!It seems to me that, despite the apparent absurdity of some of the implications of these claims, particularly the moral ones, the case for regarding the successful passing of a Turing test as a valid indication of the presence of thought, intelligence, understanding, or consciousness is actually quite a strong one. For how else do we normally form our judgements that people other than ourselves possess just such qualities, except by conversation? Actually there are other criteria, such as facial expressions, movements of the body, and actions generally, which can influence us very significantly when we are making such judgements. But we could imagine that (perhaps somewhat more distantly in the future) a robot could be constructed which could successfully imitate all these expressions and movements. It would now not be necessary to hide the robot and the human subject from the view of the interrogator, but the criteria that the interrogator has at her disposal are, in principle, the same as before.From my own point of view, I should be prepared to weaken the requirements of the Turing test very considerably. It seems to me that asking the computer to imitate a human being so closely so as to be indistinguishable from one in the relevant ways is really asking more of the computer than necessary. All I would myself ask for would be that our perceptive interrogator should really feel convinced, from the nature of the computer’s replies, that there is a conscious presence underlying these replies – albeit a possibly alien one. This is something manifestly absent from all computer systems that have been constructed to date. However, I can appreciate that there would be a danger that if the interrogator were able to decide which subject was in fact the computer, then, perhaps unconsciously, she might be reluctant to attribute a consciousness to the computer even when she could perceive it. Or, on the other hand, she might have the impression that she ‘senses’ such an ‘alien presence’ – and be prepared to give the computer the benefit of the doubt – even when there is none. For such reasons, the original Turing version of the test has a considerable advantage in its greater objectivity, and I shall generally stick to it in what follows. The consequent ‘unfairness’ towards the computer to which I have referred earlier (i.e. that it must be able to do all that a human can do in order to pass, whereas the human need not be able to do all that a computer can do) is not something that seems to worry supporters of the Turing test as a true test of thinking, etc. In any case their point of view often tends to be that it will not be too long before a computer will be able actually to pass the test – say by the year 2010. (Turing originally suggested that a 30 per cent success rate for the computer, with an ‘average’ interrogator and just five minutes’ questioning, might be achieved by the year 2000.) By implication, they are rather confident that this bias is not significantly delaying that day!All these matters are relevant to an essential question: namely does the operational point of view actually provide a reasonable set of criteria for judging the presence or absence of mental qualities in an object? Some would argue strongly that it does not. Imitation, no matter how skilful, need not be the same as the real thing. My own position is a somewhat intermediate one in this respect. I am inclined to believe, as a general principle, that imitation, no matter how skilful, ought always to be detectable by skilful enough probing – though this is more a matter of faith (or scientific optimism) that proven fact. Thus I am, on the whole, prepared to accept the Turing test as a roughly valid one in its chosen context. That is to say, if the computer were indeed able to answer all questions put to it in a manner indistinguishable from the way that a human being might answer them – and thus to fool our perceptive interrogator properly* and consistently – then, in
the absence of any contrary evidence, my guess would be that the computer actually thinks, feels, etc. By my use of words such as ‘evidence’, ‘actually’, and ‘guess’ here, I am implying that when I refer to thinking, feeling, or understanding, or, particularly, to consciousness, I take the concepts to mean actual objective ‘things’ whose presence or absence in physical bodies is something we are trying to ascertain, and not to be merely conveniences of language! I regard this as a crucial point. In trying to discern the presence of such qualities, we make guesses based on all the evidence that may be available to us. (This is not, in principle, different from, say, an astronomer trying to ascertain the mass of a distant star.)What kind of contrary evidence might have to be considered? It is hard to lay down rules about this ahead of time. But I do want to make clear that the mere fact that the computer might be made from transistors, wires, and the like, rather than neurons, blood vessels, etc. is not, in itself, the kind of thing that I would regard as contrary evidence. The kind of thing I do have in mind is that at some time in the future a successful theory of consciousness might be developed – successful in the sense that it is a coherent and appropriate physical theory, consistent in a beautiful way with the rest of physical understanding, and such that its predictions correlate precisely with human beings’ claims as to when, whether, and to what degree they themselves seem to be conscious – and that this theory might indeed have implications regarding the putative consciousness of our computer. One might even envisage a ‘consciousness detector’, built according to the principles of this theory, which is completely reliable with regard to human subjects, but which gives results at variance with those of a Turing test in the case of a computer. In such circumstances one would have to be very careful about interpreting the results of Turing tests. It seems to me that how one views the question of the appropriateness of the Turing test depends partly on how one expects science and technology to develop. We shall need to return to some of these considerations later on.ARTIFICIAL INTELLIGENCEAn area of much interest in recent years is that referred to as artificial intelligence, often shortened simply to ‘AI’. The objectives of AI are to imitate by means of machines, normally electronic ones, as much of human mental activity as possible, and perhaps eventually to improve upon human abilities in these respects. There is interest in the results of AI from at least four directions. In particular there is the study of robotics, which is concerned, to a large extent, with the practical requirements of industry for mechanical devices which can perform ‘intelligent’ tasks – tasks of a versatility and complication which have previously demanded human intervention or control – and to perform them with a speed and reliability beyond any human capabilities, or under adverse conditions where human life could be at risk. Also of interest commercially, as well as generally, is the development of expert systems, according to which the essential knowledge of an entire profession – medical, legal, etc. – is intended to be coded into a computer package! Is it possible that the experience and expertise of human members of these professions might actually be supplanted by such packages? Or is it merely that long lists of factual information, together with comprehensive cross-referencing, are all that can be expected to be achieved? The question of whether the computers can exhibit (or simulate) genuine intelligence clearly has considerable social implications. Another area in which AI could have direct relevance is psychology. It is hoped that by trying to imitate the behaviour of a human brain (or that of some other animal) by means of an electronic device – or by failing to do so – one may learn something of importance concerning the brain’s workings. Finally, there is the optimistic hope that for similar reasons AI might have something to say about deep questions of philosophy, by providing insights into the meaning of the concept of mind.How far has AI been able to progress to date? It would be hard for me to try to summarize. There are many active groups in different parts of the world and I am familiar with details of only a little of this work. Nevertheless, it would be fair to say that, although many clever things have indeed been done, the simulation of anything that could pass for genuine intelligence is yet a long way off. To convey something of the flavour of the subject, I shall first mention some of the (still quite impressive) early achievements, and then some remarkable recent progress with chess computers.One of the first AI devices was W. Grey Walter’s ‘tortoise’, made in the early 1950s,3 which moved around the floor under its own power until its batteries ran low, and then it would make for the nearest power socket, plug itself in and recharge its batteries. When thoroughly replenished it would detach itself from the socket and renew its adventures across the floor! Since then many other similar objects have been constructed (see, for example, Waltz 1982). A somewhat separate line of development is exemplified by Terry Winograd’s computer program, designed in 1972, which can converse sensibly about a collection of blocks of various shapes and colours which, in simulation, it places on top of one another in different orders and arrangements. Despite such early successes, it is remarkable how difficult it has proved to be to design the control of even a simple, jointed ‘robot arm’, which has to manoeuvre around, avoiding crowded obstacles, where the required motions may seem ‘obvious’ to a human looking on. The interpretation of visual scenes generally is also an area where the procedures of AI have so far got nowhere close to what the human brain (and, indeed, most other animals) seems ‘effortlessly’ to be able to achieve.Perhaps more in the direct line of a Turing test is the computer program of K. M. Colby, developed in the mid-1960s, which simulates a psychotherapist – so successfully that, some patients find that they prefer the computer to a human therapist, and are prepared to reveal more of themselves to it than they are to its human counterparts. For an example4 of such an interchange between the computer ‘therapist’ and a patient, consider the following conversation (conducted via teletype):THERAPIST: How can I help youPATIENT: I don’t knowTHERAPIST: Try to tell me something about itPATIENT: NoTHERAPIST: Can you elaboratePATIENT: I am sick and tired todayTHERAPIST: Do you have any idea whyPATIENT: Yes I’m oldTHERAPIST: Please go onPATIENT: I seem to have lost interest in everythingTHERAPIST: Tell me more about such feelingsPATIENT: I have some idea why I feel this way tooTHERAPIST: You havePATIENT: It seems that I miss having a familyTHERAPIST: What makes you feel you miss having a familyPATIENT: There is a challenge about itTHERAPIST: Why is that important to youThough this may give an eerie impression that the computer has some understanding, in fact it has none, and is merely following some fairly simple mechanical rules. (There is also a ‘converse’ to this in a system where the computer simulates a human schizophrenic patient, giving all the textbook answers and symptoms, and is capable of fooling some medical students into believing that a human patient is actually supplying the answers!)Chess-playing computers probably provide the best examples of machines exhibiting what might be thought of as ‘intelligent behaviour’. In fact, some machines have now (in 1989) reached an extremely respectable level of performance in relation to human players – approaching that of ‘International Master’. (These computers’ ratings would be a little below 2300, where, for comparison, Kasparov, the world champion, has a rating greater than 2700.) In particular, a computer program (for a Fidelity Excel commercial microprocessor) by Dan and Kathe Spracklen has achieved a rating (Elo) of 2110 and has now been awarded the USCF ‘Master’ title. Even more impressive is ‘Deep Thought’, programmed largely by Hsiung Hsu, of Carnegie Mellon University, which has a rating of about 2500 Elo, and recently achieved the remarkable feat of sharing first prize (with Grandmaster Tony Miles) in a chess tournament (in Longbeach, California, November 1988), actually defeating a Grandmaster (Bent Larsen) for the first time!5 Chess computers now also excel at solving chess problems, and can easily outstrip humans at this endeavour.6Chess-playing machines rely a lot on ‘book knowledge’ in addition to accurate calculational power. It is worth remarking that chess-playing machines fare better on the whole, relative to a comparable human player, when it is required that the moves are made very quickly; the human players perform relatively better in relation to the machines when a good measure of time is allowed for each move. One can understand this in terms of the fact that the computer’s decisions are made on the basis of precise and rapid extended computations, whereas the human player takes advantage of ‘judgements’, that rely upon comparatively slow conscious assessments. These human judgements serve to cut down drastically the number of serious possibilities that need be considered at each stage of calculation, and much greater depth can be achieved in the analysis, when the time is available, than in the machine’s simply calculating and directly eliminating possibilities, without using such judgements. (This difference is even more noticeable with the difficult Oriental game of ‘go’, where the number of possibilities per move is considerably greater than in chess.) The relationship between consciousness and the forming of judgements will be central to my later arguments, especially in Chapter 10.AN AI APPROACH TO ‘PLEASURE’ AND ‘PAIN’One of the claims of AI is that it provides a route towards some sort of understanding of mental qualities, such as happiness, pain, hunger. Let us take the example of Grey Walter’s tortoise. When its batteries ran low its behaviour pattern would change, and it would then act in a way designed to replenish its store of energy. There are clear analogies between this and the way that a human being – or any other animal – would act when feeling hungry. It perhaps might not be too much of a distortion of language to say that the Grey Walter tortoise was ‘hungry’ when it acted in this way. Some mechanism within it was sensitive to the state of charge in its battery, and when this got below a certain point it switched the tortoise over to a different behaviour pattern. No doubt there is something similar operating within animals when they become hungry, except that the changes in behaviour patterns are more complicated and subtle. Rather than simply switching over from one behaviour pattern to another, there is a change in tendencies to act in certain ways, these changes becoming stronger (up to a point) as the need to replenish the energy supply increases.Likewise, some AI supporters envisage that concepts such as pain or happiness can be appropriately modelled in this way. Let us simplify things and consider just a single scale of ‘feelings’ ranging from extreme ‘pain’ (score: -100) to extreme ‘pleasure’ (score: +100). Imagine that we have a device – a machine of some kind, presumably electronic – that has a means of registering its own (putative) ‘pleasure-pain’ score, which I refer to as its ‘pp-score’. The device is to have certain modes of behaviour and certain inputs, either internal (like the state of its batteries) or external. The idea is that its actions are geared so as to maximize its pp-score. There could be many factors which influence the pp-score. We could certainly arrange that the charge in its battery is one of them, so that a low charge counts negatively and a high charge positively, but there could be other factors too. Perhaps our device has some solar panels on it which give it an alternative means of obtaining energy, so that its batteries need not be used when the panels are in operation. We could arrange that by moving towards the light it can increase its pp-score a little, so that in the absence of other factors this is what it would tend to do. (Actually, Grey Walter’s tortoise used to avoid the light!) It would need to have some means of performing computations so that it could work out the likely effects that different actions on its part would ultimately have on its pp-score. It could introduce probability weightings, so that a calculation would count as having a larger or smaller effect on the score depending upon the reliability of the data upon which it is based.It would be necessary also to provide our device with other ‘goals’ than just maintaining its energy supply, since otherwise we should have no means of distinguishing ‘pain’ from ‘hunger’. No doubt it is too much to ask that our device have a means of procreation so, for the moment, sex is out! But perhaps we can implant in it a ‘desire’ for companionship with other such devices, by giving meetings with them a positive pp-score. Or we could make it ‘crave’ learning for its own sake, so that the mere storing of facts about the outside world would also score positively on its pp-scale. (More selfishly, we could arrange that performing various services for us have a positive score, as one would need to do if constructing a robot servant!) It might be argued that there is an artificiality about imposing such ‘goals’ on our device according to our whim. But this is not so very different from the way that natural selection has imposed upon us, as individuals, certain ‘goals’ which are to a large extent governed by the need to propagate our genes.Suppose, now, that our device has been successfully constructed in accordance with all this. What right would we have to assert that it actually feels pleasure when its pp-score is positive and pain when the score is negative? The AI (or operational) point of view would be that we judge this simply from the way that the device behaves. Since it acts in a way which increases its score to as large a positive value as possible (and for as long as possible) and it correspondingly also acts to avoid negative scores, then we could reasonably define its feeling of pleasure as the degree of positivity of its score, and correspondingly define its feeling of pain to be the degree of negativity of the score. The ‘reasonableness’ of such a definition, it would be argued, comes from the fact that this is precisely the way that a human being reacts in relation to feelings of pleasure or pain. Of course, with human beings things are actually not nearly so simple as that, as we all know: sometimes we seem deliberately to court pain, or to go out of our way to avoid certain pleasures. It is clear that our actions are really guided by much more complex criteria than these (cf. Dennett 1978, pp. 190-229). But as a very rough approximation, avoiding pain and courting pleasure is indeed the way we act. To an operationalist this would be enough to provide justification, at a similar level of approximation, for the identification of pp-score in our device with its pain–pleasure rating. Such identifications seem also to be among the aims of AI theory.We must ask: Is it really the case that our device would actually feel pain when its pp-score is negative and pleasure when it is positive? Indeed, could our device feel anything at all? The operationalist would, no doubt, either say ‘Obviously yes’, or dismiss such questions as meaningless. But it seems to me to be clear that there is a serious and difficult question to be considered here. In ourselves, the influences that drive us are of various kinds. Some are conscious, like pain or pleasure; but there are others of which we are not directly aware. This is clearly illustrated by the example of a person touching a hot stove. An involuntary action is set up which causes him to withdraw his hand even before he experiences any sensation of pain. It would seem to be the case that such involuntary actions are very much closer to the responses of our device to its pp-score than are the actual effects of pain or pleasure.One often uses anthropomorphic terms in a descriptive, often jocular, way to describe the behaviour of machines: ‘My car doesn’t seem to want to start this morning’; or ‘My watch still thinks it’s running on Californian time’; or ‘My computer claims it didn’t understand that last instruction and doesn’t know what to do next.’ Of course we don’t really mean to imply that the car actually might want something, or that the watch thinks, or that the computer* actually claims anything or that it understands or even knows what it is doing. Nevertheless such statements can be genuinely descriptive and helpful to our own understanding, provided that we take them merely in the spirit in which they are intended and do not regard them as literal assertions. I would take a rather similar attitude to various claims of AI that mental qualities might be present in the devices which have been constructed – irrespective of the spirit in which they are intended! If I agree to say that Grey Walter’s tortoise can be hungry, it is in this half-jocular sense that I mean it. If I am prepared to use terms such as ‘pain’ or ‘pleasure’ for the pp-score of a device as envisaged above, it is because I find these terms helpful to my understanding of its behaviour, owing to certain analogies with my own behaviour and mental states. I do not mean to imply that these analogies are really particularly close or, indeed, that there are not other un conscious things which influence my behaviour in a much more analogous way.I hope it is clear to the reader that in my opinion there is a great deal more to the understanding of mental qualities than can be directly obtained from AI. Nevertheless, I do believe that AI presents a serious case which must be respected and reckoned with. In saying this I do not mean to imply that very much, if anything, has yet been achieved in the simulation of actual intelligence. But one has to bear in mind that the subject is very young. Computers will get faster, have larger rapid-access stores, more logical units, and will have large numbers of operations performed in parallel. There will be improvements in logical design and in programming technique. These machines, the vehicles of the AI philosophy, will be vastly improved in their technical capabilities. Moreover, the philosophy itself is not an intrinsically absurd one. Perhaps human intelligence can indeed be very accurately simulated by electronic computers – essentially the computers of today, based on principles that are already understood, but with the much greater capacity, speed, etc., that they are bound to have in the years to come. Perhaps, even, these devices will actually be intelligent; perhaps they will think, feel, and have minds. Or perhaps they will not, and some new principle is needed, which is at present thoroughly lacking. That is what is at issue, and it is a question that cannot be dismissed lightly. I shall try to present evidence, as best I see it. Eventually I shall put forward my own suggestions.STRONG AI AND SEARLE’S CHINESE ROOMThere is a point of view, referred to as strong AI which adopts a rather extreme position on these issues.7 According to strong AI, not only would the devices just referred to indeed be intelligent and have minds, etc., but mental qualities of a sort can be attributed to the logical functioning of any computational device, even the very simplest mechanical ones, such as a thermostat.8 The idea is that mental activity is simply the carrying out of some well-defined sequence of operations, frequently referred to as an algorithm. I shall be more precise later on, as to what an algorithm actually is. For the moment, it will be adequate to define an algorithm simply as a calculational procedure of some kind. In the case of a thermostat, the algorithm is extremely simple: the device registers whether the temperature is greater or smaller than the setting, and then it arranges that the circuit be disconnected in the former case and connected in the latter. For any significant kind of mental activity of a human brain, the algorithm would have to be something vastly more complicated but, according to the strong-AI view, an algorithm nevertheless It would differ very greatly in degree from the simple algorithm of the thermostat, but need not differ in principle. Thus, according to strong AI, the difference between the essential functioning of a human brain (including all its conscious manifestations) and that of a thermostat lies only in this much greater complication (or perhaps ‘higher-order structure’ or ‘self-referential properties’, or some other attribute that one might assign to an algorithm) in the case of a brain. Most importantly, all mental qualities – thinking, feeling, intelligence, understanding, consciousness – are to be regarded, according to this view, merely as aspects of this complicated functioning; that is to say, they are features merely of the algorithm being carried out by the brain.The virtue of any specific algorithm would lie in its performance, namely in the accuracy of its results, its scope, its economy, and the speed with which it can be operated. An algorithm purporting to match what is presumed to be operating in a human brain would need to be a stupendous thing. But if an algorithm of this kind exists for the brain – and the supporters of strong AI would certainly claim that it does – then it could in principle be run on a computer. Indeed it could be run on any modern general-purpose electronic computer, were it not for limitations of storage space and speed of operation. (The justification of this remark will come later, when we come to consider the universal Turing machine.) It is anticipated that any such limitations would be overcome for the large fast computers of the not-too-distant future. In that eventuality, such an algorithm, if it could be found, would presumably pass the Turing test. The supporters of strong AI would claim that whenever the algorithm were run it would, in itself: experience feelings; have a consciousness; be a mind.By no means everyone would be in agreement that mental states and algorithms can be identified with one another in this kind of way. In particular, the American philosopher John Searle (1980, 1987) has strongly disputed that view. He has cited examples where simplified versions of the Turing test have actually already been passed by an appropriately programmed computer, but he gives strong arguments to support the view that the relevant mental attribute of ‘understanding’ is, nevertheless, entirely absent. One such example is based on a computer program designed by Roger Schank (Schank and Abelson 1977). The aim of the program is to provide a simulation of the understanding of simple stories like: ‘A man went into a restaurant and ordered a hamburger. When the hamburger arrived it was burned to a crisp, and the man stormed out of the restaurant angrily, without paying the bill or leaving a tip.’ For a second example: ‘A man went into a restaurant and ordered a hamburger; when the hamburger came he was very pleased with it; and as he left the restaurant he gave the waitress a large tip before paying his bill.’ As a test of ‘understanding’ of the stories, the computer is asked whether the man ate the hamburger in each case (a fact which had not been explicitly mentioned in either story). To this kind of simple story and simple question the computer can give answers which are essentially indistinguishable from the answers an English-speaking human being would give, namely, for these particular examples, ‘no’ in the first case and ‘yes’ in the second. So in this very limited sense a machine has already passed a Turing test!The question that we must consider is whether this kind of success actually indicates any genuine understanding on the part of the computer – or, perhaps, on the part of the program itself. Searle’s argument that it does not is to invoke his concept of a ‘Chinese room’. He envisages first of all, that the stories are to be told in Chinese rather than English – surely an inessential change – and that all the operations of the computer’s algorithm for this particular exercise are supplied (in English) as a set of instructions for manipulating counters with Chinese symbols on them. Searle imagines himself doing all the manipulations inside a locked room. The sequences of symbols representing the stories, and then the questions, are fed into the room through some small slot. No other information whatever is allowed in from the outside. Finally, when all the manipulations are complete, the resulting sequence is fed out again through the slot. Since all these manipulations are simply carrying out the algorithm of Schank’s program, it must turn out that this final resulting sequence is simply the Chinese for ‘yes’ or ‘no’, as the case may be, giving the correct answer to the original question in Chinese about a story in Chinese. Now Searle makes it quite clear that he doesn’t understand a word of Chinese, so he would not have the faintest idea what the stories are about. Nevertheless, by correctly carrying out the series of operations which constitute Schank’s algorithm (the instructions for this algorithm having been given to him in English) he would be able to do as well as a Chinese person who would indeed understand the stories. Searle’s point – and I think it is quite a powerful one – is that the mere carying out of a successful algorithm does not in itself imply that any understanding has taken place. The (imagined) Searle, locked in his Chinese room, would not understand a single word of any of the stories!A number of objections have been raised against Searle’s argument. I shall mention only those that I regard as being of serious significance. In the first place, there is perhaps something rather misleading in the phrase ‘not understand a single word’, as used above. Understanding has as much to do with patterns as with individual words. While carrying out algorithms of this kind, one might well begin to perceive something of the patterns that the symbols make without understanding the actual meanings of many of the individual symbols. For example, the Chinese character for ‘hamburger’ (if, indeed, there is such a thing) could be replaced by that for some other dish, say ‘chow mein’, and the stories would not be significantly affected. Nevertheless, it seems to me to be reasonable to suppose that in fact very little of the stories’ actual meanings (even regarding such replacements as being unimportant) would come through if one merely kept following through the details of such an algorithm.In the second place, one must take into account the fact that the execution of even a rather simple computer program would normally be something extraordinarily lengthy and tedious if carried out by human beings manipulating symbols. (This is, after all, why we have computers to do such things for us!) If Searle were actually to perform Schank’s algorithm in the way suggested, he would be likely to be involved with many days, months, or years of extremely boring work in order to answer just a single question – not an altogether plausible activity for a philosopher! However, this does not seem to me to be a serious objection since we are here concerned with matters of principle and not with practicalities. The difficulty arises more with a putative computer program which is supposed to have sufficient complication to match a human brain and thus to pass the Turing test proper. Any such program would have to be horrendously complicated. One can imagine that the operation of this program, in order to effect the reply to even some rather simple Turing-test question, might involve so many steps that there would be no possibility of any single human being carrying out the algorithm by hand within a normal human lifetime. Whether this would indeed be the case is hard to say, in the absence of such a program.9 But, in any case, this question of extreme complication cannot, in my opinion, simply be ignored. It is true that we are concerned with matters of principle here, but it is not inconceivable to me that there might be some ‘critical’ amount of complication in an algorithm which it is necessary to achieve in order that the algorithm exhibit mental qualities. Perhaps this critical value is so large that no algorithm, complicated to that degree, could conceivably be carried out by hand by any human being, in the manner envisaged by Searle.Searle himself has countered this last objection by allowing a whole team of human non-Chinese-speaking symbol manipulators to replace the previous single inhabitant (‘himself’) of his Chinese room. To get the numbers large enough, he even imagines replacing his room by the whole of India, its entire population (excluding those who understand Chinese!) being now engaged in symbol manipulation. Though this would be in practice absurd, it is not in principle absurd, and the argument is essentially the same as before: the symbol manipulators do not understand the story, despite the strong-AI claim that the mere carrying out of the appropriate algorithm would elicit the mental quality of ‘understanding’. However, now another objection begins to loom large. Are not these individual Indians more like the individual neurons in a person’s brain than like the whole brain itself? No-one would suggest that neurons, whose firings apparently constitute the physical activity of a brain in the act of thinking, would themselves individually understand what that person is thinking, so why expect the individual Indians to understand the Chinese stories? Searle replies to this suggestion by pointing out the apparent absurdity of India, the actual country, understanding a story that none of its individual inhabitants understands. A country, he argues, like a thermostat or an automobile, is not in the ‘business of understanding’, whereas an individual person is.This argument has a good deal less force to it than the earlier one. I think that Searle’s argument is at its strongest when there is just a single person carrying out the algorithm, where we restrict attention to the case of an algorithm which is sufficiently uncomplicated for a person actually to carry it out in less than a lifetime. I do not regard his argument as rigorously establishing that there is not some kind of disembodied ‘understanding’ associated with the person’s carrying out of that algorithm, and whose presence does not impinge in any way upon his own consciousness. However, I would agree with Searle that this possibility has been rendered rather implausible, to say the least. I think that Searle’s argument has a considerable force to it, even if it is not altogether conclusive. It is rather convincing in demonstrating that algorithms with the kind of complication that Schank’s computer program possesses cannot have any genuine understanding whatsoever of the tasks that they perform; also, it suggests (but no more) that no algorithm, no matter how complicated, can ever, of itself alone, embody genuine understanding – in contradistinction to the claims of strong AI.There are, as far as I can see, other very serious difficulties with the strong-AI point of view. According to strong AI, it is simply the algorithm that counts. It makes no difference whether that algorithm is being effected by a brain, an electronic computer, an entire country of Indians, a mechanical device of wheels and cogs, or a system of water pipes. The viewpoint is that it is simply the logical structure of the algorithm that is significant for the ‘mental state’ it is supposed to represent, the particular physical embodiment of that algorithm being entirely irrelevant. As Searle points out, this actually entails a form of ‘dualism’. Dualism is a philosophical viewpoint espoused by the highly influential seventeenth century philosopher and mathematician René Descartes, and it asserts that there are two separate kinds of substance: ‘mind-stuff’ and ordinary matter. Whether, or how, one of these kinds of substance might or might not be able to affect the other is an additional question. The point is that the mind-stuff is not supposed to be composed of matter, and is able to exist independently of it. The mind-stuff of strong AI is the logical structure of an algorithm. As I have just remarked, the particular physical embodiment of an algorithm is something totally irrelevant. The algorithm has some kind of disembodied ‘existence’ which is quite apart from any realization of that algorithm in physical terms. How seriously we must take this kind of existence is a question I shall need to return to in the next chapter. It is part of the general question of the Platonic reality of abstract mathematical objects. For the moment I shall sidestep this general issue and merely remark that the supporters of strong AI do indeed seem to be taking the reality at least of algorithms seriously, since they believe that algorithms form the ‘substance’ of their thoughts, their feelings, their understanding, their conscious perceptions. There is a remarkable irony in this fact that, as Searle has pointed out, the standpoint of strong AI seems to drive one into an extreme form of dualism, the very viewpoint with which the supporters of strong AI would least wish to be associated!This dilemma lies behind the scenes of an argument put forward by Douglas Hofstadter (1981) – himself a major proponent of the strong-AI view – in a dialogue entitled ‘A Conversation with Einstein’s Brain’. Hofstadter envisages a book, of absurdly monstrous proportions, which is supposed to contain a complete description of the brain of Albert Einstein. Any question that one might care to put to Einstein can be answered, just as the living Einstein would have, simply by leafing through the book and carefully following all the detailed instructions it provides. Of course ‘simply’ is an utter misnomer, as Hofstadter is careful to point out. But his claim is that in principle the book is completely equivalent, in the operational sense of a Turing test, to a ridiculously slowed-down version of the actual Einstein. Thus, according to the contentions of strong AI, the book would think, feel, understand, be aware, just as though it were Einstein himself, but perhaps living at a monstrously slowed-down rate (so that to the book-Einstein the world outside would seem to flash by at a ridiculously speeded-up rate). Indeed, since the book is supposed to be merely a particular embodiment of the algorithm which constitutes Einstein’s ‘self’, it would actually be Einstein.But now a new difficulty presents itself. The book might never be opened, or it might be continually pored over by innumerable students and searchers after truth. How would the book ‘know’ the difference? Perhaps the book would not need to be opened, its information being retrieved by means of X-ray tomography, or some other technological wizardry. Would Einstein’s awareness be enacted only when the book is being so examined? Would he be aware twice over if two people chose to ask the book the same question at two completely different times? Or would that entail two separate and temporally distinct instances of the same state of Einstein’s awareness? Perhaps his awareness would be enacted only if the book is changed? After all, normally when we are aware of something we receive information from the outside world which affects our memories, and the states of our minds are indeed slightly changed. If so, does this mean that it is (suitable) changes in algorithms (and here I am including the memory store as part of the algorithm) which are to be associated with mental events rather than (or perhaps in addition to) the activation of algorithms? Or would the book-Einstein remain completely self-aware even if it were never examined or disturbed by anyone or anything? Hofstadter touches on some of these questions, but he does not really attempt to answer or to come to terms with most of them.What does it mean to activate an algorithm, or to embody it in physical form? Would changing an algorithm be different in any sense from merely discarding one algorithm and replacing it with another? What on earth does any of this have to do with our feelings of conscious awareness? The reader (unless himself or herself a supporter of strong AI) may be wondering why I have devoted so much space to such a patently absurd idea. In fact, I do not regard the idea as intrinsically an absurd one – mainly just wrong! There is, indeed some force in the reasoning behind strong AI which must be reckoned with, and this I shall try to explain. There is, also, in my opinion, a certain appeal in some of the ideas – if modified appropriately – as I shall also try to convey. Moreover, in my opinion, the particular contrary view expressed by Searle also contains some serious puzzles and seeming absurdities, even though, to a partial extent, I agree with him!Searle, in his discussion, seems to be implicitly accepting that electronic computers of the present-day type, but with considerably enhanced speed of action and size of rapid-access store (and possibly parallel action) may well be able to pass the Turing test proper, in the not-too-distant future. He is prepared to accept the contention of strong AI (and of most other ‘scientific’ viewpoints) that ‘we are the instantiations of any number of computer programs’. Moreover, he succumbs to: ‘Of course the brain is a digital computer. Since everything is a digital computer, brains are too.’10 Searle maintains that the distinction between the function of human brains (which can have minds) and of electronic computers (which, he has argued, cannot) both of which might be executing the same algorithm, lies solely in the material construction of each. He claims, but for reasons he is not able to explain, that the biological objects (brains) can have ‘intentionality’ and ‘semantics’, which he regards as defining characteristics of mental activity, whereas the electronic ones cannot. In itself this does not seem to me to point the way towards any helpful scientific theory of mind. What is so special about biological systems, apart perhaps from the ‘historical’ way in which they have evolved (and the fact that we happen to be such systems), which sets them apart as the objects allowed to achieve intentionality or semantics? The claim looks to me suspiciously like a dogmatic assertion, perhaps no less dogmatic, even, than those assertions of strong AI which maintain that the mere enacting of an algorithm can conjure up a state of conscious awareness!In my opinion Searle, and a great many other people, have been led astray by the computer people. And they, in turn, have been led astray by the physicists. (It is not the physicists’ fault. Even they don’t know everything!) The belief seems to be widespread that, indeed, ‘everything is a digital computer’. It is my intention, in this book, to try to show why, and perhaps how, this need not be the case.HARDWARE AND SOFTWAREIn the jargon of computer science, the term hardware is used to denote the actual machinery involved in a computer (printed circuits, transistors, wires, magnetic storage space, etc.), including the complete specification for the way in which everything is connected up. Correspondingly, the term software refers to the various programs which can be run on the machine. It was one of Alan Turing’s remarkable discoveries that, in effect, any machine for which the hardware has achieved a certain definite degree of complication and flexibility, is equivalent to any other such machine. This equivalence is to be taken in the sense that for any two such machines A and B there would be a specific piece of software which if given to machine A would make it act precisely as though it were machine B; likewise, there would be another piece of software which would make machine B act precisely like machine A. I am using the word ‘precisely’ here to refer to the actual output of the machines for any given input (fed in after the converting software is fed in) and not to the time that each machine might take to produce that output. I am also allowing that if either machine at any stage runs out of storage space for its calculations then it can call upon some (in principle unlimited) external supply of blank ‘rough paper’ – which could take the form of magnetic tape, discs, drums or whatever. In fact, the difference in the time taken by machines A and B to perform some task, might well be a very serious consideration. It might be the case, for example, that A is more than a thousand times faster at performing a particular task than B. It might also be the case that, for the very same machines, there is some other task for which B is a thousand times faster than A. Moreover, these timings could depend very greatly on the particular choices of converting software that are used. This is very much an ‘in-principle’ discussion, where one is not really concerned with such practical matters as achieving one’s calculations in a reasonable time. I shall be more precise in the next section about the concepts being referred to here: the machines A and B are instances of what are called universal Turing machines.In effect, all modern general purpose computers are universal Turing machines. Thus, all general purpose computers are equivalent to one another in the above sense: the differences between them can be entirely subsumed in the software, provided that we are not concerned about differences in the resulting speed of operation and possible limitations on storage size. Indeed, modern technology has enabled computers to perform so swiftly and with such vast storage capacities that, for most ‘everyday’ purposes, neither of these practical considerations actually represents any serious limitation to what is normally needed,* so this effective theoretical equivalence between computers can also be seen at the practical level. Technology has, it seems, transformed entirely academic discussions concerning idealized computing devices into matters which directly affect all our lives!As far as I can make out, one of the most important factors underlying the strong-AI philosophy is this equivalence between physical computing devices. The hardware is seen as being relatively unimportant (perhaps even totally unimportant) and the software, i.e. the program, or the algorithm, is taken to be the one vital ingredient. However, it seems to me that there are also other important underlying factors, coming more from the direction of physics. I shall try to give some indication of what these factors are.What is it that gives a particular person his individual identity? Is it, to some extent, the very atoms that compose his body? Is his identity dependent upon the particular choice of electrons, protons, and other particles that compose those atoms? There are at least two reasons why this cannot be so. In the first place, there is a continual turnover in the material of any living person’s body. This applies in particular to the cells in a person’s brain, despite the fact that no new actual brain cells are produced after birth. The vast majority of atoms in each living cell (including each brain cell) – and, indeed, virtually the entire material of our bodies – has been replaced many times since birth.The second reason comes from quantum physics – and by a strange irony is, strictly speaking, in contradiction with the first! According to quantum mechanics (and we shall see more about this in Chapter 6, p. 360), any two electrons must necessarily be completely identical, and the same holds for any two protons and for any two particles whatever, of any one particular kind. This is not merely to say that there is no way of telling the particles apart: the statement is considerably stronger than that. If an electron in a person’s brain were to be exchanged with an electron in a brick, then the state of the system would be exactly11
the same state as it was before, not merely indistinguishable from it! The same holds for protons and for any other kind of particle, and for whole atoms, molecules, etc. If the entire material content of a person were to be exchanged with corresponding particles in the bricks of his house then, in a strong sense, nothing would have happened whatsoever. What distinguishes the person from his house is the pattern of how his constituents are arranged, not the individuality of the constituents themselves.There is perhaps an analogue of this at an everyday level, which is independent of quantum mechanics, but made particularly manifest to me as I write this, by the electronic technology which enables me to type at a word-processor. If I desire to change a word, say to transform ‘make’ into ‘made’, I may do this by simply replacing the ‘k’ by a ‘d’, or I may choose instead to type out the whole word again. If I do the latter, is the ‘m’ the same ‘m’ as was there before, or have I replaced it with an identical one? What about the ‘e’? Even if I do simply replace ‘k’ by ‘d’, rather than retype the word, there is a moment just between the disappearance of ‘k’ and appearance of ‘d’ when the gap closes and there is (or, at least, sometimes is) a wave of re-alignment down the page as the placement of every succeeding letter (including the ‘e’) is re-calculated, and then re-re-calculated as the ‘d’ is inserted. (Oh, the cheapness of mindless calculation in this modern age!) In any case, all the letters that I see before me on the screen are mere gaps in the track of an electron beam as the whole screen is scanned sixty times each second. If I take any letter whatever and replace it by an identical one, is the situation the same after the replacement, or merely indistinguishable from it? To try to adopt the second viewpoint (i.e. ‘merely indistinguishable’) as being distinct from the first (i.e. ‘the same’) seems footling. At least, it seems reasonable to call the situation the same when the letters are the same. And so it is with the quantum mechanics of identical particles. To replace one particle by an identical one is actually to have done nothing to the state at all. The situation is indeed to be regarded as the same as before. (However, as we shall see in Chapter 6, the distinction is actually not a trivial one in a quantum-mechanical context.)The remarks above concerning the continual turnover of atoms in a person’s body were made in the context of classical rather than quantum physics. The remarks were worded as though it might be meaningful to maintain the individuality of each atom. In fact classical physics is adequate and we do not go badly wrong, at this level of description, by regarding atoms as individual objects. Provided that the atoms are reasonably well separated from their identical counterparts as they move about, one can consistently refer to them as maintaining their individual identities since each atom can be, in effect, tracked continuously, so that one could envisage keeping a tab on each separately. From the point of view of quantum mechanics it would only be a convenience of speech to refer to the individuality of the atoms, but it is a consistent enough description at the level just considered.Let us accept that a person’s individuality has nothing to do with any individuality that one might try to assign to his material constituents. Instead, it must have to do with the configuration, in some sense, of those constituents – let us say the configuration in space or in space-time. (More about that later.) But the supporters of strong AI go further than this. If the information content of such a configuration can be translated into another form from which the original can again be recovered then, so they would claim, the person’s individuality must remain intact. It is like the sequences of letters I have just typed and now see displayed on the screen of my word-processor. If I move them off the screen, they remain coded in the form of certain tiny displacements of electric charge, in some configuration in no clear way geometrically resembling the letters I have just typed. Yet, at any time I can move them back on to the screen, and there they are, just as though no transformation had taken place. If I choose to save what I have just written, then I can transfer the information of the sequences of letters into configurations of magnetization on a disc which I can then remove, and then by switching off the machine I neutralize all the (relevant) tiny charge displacements in it. Tomorrow, I can re-insert the disc, reinstate the little charge displacements and display the letter sequences again on the screen, just as though nothing had happened. To the supporters of strong AI, it is ‘clear’ that a person’s individuality can be treated in just the same way. Like the sequences of letters on my display screen, so these people would claim, nothing is lost of a person’s individuality – indeed nothing would really have happened to it at all – if his physical form were to be translated into something quite different, say into fields of magnetization in a block of iron. They appear even to claim that the person’s conscious awareness would persist while the person’s ‘information’ is in this other form. On this view, a ‘person’s awareness’ is to be taken, in effect, as a piece of software, and his particular manifestation as a material human being is to be taken as the operation of this software by the hardware of his brain and body.It seems that the reason for these claims is that, whatever material form the hardware takes – for example some electronic device – one could always ‘ask’ the software questions (in the manner of a Turing test), and assuming that the hardware performs satisfactorily in computing the replies to these questions, these replies would be identical to those that the person would make whilst in his normal state. (‘How are you feeling this morning?’ ‘Oh, fairly well, thank you, though I have a slightly bothersome headache.’ ‘You don’t feel, then, that there’s . . . er . . . anything odd about your personal identity. . . or something?’ ‘No; why do you say that? It seems rather a strange question to be asking.’ ‘Then you feel yourself to be the same person that you were yesterday?’ ‘Of course I do!’)An idea frequently discussed in this kind of context is the teleportation machine of science fiction.12 It is intended as a means of ‘transportation’ from, say, one planet to another, but whether it actually would be such, is what the discussion is all about. Instead of being physically transported by a spaceship in the ‘normal’ way, the would-be traveller is scanned from head to toe, the accurate location and complete specification of every atom and every electron in his body being recorded in full detail. All this information is then beamed (at the speed of light), by an electromagnetic signal, to the distant planet of intended destination. There, the information is collected and used as the instructions to assemble a precise duplicate of the traveller, together with all his memories, his intentions, his hopes, and his deepest feelings. At least that is what is expected; for every detail of the state of his brain has been faithfully recorded, transmitted, and reconstructed. Assuming that the mechanism has worked, the original copy of the traveller can be ‘safely’ destroyed. Of course the question is: is this really a method of travelling from one place to another or is it merely the construction of a duplicate, together with the murder of the original? Would you be prepared to use this method of ‘travel’ – assuming that the method had been shown to be completely reliable, within its terms of reference? If teleportation is not travelling, then what is the difference in principle between it and just walking from one room into another? In the latter case, are not one’s atoms of one moment simply providing the information for the locations of the atoms of the next moment? We have seen, after all, that there is no significance in preserving the identity of any particular atom. The question of the identity of any particular atom is not even meaningful. Does not any moving pattern of atoms simply constitute a kind of wave of information propagating from one place to another? Where is the essential difference between the propagation of waves which describes our traveller ambling in a commonplace way from one room to the other and that which takes place in the teleportation device?Suppose it is true that teleportation does actually ‘work’, in the sense that the traveller’s own ‘awareness’ is actually reawakened in the copy of himself on the distant planet (assuming that this question has genuine meaning). What would happen if the original copy of the traveller were not destroyed, as the rules of this game demand? Would his ‘awareness’ be in two places at once? (Try to imagine your response to being told the following: ‘Oh dear, so the drug we gave you before placing you in the Teleporter has worn off prematurely has it? That is a little unfortunate, but no matter. Anyway, you will be pleased to hear that the other you – er, I mean the actual you, that is – has now arrived safely on Venus, so we can, er, dispose of you here – er, I mean of the redundant copy here. It will, of course, be quite painless.’) The situation has an air of paradox about it. Is there anything in the laws of physics which could render teleportation in principle impossible? Perhaps, on the other hand, there is nothing in principle against transmitting a person, and a person’s consciousness, by such means, but that the ‘copying’ process involved would inevitably destroy the original? Might it then be that the preserving of two viable copies is what is impossible in principle? I believe that despite the outlandish nature of these considerations, there is perhaps something of significance concerning the physical nature of consciousness and individuality to be gained from them. I believe that they provide one pointer, indicating a certain essential role for quantum mechanics in the understanding of mental phenomena. But I am leaping ahead. It will be necessary to return to these matters after we have examined the structure of quantum theory in Chapter 6 (cf. p. 348).Let us see how the point of view of strong AI relates to the teleportation question. We shall suppose that somewhere between the two planets is a relay station, where the information is temporarily stored before being re-transmitted to its final destination. For convenience, this information is not stored in human form, but in some magnetic or electronic device. Would the traveller’s ‘awareness’ be present in association with this device? The supporters of strong AI would have us believe that this must be so. After all, they say, any question that we might choose to put to the traveller could in principle be answered by the device, by ‘merely’ having a simulation set up for the appropriate activity of his brain. The device would contain all the necessary information; and the rest would just be a matter of computation. Since the device would reply to questions exactly as though it were the traveller, then (Turing test!) it would be the traveller. This all comes back to the strong-AI contention that the actual hardware is not important with regard to mental phenomena. This contention seems to me to be unjustified. It is based on the presumption that the brain (or the mind) is, indeed, a digital computer. It assumes that no specific physical phenomena are being called upon, when one thinks, that might demand the particular physical (biological, chemical) structure that brains actually have.No doubt it would be argued (from the strong-AI point of view) that the only assumption that is really being made is that the effects of any specific physical phenomena which need to be called upon can always be accurately modelled by digital calculations. I feel fairly sure that most physicists would argue that such an assumption is actually a very natural one to make on the basis of our present physical understandings. I shall be presenting the reasons for my own contrary view in later chapters (where I shall also need to lead up to why I believe that there is even any appreciable assumption being made). But, just for the moment, let us accept this (commonly held) view that all the relevant physics can always be modelled by digital calculations. Then the only real assumption (apart from questions of time and calculation space) is the ‘operational’ one that if something acts entirely like a consciously aware entity, then one must also maintain that it ‘feels’ itself to be that entity.The strong-AI view holds that, being ‘just’ a hardware question, any physics actually being called upon in the workings of the brain can necessarily be simulated by the introduction of appropriate converting software. If we accept the operational viewpoint, then the question rests on the equivalence of universal Turing machines, and on the fact that any algorithm can, indeed, be effected by such a machine – together with the presumption that the brain acts according to some kind of algorithmic action. It is time for me to be more explicit about these intriguing and important concepts.NOTES1. See, for example, Gardner (1958), Gregory (1981), and references contained therein.2. See, for example, Resnikoff and Wells (1984), pp. 181-4. For a classic account of calculating prodigies generally, see Rouse Ball (1892); also Smith (1983).3. See Gregory (1981), pp. 285-7, Grey Walter (1953).4. This example is quoted from Delbrück (1986).5. See the articles by O’Connell (1988) and Keene (1988). For more information about computer chess, see Levy (1984).6. Of course, most chess problems are designed to be hard for humans to solve. It would probably not be too difficult to construct a chess problem that human beings would not find enormously hard, but which present-day chess-problem solving computers could not solve in a thousand years. (What would be required would be a fairly obvious plan, a very large number of moves deep. Problems are known, for example, requiring some 200 moves – more than enough!) This suggests an interesting challenge.7. Throughout this book I have adopted Searle’s terminology ‘strong AI’ for this extreme viewpoint, just to be specific. The term ‘functionalism’ is frequently used for what is essentially the same viewpoint, but perhaps not always so specifically. Some proponents of this kind of view are Minsky (1968), Fodor (1983), Hofstadter (1979), and Moravec (1989).8. See Searle (1987), p. 211, for an example of such a claim.9. In his criticism of Searle’s original paper, as reprinted in The Mind’s I, Douglas Hofstadter complains that no human being could conceivably ‘internalize’ the entire description of another human being’s mind, owing to the complication involved. Indeed not! But as I see it, that is not entirely the point. One is concerned merely with the carrying out of that part of an algorithm which purports to embody the occurrence of a single mental event. This could be some momentary ‘conscious realization’ in the answering of a Turing-test question, or it could be something simpler. Would any such ‘event’ necessarily require an algorithm of stupendous complication?10. See pp. 368, 372 in Searle’s (1980) article in Hofstadter and Dennett (1981).11. Some readers, knowledgeable about such matters, might worry about a certain sign difference. But even that (arguable) distinction disappears if we rotate one of the electrons completely through 360° as we make the interchange! (See Chapter 6, p. 360 for an explanation.)12. See the Introduction to Hofstadter and Dennett (1981).

2ALGORITHMS AND TURING MACHINESBACKGROUND TO THE ALGORITHM CONCEPTWHAT PRECISELY IS an algorithm, or a Turing machine, or a universal Turing machine? Why should these concepts be so central to the modern view of what could constitute a ‘thinking device’? Are there any absolute limitations to what an algorithm could in principle achieve? In order to address these questions adequately, we shall need to examine the idea of an algorithm and of Turing machines in some detail.In the various discussions which follow, I shall sometimes need to refer to mathematical expressions. I appreciate that some readers may be put off by such things, or perhaps find them intimidating. If you are such a reader, I ask your indulgence, and recommend that you follow the advice I have given in my ‘Note to the reader’ on p. viii! The arguments given here do not require mathematical knowledge beyond that of elementary school, but to follow them in detail, some serious thought would be required. In fact, most of the descriptions are quite explicit, and a good understanding can be obtained by following the details. But much can also be gained even if one simply skims over the arguments in order to obtain merely their flavour. If, on the other hand, you are an expert, I again ask your indulgence. I suspect that it may still be worth your while to look through what I have to say, and there may indeed be a thing or two to catch your interest.The word ‘algorithm’ comes from the name of the ninth century Persian mathematician Abu Ja’far Mohammed ibn Mûsâ al-Khowârizm who wrote an influential mathematical textbook, in about 825 AD, entitled ‘Kitab al-jabr wa’l-muqabala’. The way that the name ‘algorithm’ has now come to be spelt, rather than the earlier and more accurate ‘algorism’, seems to have been due to an association with the word ‘arithmetic’. (It is noteworthy, also, that the word ‘algebra’ comes from the Arabic ‘al-jabr’ appearing in the title of his book.)Instances of algorithms were, however, known very much earlier than al-Khowârizm’s book. One of the most familiar, dating from ancient Greek times (c. 300 BC), is the procedure now referred to as Euclid’s algorithm for finding the highest common factor of two numbers. Let us see how this works. It will be helpful to have a specific pair of numbers in mind, say 1365 and 3654. The highest common factor is the largest single whole number that divides into each of these two numbers exactly. To apply Euclid’s algorithm, we divide one of our two numbers by the other and take the remainder: 1365 goes twice into 3654, with remainder 924 (= 3654 – 2730). We now replace our original two numbers by this remainder, namely 924, and the number we just divided by, namely 1365, in that order. We repeat what we just did using this new pair: 924 goes once into 1365, with remainder 441. This gives another new pair, 441 and 924, and we divide 441 into 924 to get the remainder 42 (= 924 – 882), and so on until we get a division that goes exactly. Setting all this out, we get:The last number we divided by, namely 21, is the required highest common factor.Euclid’s algorithm itself is the systematic procedure by which we found this factor. We have just applied this procedure to a particular pair of numbers, but the procedure itself applies quite generally, to numbers of any size. For very large numbers, the procedure could take a very long time to carry out, and the larger the numbers, the longer the procedure will tend to take. But in any specific case the procedure will eventually terminate and a definite answer will be obtained in a finite number of steps. At each step it is perfectly clear-cut what the operation is that has to be performed, and the decision as to the moment at which the whole process has terminated is also perfectly clear-cut. Moreover, the description of the whole procedure can be presented in finite terms, despite the fact that it applies to natural numbers of unlimited size. (The ‘natural numbers’, are simply the ordinary non-negative1 whole numbers 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, . . .) Indeed, it is easy to construct a (finite) ‘flow chart’ to describe the full logical operation of Euclid’s algorithm (see below).It should be noted that this procedure is still not quite broken down into its most elementary parts since it has been implicitly assumed that we already ‘know’ how to perform the necessary basic operation of obtaining the remainder from a division, for two arbitrary natural numbers A and B. That operation is again algorithmic – performed by the very familiar procedure for division that we learn at school. That procedure is actually rather more complicated than the rest of Euclid’s algorithm, but again a flow chart may be constructed. The main complication results from the fact that we would (presumably) be using the standard ‘denary’ notation for natural numbers, so that we would need to list all our multiplication tables and worry about carrying, etc. If we had simply used a succession of n marks of some kind to represent the number n – for example,  to represent five – then the forming of a remainder would be seen as a very elementary algorithmic operation. In order to obtain the remainder when A is divided by B we simply keep removing the succession of marks representing B from that representing A until there are not enough marks left to perform the operation again. The last remaining succession of marks provides the required answer. For example, to obtain the remainder when seventeen is divided by five we simply proceed by removing successions of  from  as follows:and the answer is clearly two since we cannot perform the operation again.A flow chart which finds the remainder in a division, achieved by this means of repeated subtraction, given as on p. 44. To complete the entire flow chart for Euclid’s algorithm, we substitute this chart for forming a remainder into the box at the centre right in our original chart. This kind of substitution of one algorithm into another is a common computer-programming procedure. The algorithm on p. 44 for finding a remainder is an example of a subroutine, that is, it is a (normally previously known) algorithm which is called upon and used by the main algorithm as a part of its operation.Of course the representation of the number n simply as n spots is very inefficient when large numbers are involved, which is why we normally use a more compact notation such as the standard (denary) system. However we shall not be much concerned with the efficiency of operations or notations here. We are concerned instead with the question of what operations can in principle be performed algorithmically. What is algorithmic if we use one notation for numbers is also algorithmic if we use the other. The only differences lie in the detail and complication in the two cases.Euclid’s algorithm is just one among the numerous, often classical, algorithmic procedures that are to be found throughout mathematics. But it is perhaps remarkable that despite the ancient historic origins of specific examples of algorithms, the precise formulation of the concept of a general algorithm dates only from this century. In fact, various alternative descriptions of this concept have been given, all in the 1930s. The most direct and persuasive of these, and also historically the most important, is in terms of the concept known as a Turing machine. It will be appropriate for us to examine these ‘machines’ in some detail.One thing to bear in mind about a Turing ‘machine’ will be that it is a piece of ‘abstract mathematics’ and not a physical object. The concept was introduced by the English mathematician, code-breaker extraordinary and seminal computer scientist Alan Turing in 1935-6 (Turing 1937), in order to tackle a very broad-ranging problem, known as the Entscheidungsproblem, partially posed by the great German mathematician David Hilbert in 1900, at the Paris International Congress of Mathematicians (‘Hilbert’s tenth problem’), and more completely, at the Bologna International Congress in 1928. Hilbert had asked for no less than a general algorithmic procedure for resolving mathematical questions – or, rather, for an answer to the question of whether or not such a procedure might in principle exist. Hilbert also had a programme for placing mathematics on an unassailably sound foundation, with axioms and rules of procedure which were to be laid down once and for all, but by the time Turing produced his great work, that programme had already suffered a crushing blow from a startling theorem proved in 1931 by the brilliant Austrian logician Kurt Gödel. We shall be considering the Gödel theorem and its significance in Chapter 4. The problem of Hilbert’s that concerned Turing (the Entscheidungsproblem) went beyond any particular formulation of mathematics in terms of axiomatic systems. The question was: is there some general mechanical procedure which could, in principle, solve all the problems of mathematics (belonging to some suitably well-defined class) one after the other?Part of the difficulty in answering this question was to decide what is to be meant by a ‘mechanical procedure’. The concept lay outside the normal mathematical ideas of the time. In order to come to grips with it, Turing tried to imagine how the concept of a ‘machine’ could be formalized, its operation being broken down into elementary terms. It seems clear that Turing also regarded a human brain to be an example of a ‘machine’ in his sense, so whatever the activities might be that are carried out by human mathematicians when they tackle their problems of mathematics, these also would have to come under the heading of ‘mechanical procedures’.Whilst this view of human thinking appears to have been valuable to Turing in the development of his highly important concept, it is by no means necessary for us to adhere to it. Indeed, by making precise what is meant by a mechanical procedure, Turing actually showed that there are some perfectly well-defined mathematical operations which cannot, in any ordinary sense, be called mechanical! There is perhaps some irony in the fact that this aspect of Turing’s own work may now indirectly provide us with a possible loophole to his own viewpoint concerning the nature of mental phenomena. However, this is not our concern for the moment. We need first to find out what Turing’s concept of a mechanical procedure actually is.TURING’S CONCEPTLet us try to imagine a device for carrying out some (finitely definable) calculational procedure. What general form would such a device take? We must be prepared to idealize a little and not worry too much about practicalities: we are really thinking of a mathematically idealized ‘machine’. We want our device to have a discrete set of different possible states, which are finite in number (though perhaps a very large number). We call these the internal states of the device. However we do not want to limit the size of the calculations that our device will perform in principle. Recall Euclid’s algorithm described above. There is, in principle, no limit to the size of the numbers on which that algorithm can act. The algorithm – or the general calculational procedure – is just the same no matter how large the numbers are. For very large numbers, the procedure may indeed take a very long time, and a considerable amount of ‘rough paper’ may be needed on which to perform the actual calculations. But the algorithm is the same finite set of instructions no matter how big the numbers.Thus, although it has a finite number of internal states, our device must be able to deal with an input which is not restricted in its size. Moreover, the device must be allowed to call upon an unlimited external storage space (our ‘rough paper’) for its calculations, and also to be able to produce an output of unlimited size. Since our device has only a finite number of distinct internal states, it cannot be expected to ‘internalize’ all the external data nor all the results of its own calculations. Instead it must examine only those parts of the data or previous calculations that it is immediately dealing with, and then perform whatever operation it is required to perform on them. It can note down, perhaps in the external storage space, the relevant results of that operation, and then proceed in a precisely determined way to the next stage of operation. It is the unlimited nature of the input, calculation space, and output which tells us that we are considering only a mathematical idealization rather than something that could be actually constructed in practice (see Fig. 2.1). But it is an idealization of great relevance. The marvels of modern computer technology have provided us with electronic storage devices which can, indeed, be treated as unlimited for most practical purposes.In fact, the type of storage space that has been referred to as ‘external’ in the above discussion could be regarded as actually part of the internal workings of a modern computer. It is perhaps a technicality whether a certain part of the storage space is to be regarded as internal or external. One way of referring to this division between the ‘device’ and the ‘external’ part would be in terms of hardware and software. The internal part could then be the hardware and the external part, the software. I shall not necessarily stick to this, but whichever way one looks at it, Turing’s idealization is indeed remarkably well approximated by the electronic computers of today.Fig. 2.1. A strict Turing machine requires an infinite tape!The way that Turing pictured the external data and storage space was in terms of a ‘tape’ with marks on it. This tape would be called upon by the device and ‘read’ as necessary, and the tape could be moved backwards or forwards by the device, as part of its operation. The device could also place new marks on the tape where required, and could obliterate old ones, allowing the same tape to act as external storage (i.e. ‘rough paper’) as well as input. In fact it is helpful not to make any clear distinction between ‘external storage’ and ‘input’ because in many operations the intermediate results of a calculation can play a role just like that of new data. Recall that with Euclid’s algorithm we kept replacing our original input (the numbers A and B) by the results of the different stages of the calculation. Likewise, the same tape can be used for the final output (i.e. the ‘answer’). The tape will keep running back and forth through the device so long as further calculations need to be performed. When the calculation is finally completed, the device comes to a halt and the answer to the calculation is displayed on that part of the tape which lies to one side of the device. For definiteness, let us suppose that the answer is always displayed on the left, while all the numerical data in the input, together with the specification of the problem to be solved, always goes in on the right.For my own part, I feel a little uncomfortable about having our finite device moving a potentially infinite tape backwards and forwards. No matter how lightweight its material, an infinite tape might be hard to shift! Instead, I prefer to think of the tape as representing some external environment through which our finite device can move. (Of course, with modern electronics, neither the ‘tape’ nor the ‘device’ need actually ‘move’ in the ordinary physical sense, but such ‘movement’ is a convenient way of picturing things.) On this view, the device receives all its input from this environment. It uses the environment as its ‘rough paper’. Finally it writes out its output on this same environment.In Turing’s picture the ‘tape’ consists of a linear sequence of squares, which is taken to be infinite in both directions. Each square on the tape is either blank or contains a single mark.* The use of marked or unmarked squares illustrates that we are allowing our ‘environment’ (i.e. the tape) to be broken down and described in terms of discrete (as opposed to continuous) elements. This seems to be a reasonable thing to do if we wish our device to function in a reliable and absolutely definite way. We are, however, allowing this ‘environment’ to be (potentially) infinite, as a feature of the mathematical idealization that we are using, but in any particular case the input, calculation and output must always be finite. Thus, although the tape is taken to be infinitely long, there must only be a finite number of actual marks on it. Beyond a certain point in each direction the tape must be entirely blank.We indicate a blank square by the symbol ‘O’ and a marked one by the symbol ‘1’, e.g.:We need our device to ‘read’ the tape, and we shall suppose that it does this one square at a time, and after each operation moves just one square to the right or left. There is no loss in generality involved in that. A device which reads n squares at a time or moves k squares at a time can easily be modelled by another device which reads and moves just one square at a time. A movement of k squares can be built up of k movements of one square, and by storing up n readings of one square it can behave as though it reads all n squares at once.What, in detail, can such a device do? What is the most general way in which something that we would describe as ‘mechanical’ might function? Recall that the internal states of our device are to be finite in number. All we need to know, beyond this finiteness is that the behaviour of the device is completely determined by its internal state and by the input. This input we have simplified to being just one of the two symbols ‘0’ or ‘1’. Given its initial state and this input, the device is to operate completely deterministically: it changes its internal state to some other (or possibly the same) internal state; it replaces the 0 or 1 that it is just reading by the same or different symbol 0 or 1; it moves one square either to the right or left; finally, it decides whether to continue the calculation or to terminate it and come to a halt.To define the operation of our device in an explicit way, let us first number the different internal states, say by the labels 0, 1, 2, 3, 4, 5, . . .; the operation of the device, or Turing machine, would then be completely specified by some explicit list of replacements, such as:The large figure on the left-hand side of the arrow is the symbol on the tape that the device is in the process of reading, and the device replaces it by the large figure at the middle on the right. R tells us that the device is to move one square to the right along the tape and L tells us that it is to move by one step to the left. (If, as with Turing’s original descriptions, we think of the tape moving rather than the device, then we must interpret R as the instruction to move the tape one square to the left and L as moving it one square to the right.) The word STOP indicates that the calculation has been completed and the device is to come to a halt. In particular, the second instruction 01 → 131L tells us that if the device is in internal state 0 and reads 1 on the tape then it must change to internal state 13, leave the 1 as a 1 on the tape, and move one square along the tape to the left. The last instruction 2591 → 0OR.STOP tells us that if the device is in state 259 and reads 1 on the tape, then it must revert to state 0, erase the 1 to produce O on the tape, move one square along the tape to the right, and terminate the calculation.Instead of using the numerals 0, 1, 2, 3, 4, 5, . . . for labelling the internal states, it would be somewhat more in keeping with the above notation for marks on the tape if we were to use symbols made up of just 0s and is. We could simply use a succession of n is to label the state n if we choose, but that is inefficient. Instead, let us use the binary numbering system, which is now a familiar mode of notation:Here the final digit on the right refers to the ‘units’ just as it does in the standard (denary) notation, but the digit just before it refers to ‘twos’ rather than ‘tens’. The one before that refers to ‘fours’ rather than ‘hundreds’ and before that, to ‘eights’ rather than ‘thousands’, and so on, the value of each successive digit, as we move to the left, being the successive powers of two: 1, 2, 4 (= 2 × 2), 8 (= 2 × 2 × 2), 16 (= 2 × 2 × 2 × 2), 32 (= 2 × 2 × 2 × 2 × 2), etc. (For some other purposes that we shall come to later, we shall also sometimes find it useful to use a base other than two or ten to represent natural numbers: e.g. in base three, the denary number 64 would be written 2101, each digit having a value which is now a power of three: 64 = (2 × 33) + 32 + 1; cf. Chapter 4, p. 138, footnote.)Using such a binary notation for the internal states, the specification of the above Turing machine would now be:In the above, I have also abbreviated R.STOP to STOP, since we may as well assume that L.STOP never occurs so that the result of the final step in the calculation is always displayed at the left of the device, as part of the answer.Let us suppose that our device is in the particular internal state represented by the binary sequence 11010010 and is in the midst of a calculation for which the tape is given as on p. 49, and we apply the instruction 110100100 → 111L. The particular digit on the tape that is being read (here the digit ‘0’) is indicated by a larger figure, to the right of the string of symbols representing the internal state. In the example of a Turing machine as partly specified above (and which I have made up more or less at random), the ‘0’ which is being read would be replaced by a ‘1’ and the internal state would be changed to ‘11’; then the device would be moved one step to the left:The device is now ready to read another digit, again a ‘0’. According to the table, it now leaves this ‘0’ unchanged, but replaces the internal state by ‘100101’ and moves back along the tape to the right by one step. Now it reads ‘1’, and somewhere down the table would be a further instruction as to what replacement to make in its internal state, whether it should change the digit it is reading, and in which direction it should move along the tape. It would continue this way until it reaches a STOP, at which point (after it moves one further step to the right) we imagine a bell ringing to alert the operator of the machine that the calculation has been completed.We shall suppose that the machine is always started with internal state ‘0’ and that all the tape to the left of the reading device is initially blank. The instructions and data are all fed in at the right. As mentioned earlier, this information which is fed in is always to take the form of a finite string of 0s and 1s, followed by blank tape (i.e. 0s). When the machine reaches STOP, the result of the calculation appears on the tape to the left of the reading device.Since we wish to be able to include numerical data as part of our input, we shall want to have a way of describing ordinary numbers (by which I here mean the natural numbers 0, 1, 2, 3, 4,. . .) as part of the input. One way to do this might be simply to use a string of n 1s to represent the number n (although this could give us a difficulty with the natural number zero):This primitive numbering system is referred to (rather illogically) as the unary system. Then the symbol ‘0’ could be used as a space to separate different numbers from one another. It is important that we have such a means of separating numbers from one another since many algorithms act on sets of numbers rather than on just single numbers. For example, for Euclid’s algorithm, our device would need to act on the pair of numbers A and B. Turing machines can be written down, without great difficulty, which effect this algorithm. As an exercise, some dedicated readers might perhaps care to verify that the following explicit description of a Turing machine (which I shall call EUC) does indeed effect Euclid’s algorithm when applied to a pair of unary numbers separated by a 0:Before embarking on this, however, it would be wise for any such reader to start with something much simpler, such as the Turing machine UN + 1:which simply adds one to a unary number. To check that UN + 1 does just that, let us imagine that it is applied to, say, the tapewhich represents the number 4. We take the device to be initially somewhere off to the left of the Is. It is in internal state 0 and reads a 0. This it leaves as 0, according to the first instruction, and it moves off one step to the right, staying in internal state 0. It keeps doing this, moving one step to the right until it meets the first 1. Then the second instruction comes into play: it leaves the 1 as a 1 and moves to the right again, but now in internal state 1. In accordance with the fourth instruction, it stays in internal state l, leaving the Is alone, moving along to the right until it reaches the first 0 following the Is. The third instruction then tells it to change that 0 to a 1, move one further step to the right (recall that STOP stands for R.STOP) and then halt. Thus, another 1 has been added to the string of Is, and the 4 of our example has indeed been changed to 5, as required.As a somewhat more taxing exercise, one may check that the machine UN × 2, defined bydoubles a unary number, as it is intended to.In the case of EUC, to get the idea of what is involved, some suitable explicit pair of numbers can be tried, say 6 and 8. The reading device is, as before, taken to be in state 0 and initially on the left, and the tape would now be initially marked as:After the Turing machine comes to a halt, many steps later, we get a tape markedwith the reading device to the right of the non-zero digits. Thus the required highest common factor is (correctly) given as 2.The full explanation of why EUC (or, indeed, UN × 2) actually does what it is supposed to do involves some subtleties and would be rather more complicated to explain than the machine is complicated itself – a not uncommon feature of computer programs! (To understand fully why an algorithmic procedure does what it is supposed to involves insights. Are ‘insights’ themselves algorithmic? This is a question that will have importance for us later.) I shall not attempt to provide such an explanation here for the examples EUC or UN × 2. The reader who does check them through will find that I have taken a very slight liberty with Euclid’s actual algorithm in order to express things more concisely in the required scheme. The description of EUC is still somewhat complicated, comprising 22 elementary instructions for 11 distinct internal states. Most of the complication is of a purely organizational kind. It will be observed, for example, that of the 22 instructions, only 3 actually involve altering marks on the tape! (Even for UN × 2 I have used 12 instructions, half of which involve altering the marks.)BINARY CODING OF NUMERICAL DATAThe unary system is exceedingly inefficient for the representation of numbers of large size. Accordingly, we shall usually use the binary number system, as described earlier. However, we cannot just do this directly, attempting to read the tape simply as a binary number. As things stand, there would be no way of telling when the binary representation of the number has come to an end and the infinite succession of 0s representing the blank tape on the right begins. We need some notation for terminating the binary description of a number. Moreover, we shall often want to feed in several numbers, as with the pair of numbers2 required for Euclid’s algorithm. As things stand, we cannot distinguish the spaces between numbers from the 0s or strings of 0s that appear as parts of the binary representation of single numbers. In addition, we might perhaps also want to include all kinds of complicated instructions on the input tape, as well as numbers. In order to overcome these difficulties, let us adopt a procedure which I shall refer to as contraction, according to which any string of 0s and 1s (with a finite total number of Is) is not simply read as a binary number, but is replaced by a string of 0s, 1s, 2s, 3s, etc., by a prescription whereby each digit of the second sequence is simply the number of Is lying between successive 0s of the first sequence. For example, the sequencewould be replacedWe can now read the numbers 2, 3, 4, . . . as markers or instructions of some kind. Indeed, let us regard 2 as simply a ‘comma’, indicating the space between two numbers, whereas 3, 4, 5, . . . could, according to our wishes, represent various instructions or notations of interest, such as ‘minus sign’, ‘plus’, ‘times’, ‘go to the location with the following number’, ‘iterate the previous operation the following number of times’, etc. We now have various strings of 0s, and 1s which are separated by higher digits. The former are to represent ordinary numbers written in the binary scale. Thus, the above would read (with ‘comma’ for ‘2’):(binary number 1001) comma (binary number 11) comma . . .Using standard Arabic notation ‘9’, ‘3’, ‘4’, ‘0’ for the respective binary numbers 1001, 11, 100, 0, we get, for the entire sequence:9, 3, 4 (instruction 3) 3 (instruction 4) 0,In particular, this procedure gives us a means of terminating the description of a number (and thereby distinguishing it from an infinite stretch of blank tape on the right) simply by using a comma at the end. Moreover, it enables us to code any finite sequence of natural numbers, written in the binary notation, as a single sequence of 0s and 1s, where we use commas to separate the numbers. Let us see how this works in a specific case. Consider the sequence5, 13, 0, 1, 1, 4,for example. In binary notation this is101, 1101, 0, 1, 1, 100,which is coded on the tape, by expansion (i.e. the inverse of the above contraction procedure), as. . . 000010010110101001011001101011010110100011000 . . .To achieve this coding in a simple direct way we can make replacements in our original sequence of binary numbers as followsand then adjoin an unlimited supply of 0s at both ends. It is made clearer how this has been applied to the above tape, if we space it out:0000 10 0 10 110 10 10 0 10 110 0 110 10 110 10 110 10 0 0 110 00I shall refer to this notation for (sets of) numbers as the expanded binary notation. (So, in particular, the expanded binary form of 13 is 1010010.)There is one final point that should be made about this coding. It is just a technicality, but necessary for completeness.3 In the binary (or denary) representation of natural numbers there is a slight redundancy in that 0s placed on the far left of an expression do not ‘count’ – and are normally omitted, e.g. 00110010 is the same binary number as 110010 (and 0050 is the same denary number as 50). This redundancy extends to the number zero itself, which can be written 000 or 00 just as well as 0. Indeed a blank space should, logically, denote zero as well! In ordinary notation that would lead to great confusion, but it fits in well with the notation just described above. Thus a zero between two commas can just as well be written as two commas next to one another (,,) which would be coded on the tape as two pairs 11 separated by a single 0:. . . 001101100 . . .Thus the above set of six numbers can also be written in binary notation as101,1101,,1,1,100,and coded on the tape, in expanded binary form, as. . . 00001001011010100101101101011010110100011000 . . .(which has one 0 missing from the sequence that we had before).We can now consider a Turing machine for effecting, say, Euclid’s algorithm, applying it to pairs of numbers written in the expanded binary notation. For example, for the pair of numbers 6, 8 that we considered earlier, instead of using. . . 0000000000011111101111111100000 . . .,as we did before, we consider the binary representations of 6 and 8, namely 110 and 1000, respectively. The pair is6, 8, i.e., in binary notation, 110, 1000,which, by expansion, is coded as the tape. . . 00000101001101000011000000 . . .For this particular pair of numbers there is no gain in conciseness from the unary form. Suppose, however, that we take, say, the (denary) numbers 1583169 and 8610. In binary notation these would be110000010100001000001, 10000110100010,so we have the pair coded as the tape. . . 001010000001001000001000000101101000001010010000100110 . . .which all fits on two lines, whereas in the unary notation, the tape representing ‘1583169, 8610’ would more than fill this entire book!A Turing machine that effects Euclid’s algorithm when the numbers are expressed in expanded binary notation could, if desired, be obtained simply by adjoining to EUC a suitable pair of subroutine algorithms which translate between unary and expanded binary. This would actually be extremely inefficient, however, since the inefficiency of the unary numbering system would still be ‘internally’ present and would show up in the slowness of the device and in the inordinate amount of external ‘rough paper’ (which would be on the left-hand part of the tape) that would be needed. A more efficient Turing machine for Euclid’s algorithm operating entirely within expanded binary can also be given, but it would not be particularly illuminating for us here.Instead, in order to illustrate how a Turing machine can be made to operate on expanded binary numbers, let us try something a good deal simpler than Euclid’s algorithm, namely the process of simply adding one to a natural number. This can be effected by the Turing machine (which I shall call XN + 1):Again, some dedicated readers might care to check that this Turing machine actually does what it is supposed to do, by applying it to, say, the number 167, which has binary representation 10100111 and so would be given by the tape. . . 00001001000101010011000 . . .To add one to a binary number, we simply locate the final 0 and change it to 1 and then replace all the 1s which follow by 0s, e.g. 167 + 1 = 168 is written in binary notation as10100111 + 1 = 10101000.Thus our ‘adding-one’ Turing machine should replace the aforementioned tape by. . . 0000100100100001100000 . . .which indeed it does.Note that even the very elementary operation of simply adding one is a bit complicated with this notation, using fifteen instructions and eight different internal states! Things were a lot simpler with the unary notation, of course, since ‘adding one’ then simply means extending the string of Is by one further 1, so it is not surprising that our machine UN + 1 was more basic. However, for very large numbers, UN + 1 would be exceedingly slow because of the inordinate length of tape required, and the more complicated machine XN + 1, which operates with the more compact expanded binary notation, would be better.As an aside, I point out an operation for which the Turing machine actually looks simpler for expanded binary than for unary notation, namely multiplying by two. Here, the Turing machine XN × 2, given byachieves this in expanded binary, whereas the corresponding machine in unary notation, UN × 2, which was described earlier, is a good deal more complicated!This gives us some idea of what Turing machines can do at a very basic level. As might be expected, they can, and do, get vastly more complicated than this when operations of some complexity are to be performed. What is the ultimate scope of such devices? Let us consider this question next.THE CHURCH-TURING THESISOnce one has gained some familiarity with constructing simple Turing machines, it becomes easy to satisfy oneself that the various basic arithmetical operations, such as adding two numbers together, or multiplying them, or raising one number to the power of another, can indeed all be effected by specific Turing machines. It would not be too cumbersome to give such machines explicitly, but I shall not bother to do this here. Operations where the result is a pair of natural numbers, such as division with a remainder, can also be provided – or where the result is an arbitrarily large finite set of numbers. Moreover Turing machines can be constructed for which it is not specified ahead of time which arithmetical operation it is that needs to be performed, but the instructions for this are fed in on the tape. Perhaps the particular operation that has to be performed depends, at some stage, upon the result of some calculation that the machine has had to perform at some earlier stage. (‘If the answer to that calculation was greater than so-and-so, do this; otherwise, do that.’) Once it is appreciated that one can make Turing machines which perform arithmetic or simple logical operations, it becomes easier to imagine how they can be made to perform more complicated tasks of an algorithmic nature. After one has played with such things for a while, one is easily reassured that a machine of this type can indeed be made to perform any mechanical operation whatever! Mathematically, it becomes reasonable to define a mechanical operation to be one that can be carried out by such a machine. The noun ‘algorithm’ and the adjectives ‘computable’, ‘recursive’, and ‘effective’ are all used by mathematicians to denote the mechanical operations that can be performed by theoretical machines of this type – the Turing machines. So long as a procedure is sufficiently clear-cut and mechanical, then it is reasonable to believe that a Turing machine can indeed be found to perform it. This, after all, was the whole point of our (i.e. Turing’s) introductory discussion motivating the very concept of a Turing machine.On the other hand, it still could be felt that the design of these machines was perhaps unnecessarily restrictive. Allowing the device to read only one binary digit (0 or 1) at a time, and to move only one space at a time along only a single one-dimensional tape seems at first sight to be limiting. Why not allow four or five, or perhaps one thousand separate tapes, with a great number of interconnected reading devices running all at once? Why not allow a whole plane of squares of 0s and 1s (or perhaps a three-dimensional array) rather than insisting on a one-dimensional tape? Why not allow other symbols from some more complicated numbering system or alphabet? In fart, none of these changes makes the slightest difference to what can be in principle achieved, though some make a certain amount of difference to the economy of the operations (as would certainly be the case if we allowed more than one tape). The class of operations performed, and thus come under the heading of ‘algorithms’ (or ‘computations’ or ‘effective procedures’ or ‘recursive operations’), would be precisely the same as before even if we broadened the definition of our machines in all these ways at once!We can see that there is no necessity to have more than one tape, so long as the device can keep finding new space on the given tape, as required. For this, it may need to keep shunting data from one place to another on the tape. This may be ‘inefficient’, but it does not limit what can be in principle achieved.4 Likewise, using more than one Turing device in parallel action – which is an idea that has become fashionable in recent years, in connection with attempts to model human brains more closely – does not in principle gain anything (though there may be an improved speed of action under certain circumstances). Having two separate devices which do not directly communicate with one another achieves no more than having two which do communicate; and if they communicate, then, in effect, they are just a single device!What about Turing’s restriction to a one-dimensional tape? If we think of this tape as representing the ‘environment’, we might prefer to think of it as a planar surface rather than as a one-dimensional tape, or perhaps as a three-dimensional space. A planar surface might seem to be closer to what is needed for a ‘flow chart’ (as in the above description of the operation of Euclid’s algorithm) than a one-dimensional tape would be.* There is, however, no difficulty in principle about writing out the operation of a flow diagram in a ‘one-dimensional’ form (e.g. by the use of an ordinary verbal description of the chart). The two-dimensional planar display is only for our own convenience and ease of comprehension and it makes no difference to what can in principle be achieved. It is always possible to code the location of a mark or an object on a two-dimensional plane, or even in a three-dimensional space, in a straightforward way on a one-dimensional tape. (In fact, using a two-dimensional plane is completely equivalent to using two tapes. The two tapes would supply the two ‘coordinates’ that would be needed for specifying a point on a two-dimensional plane; likewise three tapes can act as ‘coordinates’ for a point in a three-dimensional space.) Again this one-dimensional coding may be ‘inefficient’, but it does not limit what can be achieved in principle.Despite all of this, we might still question whether the concept of a Turing machine really does incorporate every logical or mathematical operation that we would wish to call ‘mechanical’. At the time that Turing wrote his seminal paper, this was considerably less clear than it is today, so Turing found it necessary to put his case in appreciable detail. Turing’s closely argued case found additional support from the fact that, quite independently (and actually a little earlier), the American logician Alonzo Church (with the help of S. C. Kleene) had put forward a scheme -the lambda calculus – also aimed at resolving Hilbert’s Entscheidungsproblem. Though it was much less obviously a fully comprehensive mechanical scheme than was Turing’s, it had some advantages in the striking economy of its mathematical structure. (I shall be describing Church’s remarkable calculus at the end of this chapter.) Also independently of Turing there were yet other proposals for resolving Hilbert’s problem (see Gandy 1988), most particularly that of the Polish-American logician Emil Post (a little later than Turing, but with ideas considerably more like those of Turing than of Church). All these schemes were soon shown to be completely equivalent. This added a good deal of force to the viewpoint, which became known as the Church-Turing Thesis, that the Turing machine concept (or equivalent) actually does define what, mathematically, we mean by an algorithmic (or effective or recursive or mechanical) procedure. Now that highspeed electronic computers have become such a familiar part of our lives, not many people seem to feel the need to question this thesis in its original form. Instead, some attention has been turned to the matter of whether actual physical systems (presumably including human brains) – subject as they are to precise physical laws – are able to perform more than, less than, or precisely the same logical and mathematical operations as Turing machines. For my own part, I am very happy to accept the original mathematical
form of the Church-Turing Thesis. Its relation to the behaviour of actual physical systems, on the other hand, is a separate issue which will be a major concern for us later in this book.NUMBERS OTHER THAN NATURAL NUMBERSIn the discussion given above, we considered operations on natural numbers, and we noted the remarkable fact that single Turing machines can handle natural numbers of arbitrarily large size, despite the fact that each machine has a fixed finite number of distinct internal states. However, one often needs to work with more complicated kinds of number than this, such as negative numbers, fractions, or infinite decimals. Negative numbers and fractions (e.g. numbers like -597/26) can be easily handled by Turing machines, and the numerators and denominators can be as large as we like. All we need is some suitable coding for the signs ‘-’ and V, and this can easily be done using the expanded binary notation described earlier (for example, ‘3’ for ‘–’ and ‘4’ for ‘/’ – coded as 1110 and 11110, respectively, in the expanded binary notation). Negative numbers and fractions are thus handled in terms of finite sets of natural numbers, so with regard to general questions of computability they give us nothing new.Likewise, finite decimal expressions of unrestricted length give us nothing new, since these are just particular cases of fractions. For example, the finite decimal approximation to the irrational number π, given by 3.14159265, is simply the fraction 314159265/100000000. However, infinite decimal expressions, such as the full non-terminating expansionπ = 3.14159265358979 . . .present certain difficulties. Neither the input nor the output of a Turing machine can, strictly speaking, be an infinite decimal. One might think that we could find a Turing machine to churn out all the successive digits, 3, 1, 4, 1, 5, 9, . . ., of the above expansion for π one after the other on the output tape, where we simply allow the machine to run on forever. But this is not allowed for a Turing machine. We must wait for the machine to halt (indicated by the bell ringing!) before we are allowed to examine the output. So long as the machine has not reached a STOP order, the output is subject to possible change and so cannot be trusted. After it has reached STOP, on the other hand, the output is necessarily finite.There is, however, a procedure for legitimately making a Turing machine produce digits one after the other, in a way very similar to this. If we wish to generate an infinite decimal expansion, say that of π, we could have a Turing machine produce the whole-number part, 3, by making the machine act on 0, then we could produce the first decimal digit, 1, by making the machine act on 1, then the second decimal digit, 4, by making it act on 2, then the third, 1, by making it act on 3, and so on. In fact a Turing machine for producing the entire decimal expansion of π in this sense certainly does exist, though it would be a little complicated to work it out explicitly. A similar remark applies to many other irrational numbers, such as √2 = 1.414213562 . . . It turns out, however, that some irrationals (remarkably) cannot be produced by any Turing machine at all, as we shall see in the next chapter. The numbers that can be generated in this way are called computable (Turing 1937). Those that cannot (actually the vast majority!) are non-computable. I shall come back to this matter, and related issues, in later chapters. It will have some relevance for us in relation to the question of whether an actual physical object (e.g. a human brain) can, according to our physical theories, be adequately described in terms of computable mathematical structures.The issue of computability is an important one generally in mathematics. One should not think of it as a matter which applies just to numbers as such. One can have Turing machines which operate directly on mathematical formulae, such as algebraic or trigonometric expressions, for example, or which carry through the formal manipulations of the calculus. All that one needs is some form of precise coding into sequences of 0s and 1s, of all the mathematical symbols that are involved, and then the Turing machine concept can be applied. This, after all, was what Turing had in mind in his attack on the Entscheidungsproblem, which asks for an algorithmic procedure for answering mathematical questions of a general nature. We shall be coming back to this shortly.THE UNIVERSAL TURING MACHINEI have not yet described the concept of a universal Turing machine. The principle behind this is not too difficult to give, even though the details are complicated. The basic idea is to code the list of instructions for an arbitrary Turing maching T into a string of 0s and 1s that can be represented on a tape. This tape is then used as the initial part of the input for some particular Turing machine U – called a universal Turing machine – which then acts on the remainder of the input just as T would have done. The universal Turing machine is a universal mimic. The initial part of the tape gives the universal machine U the full information that it needs for it to imitate any given machine T exactly!To see how this works we first need a systematic way of numbering Turing machines. Consider the list of instructions defining some particular Turing machine, say one of those described above. We must code this list into a string of 0s and 1s according to some precise scheme. This can be done with the aid of the ‘contraction’ procedure that we adopted before. For if we represent the respective symbols R, L, STOP, the arrow (→), and the comma as, say, the numerals 2, 3, 4, 5, and 6, we can code them as contractions, by 110, 1110, 11110, 111110, and 1111110. Then the digits 0 and 1, coded as 0 and 10, respectively, can be used for the actual strings of these symbols appearing in the table. We do not need to have a different notation to distinguish the large figure 0 and 1 in the Turing machine table from the smaller boldface ones, since the position of the large digits at the end of the binary numbering is sufficient to distinguish them from the others. Thus, for example, 1101 would be read as the binary number 1101 and coded on the tape as 1010010. In particular, 00 would be read as 00, which can, without ambiguity, be coded 0, or as a symbol omitted altogether. We can economize considerably by not actually bothering to code any arrow nor any of the symbols immediately preceding it, relying instead upon the numerical ordering of instructions to specify what those symbols must be – although to adopt this procedure we must make sure that there are no gaps in this ordering, supplying a few extra ‘dummy’ orders where required. (For example, the Turing machine XN + 1 has no order telling us what to do with 1100, since this combination never occurs in the running of the machine, so we must insert a ‘dummy’ order, say 1100 → 00R, which can be incorporated into the list without changing anything. Similarly we should insert 101 → 00R into the machine XN × 2.) Without such ‘dummies’, the coding of the subsequent orders in the list would be spoiled. We do not actually need the comma at the end of each instruction, as it turns out, since the symbols L or R suffice to separate the instructions from one another. We therefore simply adopt the following coding:As an example, let us code the Turing machine XN + 1 (with the 1100 → 00R instruction inserted). Leaving out the arrows, the digits immediately preceding them, and also the commas, we haveWe can improve on this by leaving out every 00 and replacing each 01, by simply 1, in accordance with what has been said earlier, to getThis is coded as the tape sequenceAs two further minor economies, we may as well always delete the initial 110 (together with the infinite stretch of blank tape that precedes it) since this denotes 0OR, which represents the initial instruction 00 → 00R that I have been implicitly taking to be common to all Turing machines – so that the device can start arbitrarily far to the left of the marks on the tape and run to the right until it ccmes up to the first mark – and we may as well always delete the final 110 (and the implicit infinite sequence of 0s which is assumed to follow it) since all Turing machines must have their descriptions ending this way (because they all end with R, L, or STOP). The resulting binary number is the number of the Turing machine, which in the case of XN + 1 is:10101101101001011010100111010010110101111010000111010010101110100010111010100011010010110110101010101101010101101010100.In standard denary notation, this particular number is450813704461563958982113775643437908.We sometimes loosely refer to the Turing machine whose number is n as the nth Turing machine, denoted Tn. Thus XN + 1 is the 450813704461563958982113775643437908th Turing machine!It is a striking fact that we appear to have to go this far along the ‘list’ of Turing machines before we find one that even performs so trivial an operation as adding one (in the expanded binary notation) to a natural number! (I do not think that I have been grossly inefficient in my coding, though I can see room for some minor improvements.) Actually there are some Turing machines with smaller numbers which are of interest. For example, UN + 1 has the binary number101011010111101010which is merely 177642 in denary notation! Thus the particularly trivial Turing machine UN + 1, which merely places an additional 1 at the end of a sequence of Is, is the 177642nd Turing machine. For curiosity’s sake, we may note that ‘multiplying by two’ comes somewhere between these two in the list of Turing machines, in either notation, for we find that the number for XN × 2 is 10389728107 while that of UN × 2 is 1492923420919872026917547669.It is perhaps not surprising to learn, in view of the sizes of these numbers, that the vast majority of natural numbers do not give working Turing machines at all. Let us list the first thirteen Turing machines according to this numbering:Of these, T0 simply moves on to the right obliterating everything that it encounters, never stopping and never turning back. The machine T1 ultimately achieves the same effect, but in a clumsier way, jerking backwards after it obliterates each mark on the tape. Like T0, the machine T2 also moves on endlessly to the right, but is more respectful, simply leaving everything on the tape just as it was before. None of these is any good as a Turing machine since none of them ever stops. T3 is the first respectable machine. It indeed stops, modestly, after changing the first (leftmost) 1 into a 0.T4 encounters a serious problem. After it finds its first 1 on the tape it enters an internal state for which there is no listing, so it has no instructions as to what to do next. T8, T9, and T10 encounter the same problem. The difficulty with T7 is even more basic. The string of 0s and 1s which codes it involves a sequence of five successive 1s: 110111110. There is no interpretation for such a sequence, so T7 will get stuck as soon as it finds its first 1 on the tape. (I shall refer to T7, or any other machine Tn for which the binary expansion of n contains a sequence of more than four 1s as being not correctly specified.) The machines T5, T6, and T12 encounter problems similar to those of T0, T1 and T2. They simply run on indefinitely without ever stopping. All of the machines T0, T1, T2, T4, T5, T6, T7, T8, T9, T10, and T12 are duds! Only T3 and T11 are working Turing machines, and not very interesting ones at that. T11 is even more modest than T3. It stops at its first encounter with a 1 and it doesn’t change a thing!We should note that there is also a redundancy in our list. The machine T12 is identical with T6, and also identical in action with T0, since the internal state 1 of T6 and T12 is never entered. We need not be disturbed by this redundancy, nor by the proliferation of dud Turing machines in the list. It would indeed be possible to improve our coding so that a good many of the duds are removed and the redundancy considerably reduced. All this would be at the expense of complicating our poor universal Turing machine which has to decipher the code and pretend to be the Turing machine Tn whose number n it is reading. This might be worth doing if we could remove all the duds (or the redundancy). But this is not possible, as we shall see shortly! So let us leave our coding as it is.It will be convenient to interpret a tape with its succession of marks, e.g.. . . 0001101110010000 . . .as the binary representation of some number. Recall that the 0s continue indefinitely at both ends, but that there is only a finite number of 1s. I am also assuming that the number of 1s is non-zero (i.e. there is at least one 1). We could choose to read the finite string of symbols between the first and last 1 (inclusive), which in the above case is110111001,as the binary description of a natural number (here 441, in denary notation). However, this procedure would only give us odd numbers (numbers whose binary representation ends in a 1) and we want to be able to represent all natural numbers. Thus we adopt the simple expedient of removing the final 1 (which is taken to be just a marker indicating the termination of the expression) and reading what is left as a binary number.5 Thus, for the above example, we have the binary number11011100,which, in denary notation, is 220. This procedure has the advantage that zero is also represented as a marked tape, namely. . .0000001000000. . .Let us consider the action of the Turing machine Tn on some (finite) string of 0s and 1s on a tape which we feed in on the right. It will be convenient to regard this string also as the binary representation of some number, say m, according to the scheme given above. Let us assume that after a succession of steps the machine Tn finally comes to a halt (i.e. reaches STOP). The string of binary digits that the machine has now produced at the left is the answer to the calculation. Let us also read this as the binary representation of a number in the same way, say p. We shall write this relation, which expresses the fact that when the nth Turing machine acts on m it produces p, as:Now let us look at this relation in a slightly different way. We think of it as expressing one particular operation which is applied to the pair of numbers n and m in order to produce the number p. (Thus: given the two numbers n and m, we can work out from them what p is by seeing what the nth Turing machine does to m.) This particular operation is an entirely algorithmic procedure. It can therefore be carried out by one particular Turing machine U; that is, U acts on the pair (n, m) to produce p. Since the machine U has to act on both of n and m to produce the single result p, we need some way of coding the pair (n, m) on the one tape. For this, we can assume that n is written out in ordinary binary notation and then immediately terminated by the sequence 111110. (Recall that the binary number of every correctly specified Turing machine is a sequence made up just of 0s, 10s, 110s, 1110s, and 11110s, and it therefore contains no sequence of more than four Is. Thus if Tn is a correctly specified machine, the occurrence of 111110 indeed signifies that the description of the number n is finished with.) Everything following it is to be simply the tape represented by m according to our above prescription (i.e. the binary number m immediately followed by 1000 . . .). Thus this second part is simply the tape that Tn is supposed to act on.As an example, if we take n = 11 and m = 6, we have, for the tape on which U has to act, the sequence of marks. . . 000101111111011010000 . . .This is made up as follows:What the Turing machine U would have to do, at each successive step of the operation of Tn on m, would be to examine the structure of the succession of digits in the expression for n so that the appropriate replacement in the digits for m (i.e. Tn’s ‘tape’) can be made. In fact it is not difficult in principle (though decidedly tedious in practice) to see how one might actually construct such a machine. Its own list of instructions would simply be providing a means of reading the appropriate entry in that ‘list’ which is encoded in the number n, at each stage of application to the digits of the ‘tape’, as given by m. There would admittedly be a lot of dodging backwards and forwards between the digits of m and those of n, and the procedure would tend to be exceedingly slow. Nevertheless, a list of instructions for such a machine can certainly be provided; and we call such a machine a universal Turing machine. Denoting the action of this machine on the pair of numbers n and m by U(n, m), we have:for each (n, m) for which Tn is a correctly specified Turing machine.6 The machine U, when first fed with the number n, precisely imitates the nth Turing machine!Since U is a Turing machine, it will itself have a number; i.e. we haveU = Tu,for some number u. How big is u? In fact we can take precisely(or some other possibility of at least that kind of size). This number no doubt seems alarmingly large! Indeed it is alarmingly large, but I have not been able to see how it could have been made significantly smaller. The coding procedures and specifications that I have given for Turing machines are quite reasonable and simple ones, yet one is inevitably led to a number of this kind of size for the coding of an actual universal Turing machine.7I have said that all modern general purpose computers are, in effect, universal Turing machines. I do not mean to imply that the logical design of such computers need resemble at all closely the kind of description for a universal Turing machine that I have just given. The point is simply that, by supplying any universal Turing machine first with an appropriate program (initial part of the input tape), it can be made to mimic the behaviour of any Turing machine whatever! In the description above, the program simply takes the form of a single number (the number n), but other procedures are possible, there being many variations on Turing’s original theme. In fact in my own descriptions I have deviated somewhat from those that Turing originally gave. None of these differences is important for our present needs.THE INSOLUBILITY OF HILBERT’S PROBLEMWe now come to the purpose for which Turing originally put forward his ideas, the resolution of Hilbert’s broad-ranging Entscheidungsproblem: is there some mechanical procedure for answering all mathematical problems, belonging to some broad, but well-defined class? Turing found that he could phrase his version of the question in terms of the problem of deciding whether or not the nth Turing machine would actually ever stop when acting on the number m. This problem was referred to as the halting problem. It is an easy matter to construct an instruction list for which the machine will not stop for any number m (for example, n = 1 or 2, as given above, or any other case where there are no STOP instructions whatever). Also there are many instruction lists for which the machine would always stop, whatever number it is given (e.g. n = 11); and some machines would stop for some numbers but not for others. One could fairly say that a putative algorithm is not much use when it runs forever without stopping. That is no algorithm at all. So an important question is to be able to decide whether or not Tn applied to m actually ever gives any answer! If it does not (i.e. if the calculation does not stop), then I shall writeTn(m) = .(Included in this notation would be those situations where the Turing machine runs into a problem at some stage because it finds no appropriate instruction to tell it what to do – as with the dud machines such as T4 and T7 considered above. Also, unfortunately, our seemingly successful machine T3 must now also be considered a dud: T3(m)= , because the result of the action of T3 is always just blank tape, whereas we need at least one 1 in the output in order that the result of the calculation be assigned a number! The machine T11 is, however, legitimate since it produces a single 1. This output is the tape numbered 0, so we have T11(m) = 0 for all m.)It would be an important issue in mathematics to be able to decide when Turing machines stop. For example, consider the equation:(x + 1)w + 3 + (y + 1)w + 3 = (z + 1)w + 3.(If technical mathematical equations are things that worry you, don’t be put off! This equation is being used only as an example, and there is no need to understand it in detail.) This particular equation relates to a famous unsolved problem in mathematics -perhaps the most famous of all. The problem is this: is there any set of natural numbers w, x, y, z for which this equation is satisfied? The famous statement known as ‘Fermat’s last theorem’, made in the margin of Diophantus’s Arithmetica, by the great seventeenth century French mathematician Pierre de Fermat (1601–1665), is the assertion that the equation is never satisfied.*8 Though a lawyer by profession (and a contemporary of Descartes), Fermat was the finest mathematician of his time. He claimed to have ‘a truly wonderful proof of his assertion, which the margin was too small to contain; but to this day no-one has been able to reconstruct such a proof nor, on the other hand, to find any counter-example to Fermat’s assertion!It is clear that given the quadruple of numbers (w, x, y, z), it is a mere matter of computation to decide whether or not the equation holds. Thus we could imagine a computer algorithm which runs through all the quadruples of numbers one after the other, and stops only when the equation is satisfied. (We have seen that there are ways of coding finite sets of numbers, in a computable way, on a single tape, i.e. simply as single numbers, so we can ‘run through’ all the quadruples by just following the natural ordering of these single numbers.) If we could establish that this algorithm does not stop, then we would have a proof of the Fermat assertion.In a similar way it is possible to phrase many other unsolved mathematical problems in terms of the Turing machine halting problem. Such an example is the ‘Goldbach conjecture’, which asserts that every even number greater than 2 is the sum of two prime numbers.* It is an algorithmic process to decide whether or not a given natural number is prime since one needs only to test its divisibility by numbers less than itself, a matter of only finite calculation. We could devise a Turing machine which runs through the even numbers 6, 8, 10, 12, 14,. . . trying all the different ways of splitting them into pairs of odd numbersand testing to make sure that, for each such even number, it splits to some pair for which both members are prime. (Clearly we need not test pairs of even summands, except 2 + 2, since all primes except 2 are odd.) Our machine is to stop only when it reaches an even number for which none of the pairs into which that number splits consists of two primes. In that case we should have a counter-example to the Goldbach conjecture, namely an even number (greater than 2) which is not the sum of two primes. Thus if we could decide whether or not this Turing machine ever stops, we should have a way of deciding the truth of the Goldbach conjecture also.A natural question arises: how are we to decide whether or not any particular Turing machine (when fed with some specific input) will ever stop? For many Turing machines this might not be hard to answer; but occasionally, as we have seen above, the answer could involve the solution of an outstanding mathematical problem. So, is there some algorithmic procedure for answering the general question – the halting problem – completely automatically? Turing showed that indeed there is not.His argument was essentially the following. We first suppose that, on the contrary, there is such an algorithm.* Then there must be some Turing machine H which ‘decides’ whether or not the nth Turing machine, when acting on the number m, eventually stops. Let us say that it outputs the tape numbered O if it does not stop and 1 if it does:Here, one might take the coding of the pair (n, m) to follow the same rule as we adopted for the universal machine U. However this could run into the technical problem that for some number n (e.g. n = 7), Tn is not correctly specified; and the marker 111101 would be inadequate to separate n from m on the tape. To obviate this problem, let us assume that n is coded using the expanded binary notation rather than just the binary notation, with m in ordinary binary, as before. Then the marker 110 will actually be sufficient to separate n from m. The use of the semicolon in H(n; m), as distinct from the comma in U(n, m) is to indicate this change.Now let us imagine an infinite array, which lists all the outputs of all possible Turing machines acting on all the possible different inputs. The nth row of the array displays the output of the nth Turing machine, as applied to the various inputs 0, 1, 2, 3, 4, . . . :In the above table I have cheated a little, and not listed the Turing machines as they are actually numbered. To have done so would have yielded a list that looks much too boring to begin with, since all the machines for which n is less than 11 yield nothing but s, and for n = 11 itself we get nothing but 0s. In order to make the list look initially more interesting, I have assumed that some much more efficient coding has been achieved. In fact I have simply made up the entries fairly randomly, just to give some kind of impression as to what its general appearance could be like.I am not asking that we have actually calculated this array, say by some algorithm. (In fact, there is no such algorithm, as we shall see in a moment.) We are just supposed to imagine that the true list has somehow been laid out before us, perhaps by God! It is the occurrence of the s which would cause the difficulties if we were to attempt to calculate the array, for we might not know for sure when to place a  in some position since those calculations simply run on forever!However, we could provide a calculational procedure for generating the table if we were allowed to use our putative H, for H would tell us where the  s actually occur. But instead, let us use H to eliminate every  by replacing each occurrence with 0. This is achieved by preceding the action of Tn on m by the calculation H(n; m); then we allow Tn to act on m only if H(n; m) = 1 (i.e. only if the calculation T(m) actually gives an answer), and simply write 0 if H(n; m) = 0 (i.e. if Tn = ). We can write our new procedure (i.e. that obtained by preceding Tn(m) by the action of H(n; m)) asTn(m) × H(n; m).(Here I am using a common mathematical convention about the ordering of mathematical operations: the one on the right is to be performed first. Note that, symbolically, we have  × 0 = 0.)The table for this now reads:Note that, assuming H exists, the rows of this table consist of computable sequences. (By a computable sequence I mean an infinite sequence whose successive values can be generated by an algorithm; i.e. there is some Turing machine which, when applied to the natural numbers m = 0, 1, 2, 3, 4, 5,. . . in turn, yields the successive members of the sequence.) Now, we take note of two facts about this table. In the first place, every computable sequence of natural numbers must appear somewhere (perhaps many times over) amongst its rows. This property was already true of the original table with its  s. We have simply added some rows to replace the ‘dud’ Turing machines (i.e. the ones which produce at least one ). In the second place, the assumption having been made that the Turing machine H actually exists, the table has been computably generated (i.e. generated by some definite algorithm), namely by the procedure Tn(m) × H(n; m). That is to say, there is some Turing machine Q which, when acting on the pair of numbers (n, m) produces the appropriate entry in the table. For this, we may code n and m on Q’s tape in the same way as for H, and we haveQ(n;m) = Tn(m) × H(n; m).We now apply a variant of an ingenious and powerful device, the ‘diagonal slash’ of Georg Cantor. (We shall be seeing the original version of Cantor’s diagonal slash in the next chapter.) Consider the elements of the main diagonal, marked now with bold figures:The elements provide some sequence 0, 0, 1, 2, 1, 0, 3, 7, 1, . . . to each of whose terms we now add 1:1, 1, 2, 3, 2, 1, 4, 8, 2, . . .This is clearly a computable procedure and, given that our table was computably generated, it provides us with some new computable sequence, in fact with the sequence 1 + Q(n; n), i.e.1 + Tn(n) × H(n; n)(since the diagonal is given by making m equal to n). But our table contains every computable sequence, so our new sequence must be somewhere in the list. Yet this cannot be so! For our new sequence differs from the first row in the first entry, from the second row in the second entry, from the third row in the third entry, and so on. This is manifestly a contradiction. It is the contradiction which establishes what we have been trying to prove, namely that the Turing machine H does not in fact exist! There is no universal algorithm for deciding whether or not a Turing machine is going to stop.Another way of phrasing this argument is to note that, on the assumption that H exists, there is some Turing machine number, say k, for the algorithm (diagonal process!) 1 + Q(n; n), so we have1 + Tn(n) × H(n;n) = Tk(n).But if we substitute n = k in this relation we get1 + Tk(k) × H(k; k) = Tk(k).This is a contradiction because if Tk(k) stops we get the impossible relation1 + Tk(k) = Tk(k)(since H(k; k) = 1), whereas if Tk(k) does not stop (so H(k; k) = 0) we have the equally inconsistent1 + 0 = .The question of whether or not a particular Turing machine stops is a perfectly well-defined piece of mathematics (and we have already seen that, conversely, various significant mathematical questions can be phrased as the stopping of Turing machines). Thus, by showing that no algorithm exists for deciding the question of the stopping of Turing machines, Turing showed (as had Church, using his own rather different type of approach) that there can be no general algorithm for deciding mathematical questions. Hilbert’s Entscheidungsproblem has no solution!This is not to say that in any individual case we may not be able to decide the truth, or otherwise, of some particular mathematical question; or decide whether or not some given Turing machine will stop. By the exercise of ingenuity, or even of just common sense, we may be able to decide such a question in a given case. (For example, if a Turing machine’s instruction list contains no STOP order, or contains only STOP orders, then common sense alone is sufficient to tell us whether or not it will stop!) But there is no one algorithm that works for all mathematical questions, nor for all Turing machines and all numbers on which they might act.It might seem that we have now established that there are at least some undecidable mathematical questions. However, we have done nothing of the kind! We have not shown that there is some especially awkward Turing machine table for which, in some absolute sense, it is impossible to decide whether or not the machine stops when it is fed with some especially awkward number – indeed, quite the reverse, as we shall see in a moment. We have said nothing whatever about the insolubility of single problems, but only about the algorithmic insolubility of families of problems. In any single case the answer is either ‘yes’ or ‘no’, so there certainly is an algorithm for deciding that particular case, namely the algorithm that simply says ‘yes’, when presented with the problem, or the one that simply says ‘no’, as the case may be! The difficulty is, of course, that we may not know which of these algorithms to use. That is a question of deciding the mathematical truth of a single statement, not the systematic decision problem for a family of statements. It is important to realize that algorithms do not, in themselves, decide mathematical truth. The validity of an algorithm must always be established by external means.HOW TO OUTDO AN ALGORITHMThis question of deciding the truth of mathematical statements will be returned to later, in connection with Gödel’s theorem (see Chapter 4). For the moment, I wish to point out that Turing’s argument is actually a lot more constructive and less negative than I have seemed to imply so far. We have certainly not exhibited a specific Turing machine for which, in some absolute sense, it is undecidable whether or not it stops. Indeed, if we examine the argument carefully, we find that our very procedure has actually implicitly told us the answer for the seemingly ‘especially awkward’ machines that we construct using Turing’s procedure!Let us see how this comes about. Suppose we have some algorithm which is sometimes effective for telling us when a Turing machine will not stop. Turing’s procedure, as outlined above, will explicitly exhibit a Turing machine calculation for which that particular algorithm is not able to decide whether or not the calculation stops. However, in doing so, it actually enables us to see the answer in this case! The particular Turing machine calculation that we exhibit will indeed not stop.To see how this arises in detail suppose we have such an algorithm that is sometimes effective. As before, we denote this algorithm (Turing machine) by H, but now we allow that the algorithm may not always be sure to tell us that a Turing machine will actually not stop:so H(n; m) =  is a possibility when Tn(m) = . Many such algorithms H(n; m) actually exist. (For example, H(n; m) could simply produce a 1 as soon as Tn(m) stops, although that particular algorithm would hardly be of much practical use!)We can follow through Turing’s procedure in detail just as given above, except that instead of replacing all the  s by Os, we now have some s left. As before, our diagonal procedure has provided us with1 + Tn(n) × H (n; n),as the nth term on the diagonal. (We shall get a  whenever H(n; n) = . Note that  ×  = , 1 +  = .) This is a perfectly good computation, so it is achieved by some Turing machine, say the kth one, and now we do have1 + Tn(n) × H(n; n) = Tk(n).We look at the kth diagonal term, i.e. n = k, and obtain1 + Tk(k) × H(k; k) = Tk(k).If the computation Tk(k) stops, we have a contradiction (since H(k; k) is supposed to be 1 whenever Tk(k) stops, and the equation then gives inconsistency: 1 + Tk(k) = Tk(k)). Thus Tk (k) cannot stop, i.e.Tk(k) = .But the algorithm cannot ‘know’ this, because if it gave H(k; k) = 0, we should again have contradiction (symbolically, we should have the invalid relation: 1 + 0 = ).Thus, if we can find k we shall know how to construct our specific calculation to defeat the algorithm but for which we know the answer! How do we find k? That’s hard work. What we have to do is to look in detail at the construction of H(n; m) and of Tn(m) and then see in detail how 1 + Tn(n) × H(n; n) acts as a Turing machine. We find the number of this Turing machine, which is k. This would certainly be complicated to carry out in detail, but it could be done.* Because of the complication, we would not be at all interested in the calculation Tk(k) were it not for the fact that we have specially produced it in order to defeat the algorithm H! What is important is that we have a well-defined procedure, whichever H is given to us, for finding a corresponding k for which we know that Tk (k) defeats H, and for which we can therefore do better than the algorithm. Perhaps that comforts us a little if we think we are better than mere algorithms!In fact the procedure is so well defined that we could find an algorithm for generating k, given H. So, before we get too complacent, we have to realize that this algorithm can improve9 on H since, in effect, it ‘knows’ that Tk(k) =  – or does it? It has been helpful in the above description to use the anthropomorphic term ‘know’ in reference to an algorithm. However, is it not we
who are doing the ‘knowing’, while the algorithm just follows the rules we have told it to follow? Or are we ourselves merely following rules that we have been programmed to follow from the construction of our brains and from our environment? The issue is not really simply one of algorithms, but also a question of how one judges what is true and what is not true. These are central issues that we shall have to return to later. The question of mathematical truth (and its non-algorithmic nature) will be considered in Chapter 4. At least we should now have some feeling about the meanings of the terms ‘algorithm’ and ‘computability’, and an understanding of some of the related issues.CHURCH’S LAMBDA CALCULUSThe concept of computability is a very important and beautiful mathematical idea. It is also a remarkably recent one – as things of such a fundamental nature go in mathematics – having been first put forward in the 1930s. It is an idea which cuts across all areas of mathematics (although it may well be true that most mathematicians do not, as yet, often worry themselves about computability questions). The power of the idea lies partly in the fact that some well-defined operations in mathematics are actually not computable (like the stopping, or otherwise, of a Turing machine; we shall see other examples in Chapter 4). For if there were no such non-computable things, the concept of computability would not have much mathematical interest. Mathematicians, after all, like puzzles. It can be an intriguing puzzle for them to decide, of some mathematical operation, whether or not it is computable. It is especially intriguing because the general solution of that puzzle is itself non-computable!One thing should be made clear. Computability is a genuine ‘absolute’ mathematical concept. It is an abstract idea which lies quite beyond any particular realization in terms of the ‘Turing machines’ as I have described them. As I have remarked before, we do not need to attach any particular significance to the ‘tapes’ and ‘internal states’, etc., which characterize Turing’s ingenious but particular approach. There are also other ways of expressing the idea of computability, historically the first of these being the remarkable ‘lambda calculus’ of the American logician Alonzo Church, with the assistance of Stephen C. Kleene. Church’s procedure was quite different, and distinctly more abstract from that of Turing. In fact, in the form that Church stated his ideas, there is rather little obvious connection between them and anything that one might call ‘mechanical’. The key idea lying behind Church’s procedure is, indeed, abstract in its very essence – a mathematical operation that Church actually referred to as ‘abstraction’.I feel that it is worth while to give a brief description of Church’s scheme not only because it emphasizes that computability is a mathematical idea, independent of any particular concept of computing machine, but also because it illustrates the power of abstract ideas in mathematics. The reader who is not readily conversant with mathematical ideas, nor intrigued by such things for their own sake, may, at this stage, prefer to move on to the next chapter – and there would not be significant loss in the flow of argument. Nevertheless, I believe that such readers might benefit by bearing with me for a while longer, and thus witnessing some of the magical economy of Church’s scheme (see Church 1941).In this scheme one is concerned with a ‘universe’ of objects, denoted by saya, b, c, d, . . ., z, a′ b′, . . .,z′;, a′′, bʹ ʹ, . . .,a′′′, . . ., a′′′,. . .each of which stands for a mathematical operation or function. (The reason for the primed letters is simply to allow an unlimited supply of symbols to denote such functions.) The ‘arguments’ of these functions – that is to say, the things on which these functions act – are other things of the same kind, i.e. also functions. Moreover, the result (or ‘value’) of one such function acting on another is to be again a function. (There is, indeed, a wonderful economy of concepts in Church’s system.) Thus, when we write*a = bcwe mean that the result of the function b acting on the function c is another function a. There is no difficulty about expressing the idea of a function of two or more variables in this scheme. If we wish to think of f as a function of two variables p and q, say, we may simply write(fp)q(which is the result of the function fp as applied to q). For a function of three variables we consider((fp)q) r,and so on.Now comes the powerful operation of abstraction. For this we use the Greek letter λ (lambda) and follow it immediately by a letter standing for one of Church’s functions, say x, which we consider as a ‘dummy variable’. Every occurrence of the variable x in the square-bracketed expression which immediately follows is then considered merely as a ‘slot’ into which may be substituted anything that follows the whole expression. Thus if we writeλx. [fx],we mean the function which when acting on, say, a produces the result fa. That is to say.λx. [fx]a = fa.In other words, λx. [fx] is simply the function f, i.e.λx. [fx] = f.This bears a little thinking about. It is one of those mathematical niceties that seems so pedantic and trivial at first that one is liable to miss the point completely. Let us consider an example taken from familiar school mathematics. We take the function f to be the trigonometrical operation of taking the sine of an angle, so the abstract function ‘sin’ is defined byλx. [sinx] = sin.(Do not worry about how the ‘function’ x may be taken to be an angle. We shall shortly see something of the way that numbers may be regarded as functions; and an angle is just a kind of number.) So far, this is indeed rather trivial. But let us imagine that the notation ‘sin’ had not been invented, but that we are aware of the power series expression for sin x:Then we could defineNote that, even more simply, we could define, say, the ‘one-sixth cubing’ operation for which there is no standard ‘functional’ notation:and find, for example,More pertinent to the present discussion would be expressions made up simply from Church’s elementary functional operations, such asλf.[f(fx)].This is the function which, when acting on another function, say g, produces g iterated twice acting on x, i.e.(λf. [f(fx)]g = g(gx).We could also have ‘abstracted away’ the x first, to obtainλf. [λx. [f(fx)],which we may abbreviate toλfx. [f(fx)].This is the operation which, when acting on g, produces the function ‘g iterated twice’. In fact this is the very function that Church identifies with the natural number 2:so (g)y = g(gy). Similarly he defines:together withReally, Church’s ‘’ is more like ‘twice’ and his ‘’ is ‘thrice’, etc. Thus, the action of  on a function f, namely 
f, is the operation ‘iterate f three times’. The action of 
f On y, therefore, would be (f)y = f(f(y))).Let us see how a very simple arithmetical operation, namely the operation of adding  to a number, can be expressed in Church’s scheme. DefineS = λ abc.[b ((ab) c)].To illustrate that S indeed simply adds  to a number described in Church’s notation, let us test it on :since (b)c = b(b(bc)). Clearly this applies equally well to any other natural number. (In fact λ abc.[(ab)(bc)] would also have done just as well as S.)How about multiplying a number by two? This doubling can be achieved byD = λ abc.[(ab)((ab)c)],which is again illustrated by its action on :In fact, the basic arithmetical operations of addition, multiplication, and raising to a power can be defined, respectively, by:A = λfgxy.[((fx)(gx))y],M = λfgx.[f(gx)],P = λfg.[fg].The reader may care to convince herself or himself – or else to take on trust – that, indeed,where  and  are Church’s functions for two natural numbers,  +  is his function for their sum, and so on. The last of these is the most astonishing. Let us just check it for the case  = ,  = 3:The operations of subtraction and division are not so easily defined (and, indeed, we need some convention about what to do with ‘ – ’ when  is smaller than  and with ‘ ÷ ’ when  is not divisible by ). In fact a major landmark of the subject occurred in the early 1930s when Kleene discovered how to express the operation of subtraction within Church’s scheme! Other operations then followed. Finally, in 1937, Church and Turing independently showed that every computable (or algorithmic) operation whatever – now in the sense of Turing’s machines – can be achieved in terms of one of Church’s expressions (and vice versa).This is a truly remarkable fact, and it serves to emphasize the fundamentally objective and mathematical character of the notion of computability. Church’s notion of computability has, at first sight, very little to do with computing machines. Yet it has, nevertheless, some fundamental relations to practical computing. In particular, the powerful and flexible computer language LISP incorporates, in an essential way, the basic structure of Church’s calculus.As I indicated earlier, there are also other ways of defining the notion of computability. Post’s concept of computing machine was very close to Turing’s, and was produced independently, at almost the same time. There was currently also a rather more usable definition of computability (recursiveness) due to J. Herbrand and Gödel. H. B. Curry in 1929, and also M. Schöfinkel in 1924, had a different approach somewhat earlier, from which Church’s calculus was partly developed. (See Gandy 1988.) Modern approaches to computability (such as that of an unlimited register machine, described in Cutland 1980) differ considerably in detail from Turing’s original one, and they are rather more practical. Yet the concept of computability remains the same, whichever of these various approaches is adopted.Like so many other mathematical ideas, especially the more profoundly beautiful and fundamental ones, the idea of computability seems to have a kind of Platonic reality of its own. It is this mysterious question of the Platonic reality of mathematical concepts generally that we must turn to in the next two chapters.NOTES1. I am adopting the usual modern terminology which now includes zero among the ‘natural numbers’.2. There are many other ways of coding pairs, triples, etc., of numbers as single numbers, well known to mathematicians, though less convenient for our present purposes. For example, the formula  ((a + b)2 + 3a + b) represents the pairs (a, b) of natural numbers uniquely as a single natural number. Try it!3. I have not bothered, in the above, to introduce some mark to initiate the sequence of numbers (or instructions, etc.). This is not necessary for the input, since things just start when the first 1 is encountered. However, for the output something else may be needed, since one may not know a priori how far to look along the output tape in order to reach the first (i.e. leftmost) 1. Even though a long string of Os may have been encountered going off to the left, this would be no guarantee that there would not be a 1 still farther off on the left. One can adopt various viewpoints on this. One of these would be always to use a special mark (say, coded by 6 in the contraction procedure) to initiate the entire output. But for simplicity, in my descriptions I shall take a different point of view, namely that it is always ‘known’ how much of the tape has actually been encountered by the device (e.g. one can imagine that it leaves a ‘trail’ of some kind), so that one does not, in principle, have to examine an infinite amount of tape in order to be sure that the entire output has been surveyed.4. One way of coding the information of two tapes on a single tape is to interleave the two. Thus, the odd-numbered marks on the single tape could represent the marks of the first tape, whilst the even-numbered ones could represent the marks of the second tape. A similar scheme works for three or more tapes. The ‘inefficiency’ of this procedure results from the fact that the reading device would have to keep dodging backwards and forwards along the tape and leaving markers on it to keep track of where it is, at both even and odd parts of the tape.5. This procedure refers only to the way in which a marked tape can be interpreted as a natural number. It does not alter the numbers of our specific Turing machines, such as EUC or XN + 1.6. If Tn is not correctly specified, then U will proceed as though the number for n has terminated as soon as the first string of more than four Is in n’s binary expression is reached. It will read the rest of this expression as part of the tape for m, so it will proceed to perform some nonsensical calculation! This feature could be eliminated, if desired, by arranging that n be expressed in expanded binary notation. I have chosen not to do this so as not to complicate further the description of the poor universal machine U!7. I am grateful to David Deutsch for deriving the denary form of the binary description for u which I had worked out below. I am grateful to him also for checking that this binary value of u actually does give a universal Turing machine! The binary value for u is in fact:100000000101110100110100010010101011010001101000101000001101010011010001010100101101000011010001010010101101001001110100101001001011101010001110101010010010101110101010011010001010001010110100000110100100000101011010001001110100101000010101110100100011101001010100001011101001010011010000100001110101 0000111010100001001001110100010101011010100101011010000011010101001011010010010001101000000001101000000111010100101010101110100001001110100101010101010101110100001010101110100001010001011101000101001101001000010100110100101001001101001000101101010001011101001001010111010010100011101010010100100111010101010000110100101010101110101001000101101010000101101010001001101010101010001011010010101001001011010100100101110101010010101110101001010011010101000011101000100100101011101010100101011101010100000111010100100000110101010100101110101001010110100010010001110100000001110100101001010101011101001010010010101110100000101011101000010001110100000101010011101000010100111010000010001011101000100001110100001001010011101000100001011010001010010111010001010010110100100000101101000101010010011010001010101011101001000001110100100101010101110101010100110100100010101101001001001011010000000101101000001000110100000100101101000000000110100101000101110100101010001101001010010101101000001001110100101010010110100100111010100000010101110101000000110101010001010101101001010101101010000101011101010010010101110101000100101101010010000101110100000011101010010001011010100101001101010100010111010100101001011101010100000101110101010000010111010000001110101010000101011101001010101101010100001011101010001010101110101010010010111010101010000111010100000001110100100100001101001001000101101010101010011101000000001011010010000110101010101001011101001000011010010001010101110100001000111010001000011101000011010000000101101000001001011101010100101010110100010001001011101000001001110101010011010000010101011010000100001110100100001000111010101010101001110100001001001110100010010000111010000101001011010000101000011101010101010101110100010010011010001001001101010010100101110100010001010111010000000111010001001001011101001101001001000010110101010100110100010100010111010000110101000010001011010100110101010010100101101010100110100100101011101001101001000001011010001010101000111010010000101011010000001001101001000100101110100100001101010000010010111010010010100110 1001001010101101001101001001010010110100110100101000001011010010000011101010010011010101010000101110100101000010111010010101010111010100010010110100100111010010101000101110100010011101010000101101001001110100101010101011101001000111010010101010010111010010001110101000001010101110011010100000101101001001110101000000101110100101101010000010101101001010010111010100001001011101000011010100010000101101010011010100010001011010101010010111010100010100101101000101010101110100100001010110101000101110101001001010101110101010010010111010100011101010001110101001001001011101010001110101001010001011101010001011101010000100101110101000111010001010001011101001010010111010100101010010111010010101010101011010100001010101011010000100111010000101010101011101010100010101110101010001010111010000001110101010001001011101000000111010101001000101110101000000110101000010110100000011101001000000101110101000111010100100010101110101001101010101000101011010000011010101010010101011010000001001101010101001001110101001101010101001001011010100110100100100111010000011010101010100101011010100010011010001010010101011101000001101010101010100101101000100011101000101010101010110100010001110100001010111010001001000011101001101000000010011101000000100101110100010001010011101000000100101110100101010101001011010000101010101110100010010100101110100000100010111010101001011010001000100111010000010010101110100000010101011010000100011100111101000010000011101000010010011101000001010010111010000010100101101000010001010111010000100010011010001000011101011110100001001001011101000010010010111010000000101011101000010101000110100010010111010000100000111010000100111010001000001011101010100101101000100000101110100001010101011101000000101010111010001000010101110100010000101011101001000001110101001001001101000000101011101000100010010111010101000011101010010101101001010101000011010000010100110100000001110100000100100111010010110100100010100101101010100110100010100100101101010100110100010101000101100110101001001011101010100110100010101010101100110101000101010110011010010001010 101011101000100011101001001010101010110100101001010001101001000000101110100000110101010010101010110100101010110100100010001011101000101010110101000001010110100010000011010010001010110100001001110101001010101010111010010110100100100010101100110100100100101010111010011010010010010101101001011010010010010010110100101101001001010001011001101001001010010101110100010101110100100101110011010010010101001011100110100101000101010111010001000111010000101001011010010100010111010010100010101101000100111010010100010010111010001001110100101001000101110011010010001000111010001001110100101001010101110011010010100000111001101010101010110100000001110100101010010101011101001000111010010101001010111001101000010100100110011010100000110100000001110100101010100101011100110101000100001101000000011101000100101010101110100010001110101010101010101011010000100111010010001001010111010010101000100110101000000010110100100111010100001010111010010000110101000000010110100100011101010010010111010000110101000010101011010100010111010100001010010111010100010111010100010101010111001101010001010110100001101010001001010The enterprising reader, with an effective home computer, may care to check, using the prescriptions given in the text, that the above code does in fact give a universal Turing machine’s action, by applying it to various simple Turing machine numbers!Some lowering of the value of u might have been possible with a different specification for a Turing machine. For example, we could dispense with STOP and, instead, adopt the rule that the machine stops whenever the internal state 0 is re-entered after it has been in some other internal state. This would not gain a great deal (if anything at all). A bigger gain would have resulted had we allowed tapes with marks other than just 0 or 1. Very concise-looking universal Turing machines have indeed been described in the literature, but the conciseness is deceptive, for they depend upon exceedingly complicated codings for the descriptions of Turing machines generally.8. For a non-technical discussion of matters relating to this famous assertion, see Devlin (1988).9. We could, of course, defeat this improved algorithm too, by simply applying the foregoing procedure all over again. We can then use this new knowledge to improve our algorithm still further; but we could defeat that one also, and so on. The kind of consideration that this iterative procedure leads us into will be discussed in connection with Gödel’s theorem, in Chapter 4, cf. p. 142.

3MATHEMATICS AND REALITYTHE LAND OF TOR’BLED-NAMLET US IMAGINE that we have been travelling on a great journey to some far-off world. We shall call this world Tor’Bled-Nam. Our remote sensing device has picked up a signal which is now displayed on a screen in front of us. The image comes into focus and we see (Fig. 3.1):Fig. 3.1. A first glimpse of a strange world.What can it be? Is it some strange-looking insect? Perhaps, instead, it is a dark-coloured lake, with many mountain streams entering it. Or could it be some vast and oddly shaped alien city, with roads going off in various directions to small towns and villages nearby? Maybe it is an island – and then let us try to find whether there is a nearby continent with which it is associated. This we can do by ‘backing away’, reducing the magnification of our sensing device by a linear factor of about fifteen. Lo and behold, the entire world springs into view (Fig. 3.2):Fig. 3.2. Tor ‘Bled-Nam’ in its entirety. The locations of the magnifications shown in Figs 3.1, 3.3, and 3.4 are indicated beneath the arrows.Our ‘island’ is seen as a small dot indicated below ‘Fig. 3.1’ in Fig. 3.2. The filaments (streams, roads, bridges?), from the original island all come to an end, with the exception of the one attached at the inside of its right-hand crevice, which finally joins on to the very much larger object that we see depicted in Fig. 3.2. This larger object is clearly similar to the island that we saw first – though it is not precisely the same. If we focus more closely on what appears to be this object’s coastline we see innumerable protuberances – roundish, but themselves possessing similar protuberances of their own. Each small protuberance seems to be attached to a larger one at some minute place, producing many warts upon warts. As the picture becomes clearer, we see myriads of tiny filaments emanating from the structure. The filaments themselves are forked at various places and often meander wildly. At certain spots on the filaments we seem to see little knots of complication which our sensing device, with its present magnification, cannot resolve. Clearly the object is no actual island or continent, nor a landscape of any kind. Perhaps, after all, we are viewing some monstrous beetle, and the first that we saw was one of its offspring, attached to it still, by some kind of filamentary umbilical cord.Let us try to examine the nature of one of our creature’s warts, by turning up the magnification of our sensing device by a linear factor of about ten (Fig. 3.3 – the location being indicated under ‘Fig. 3.3’ in Fig. 3.2). The wart itself bears a strong resemblance to the creature as a whole – except just at the point of attachment. Notice that there are various places in Fig. 3.3 where five filaments come together. There is perhaps a certain ‘fiveness’ about this particular wart (as there would be a ‘threeness’ about the uppermost wart). Indeed, if we were to examine the next reasonable-sized wart, a little down on the left on Fig. 3.2, we should find a ‘sevenness’ about it; and for the next, a ‘nineness’, and so on. As we enter the crevice between the two largest regions of Fig. 3.2, we find warts on the right characterized by odd numbers, increasing by two each time. Let us peer deep down into this crevice, turning up the magnification from that of Fig. 3.2 by a factor of about ten (Fig. 3.4). We see numerous other tiny warts and also much swirling activity. On the right, we can just discern some tiny spiral ‘seahorse tails’ – in an area we shall know as ‘seahorse valley’. Here we shall find, if the magnification is turned up enough, various ‘sea anemones’ or regions with a distinctly floral appearance. Perhaps, after all, this is indeed some exotic coastline – maybe some coral reef, teeming with life of all kinds. What might have seemed to be a flower would reveal itself, on further magnification, to be composed of myriads of tiny, but incredibly complicated structures, each with numerous filaments and swirling spiral tails. Let us examine one of the larger seahorse tails in some detail, namely the one just discernible where indicated as ‘Fig. 3.5’ in Fig. 3.4 (which is attached to a wart with a ‘29-ness’ about it!). With a further approximate 250-fold magnification, we are presented with the spiral depicted in Fig. 3.5. We find that this is no ordinary tail, but is itself made up of the most complicated swirlings back and forth, with innumerable tiny spirals, and regions like octopuses and seahorses.Fig. 3.3. A wart with a ‘fiveness’ about its filaments.At many places, the structure is attached just where two spirals come together. Let us examine one of these places (indicated below ‘Fig. 3.6’ in Fig. 3.5), increasing our magnification by a factor of about thirty. Behold: do we discern a strange but now familiar object in the middle? A further increase of magnification by a factor of about six (Fig. 3.7) reveals a tiny baby creature – almost identical to the entire structure we have been examining! If we look closely, we see that the filaments emanating from it differ a little from those of the main structure, and they swirl about and extend to relatively much greater distances. Yet the tiny creature itself seems to differ hardly at all from its parent, even to the extent of possessing offspring of its own, in closely corresponding positions. These we could again examine if we turned up the magnification still further. The grandchildren would also resemble their common ancestor – and one readily believes that this continues indefinitely. We may explore this extraordinary world of Tor’Bled-Nam as long as we wish, tuning our sensing device to higher and higher degrees of magnification. We find an endless variety: no two regions are precisely alike – yet there is a general flavour that we soon become accustomed to. The now familiar beetle-like creatures emerge at yet tinier and tinier scales. Every time, the neighbouring filamentary structures differ from what we had seen before, and present us with fantastic new scenes of unbelievable complication.Fig. 3.4. The main crevice. ‘Seahorse valley’ is just discernible on the lower right.What is this strange, varied and most wonderfully intricate land that we have stumbled upon? No doubt many readers will already know. But some will not. This world is nothing but a piece of abstract mathematics – the set known as the Mandelbrot set.1 Complicated it undoubtedly is; yet it is generated by a rule of remarkable simplicity! To explain the rule properly, I shall first need to explain what a complex number is. It is as well that I do so here. We shall need complex numbers later. They are absolutely fundamental to the structure of quantum mechanics, and are therefore basic to the workings of the very world in which we live. They also constitute one of the Great Miracles of Mathematics. In order to explain what a complex number is, I shall need, first, to remind the reader what is meant by the term ‘real number’. It will be helpful, also, to indicate the relationship between that concept and the very reality of the ‘real world’!Fig. 3.5. A close-up of a seahorse tail.Fig. 3.6. A further magnification of a joining point where two spirals come together. A tiny baby is just visible at the central point.Fig. 3.7. On magnification, the baby is seen closely to resemble the entire world.REAL NUMBERSRecall that the natural numbers are the whole quantities:0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, . . .These are the most elementary and basic amongst the different kinds of number. Any type of discrete entity can be quantified by the use of natural numbers: we may speak of twenty-seven sheep in a field, of two lightning flashes, twelve nights, one thousand words, four conversations, zero new ideas, one mistake, six absentees, two changes of direction, etc. Natural numbers can be added or multiplied together to produce new natural numbers. They were the objects of our general discussion of algorithms, as given in the last chapter.However some important operations may take us outside the realm of the natural numbers – the simplest being subtraction. For subtraction to be defined in a systematic way, we need negative numbers; we can set out the whole system of integers. . ., –6, –5, –4, –3, –2, –1, 0, 1, 2, 3, 4, 5, 6, 7, . . .for this purpose. Certain things, such as electric charge, bank balances, or dates* are quantified by numbers of this kind. These numbers are still too limited in their scope, however, since we can come unstuck when we try to divide one such number by another. Accordingly, we shall need the fractions, or rational numbers as they are called0, 1, –1, 1/2, –1/2, 2, –2, 3/2, –3/2, 1/3, . . .These suffice for the operations of finite arithmetic, but for a good many purposes we need to go further than this and include infinite or limiting operations. The familiar – and mathematically highly important – quantity π, for example, arises in many such infinite expressions. In particular, we have:π = 2{(2/1)(2/3)(4/3)(4/5)(6/5)(6/7)(8/7)(8/9). . .}andπ = 4(1 – 1/3 + 1/5 – + 1/9 – 1/11 + . . .).These are famous expressions, the first having been found by the English mathematician, grammarian, and cipher expert John Wallis, in 1655; and the second, in effect, by the Scottish mathematician and astronomer (and inventor of the first reflecting telescope) James Gregory, in 1671. As with π, numbers defined in this way need not be rational (i.e. not of the form n/m, where n and m are integers with m non-zero). The number system needs to be extended in order that such quantities can be included.This extended number system is referred to as the system of ‘real’ numbers – those familiar numbers which can be represented as infinite decimal expansions, such as:–583.70264439121009538. . .In terms of such a representation we have the well-known expression for π:π = 3.14159265358979323846. . .Among the types of number that can also be represented in this way are the square roots (or cube roots or fourth roots, etc.) of positive rational numbers, such as:√2 = 1.41421356237309504. . .;or, indeed, the square root (or cube root etc.) of any positive real number, as with the expression for π found by the great Swiss mathematician Leonhard Euler:π = √ {6(1 + 1/4 + 1/9 + 1/25 + 1/36 +. . .)}Real numbers are, in effect, the familiar kinds of number that we have to deal with in everyday life, although normally we are concerned merely with approximations to such numbers, and are happy to work with expansions involving only a small number of decimal places. In mathematical statements, however, real numbers may need to be specified exactly, and we require some sort of infinite description such as an entire infinite decimal expansion, or perhaps some other infinite mathematical expression such as the above formulae for π given by Wallis, Gregory, and Euler. (I shall normally use decimal expansions in my descriptions here, but only because these are most familiar. To a mathematician, there are various rather more satisfactory ways of presenting real numbers, but we shall not need to worry about this here.)It might be felt that it is impossible to contemplate an entire infinite expansion, but this is not so. A simple example where one clearly can contemplate the entire sequence is1/3 = 0.333333333333333. . .,where the dots indicate to us that the succession of 3s carries on indefinitely. To contemplate this expansion, all we need to know is that the expansion does indeed continue in the same way indefinitely with 3s. Every rational number has a repeated (or finite) decimal expansion, such as93/74 = 1.2567567567567567. . .,where the sequence 567 is repeated indefinitely, and this can also be contemplated in its entirety. Also, the expression0.220002222000002222220000000222222220. . .,which defines an irrational number, can certainly be contemplated in its entirety (the string of 0s or 2s simply increasing in length by one each time), and many similar examples can be given. In each case, we shall be satisfied when we know a rule according to which the expansion is constructed. If there is some algorithm which generates the successive digits, then knowledge of that algorithm provides us with a way of contemplating the entire infinite decimal expansion. Real numbers whose expansions can be generated by algorithms are called computable numbers (see also p. 66). (The use of a denary rather than, say, a binary expansion here has no significance. The numbers which are ‘computable’ in this sense are just the same numbers whichever base for an expansion is used.) The real numbers π and √2 that we have just been considering are examples of computable numbers. In each case the rule would be a little complicated to state in detail, but not hard in principle.However, there are also many real numbers which are not computable in this sense. We have seen in the last chapter that there are non-computable sequences which are nevertheless perfectly well defined. For example, we could take the decimal expansion whose nth digit is 1 or 0 according to whether or not the nth Turing machine acting on the number n stops or does not stop. Generally, for a real number, we just ask that there should be some infinite decimal expansion. We do not ask that there should be an algorithm for generating the nth digit, nor even that we should be aware of any kind of rule which in principle defines what the nth digit actually is.2 Computable numbers are awkward things to work with. One cannot keep all one’s operations computable, even when one works just with computable numbers. For example, it is not even a computable matter to decide, in general, whether two computable numbers are equal to one another or not! For this kind of reason, we prefer to work, instead, with all real numbers, where the decimal expansion can be anything at all, and need not just be, say, a computable sequence.Finally, I should point out that there is an identification between a real number whose decimal expansion ends with an infinite succession of 9s and one whose expansion ends with an infinite succession of Os; for example–27.1860999999. . . = –27.1861000000. . .HOW MANY REAL NUMBERS ARE THERE?Let us pause for a moment to appreciate the vastness of the generalization that has been achieved in moving from the rational numbers to the real numbers.One might think, at first, that the number of integers is already greater than the number of natural numbers; since every natural number is an integer whereas some integers (namely the negative ones) are not natural numbers, and similarly one might think that the number of fractions is greater than the number of integers. However, this is not the case. According to the powerful and beautiful theory of infinite numbers put forward in the late 1800s by the highly original Russian-German mathematician Georg Cantor, the total number of fractions, the total number of integers and the total number of natural numbers are all the same infinite number, denoted ℵ0 (‘aleph nought’). (Remarkably, this kind of idea had been partly anticipated some 250 years before, in the early 1600s, by the great Italian physicist and astronomer Galileo Galilei. We shall be reminded of some of Galileo’s other achievements in Chapter 5.) One may see that the number of integers is the same as the number of natural numbers by setting up a ‘one-to-one correspondence’ as follows:Note that each integer (in the left-hand column) and each natural number (in the right-hand column) occurs once and once only in the list. The existence of a one-to-one correspondence like this is what, in Cantor’s theory, establishes that the number of objects in the left-hand column is the same as the number of objects in the right-hand column. Thus, the number of integers is, indeed, the same as the number of natural numbers. In this case the number is infinite, but no matter. (The only peculiarity that occurs with infinite numbers is that we can leave out some of the members of the list and still find a one-to-one correspondence between the two lists!) In a similar, but somewhat more complicated way, we can set up a one-to-one correspondence between the fractions and the integers. (For this we can adapt one of the ways of representing pairs of natural numbers, the numerators and denominators, as single natural numbers; see Chapter 2, p. 56.) Sets that can be put into one-to-one correspondence with the natural numbers are called countable, so the countable infinite sets are those with So elements. We have now seen that the integers are countable, and so also are all the fractions.Are there sets which are not countable? Although we have extended the system, in passing from the natural numbers to first the integers and then the rational numbers, we have not actually increased the total number of objects that we have to work with. We have seen that the number of objects is actually countable in each case. Perhaps the reader has indeed got the impression by now that all infinite sets are countable. Not so; for the situation is very different in passing to the real numbers. It was one of Cantor’s remarkable achievements to show that there are actually more real numbers than rationals. The argument that Cantor used is the ‘diagonal slash’ that was referred to in Chapter 2 and that Turing adapted in his argument to show that the halting problem for Turing machines is insoluble. Cantor’s argument, like Turing’s later one, proceeds by reductio ad absurdum. Suppose that the result we are trying to establish is false, i.e. that the set of all real numbers is countable. Then the real numbers between 0 and 1 are certainly countable, and we shall have some list providing a one-to-one pairing of all such numbers with the natural numbers, such as:I have marked out the diagonal digits in bold type. These digits are, for this particular listing,1, 4, 1, 0, 0, 3, 1, 4, 8, 5, 1,. . .and the diagonal slash procedure is to construct a real number (between 0 and 1) whose decimal expansion (after the decimal point) differs from these digits in each corresponding place. For definiteness, let us say that the digit is to be 1 whenever the diagonal digit is different from 1 and it is 2 whenever the diagonal digit is 1. Thus, in this case we get the real number0.21211121112 . . .This real number cannot appear in our listing since it differs from the first number in the first decimal place (after the decimal point), from the second number in the second place, from the third number in the third place, etc. This is a contradiction because our list was supposed to contain all real numbers between 0 and 1. This contradiction establishes what we are trying to prove, namely that there is no one-to-one correspondence between the real numbers and the natural numbers and, accordingly, the number of real numbers is actually greater than the number of rational numbers and is not countable.The number of real numbers is the infinite number labelled C. (C stands for continuum, another name for the system of real numbers.) One might ask why this number is not called, ℵ1 say. In fact the symbol ℵ1 stands for the next infinite number greater than ℵ0 and it is a famous unsolved problem to decide whether in fact C = ℵ1, the so-called continuum hypothesis.It may be remarked that the computable numbers, on the other hand, are countable. To count them we just list, in numerical order, those Turing machines which generate real numbers (i.e. which produce the successive digits of real numbers). We may wish to strike from the list any Turing machine which generates a real number that has already appeared earlier in the list. Since the Turing machines are countable, it must certainly be the case that the computable real numbers are countable. Why can we not use the diagonal slash on that list and produce a new computable number which is not in the list? The answer lies in the fact that we cannot computably decide, in general, whether or not a Turing machine should actually be in the list. For to do so would, in effect, involve our being able to solve the halting problem. Some Turing machines may start to produce the digits of a real number, and then get stuck and never again produce another digit (because it ‘doesn’t stop’). There is no computable means of deciding which Turing machines will get stuck in this way. This is basically the halting problem. Thus, while our diagonal procedure will produce some real number, that number will not be a computable number. In fact, this argument could have been used to show the existence of non-computable numbers. Turing’s argument to show the existence of classes of problems which cannot be solved algorithmically, as was recounted in the last chapter, follows precisely this line of reasoning. We shall see other applications of the diagonal slash later.‘REALITY’ OF REAL NUMBERSSetting aside the notion of computability, real numbers are called ‘real’ because they seem to provide the magnitudes needed for the measurement of distance, angle, time, energy, temperature, or of numerous other geometrical and physical quantities. However, the relationship between the abstractly defined ‘real’ numbers and physical quantities is not as clear-cut as one might imagine. Real numbers refer to a mathematical idealization rather than to any actual physically objective quantity. The system of real numbers has the property, for example, that between any two of them, no matter how close, there lies a third. It is not at all clear that physical distances or times can realistically be said to have this property. If we continue to divide up the physical distance between two points, we would eventually reach scales so small that the very concept of distance, in the ordinary sense, could cease to have meaning. It is anticipated that at the ‘quantum gravity’ scale of 1020th of the size* of a subatomic particle, this would indeed be the case. But to mirror the real numbers, we would have to go to scales indefinitely smaller than this: 10200th, 102OOOth, or 1010 200th of a particle size, for example. It is not at all clear that such absurdly tiny scales have any physical meaning whatever. A similar remark would hold for correspondingly tiny intervals of time.The real number system is chosen in physics for its mathematical utility, simplicity, and elegance, together with the fact that it accords, over a very wide range, with the physical concepts of distance and time. It is not chosen because it is known to agree with these physical concepts over all ranges. One might well anticipate that there is indeed no such accord at very tiny scales of distance or time. It is commonplace to use rulers for the measurement of simple distances, but such rulers will themselves take on a granular nature when we get down to the scale of their own atoms. This does not, in itself, prevent us from continuing to use real numbers in an accurate way, but a good deal more sophistication is needed for the measurement of yet smaller distances. We should at least be a little suspicious that there might eventually be a difficulty of fundamental principle for distances on the tiniest scale. As it turns out, Nature is remarkably kind to us, and it appears that the same real numbers that we have grown used to for the description of things at an everyday scale or larger retain their usefulness on scales much smaller than atoms – certainly down to less than one-hundredth of the ‘classical’ diameter of a sub-atomic particle, say an electron or proton – and seemingly down to the ‘quantum gravity scale’, twenty orders of magnitude smaller than such a particle! This is a quite extraordinary extrapolation from experience. The familiar concept of real-number distance seems to hold also out to the most distant quasar and beyond, giving an overall range of at least 1042, and perhaps 1060 or more. The appropriateness of the real number system is not often questioned, in fact. Why is there so much confidence in these numbers for the accurate description of physics, when our initial experience of the relevance of such numbers lies in a comparatively limited range? This confidence – perhaps misplaced – must rest (although this fact is not often recognized) on the logical elegance, consistency, and mathematical power of the real number system, together with a belief in the profound mathematical harmony of Nature.COMPLEX NUMBERSThe real number system does not, as it turns out, have a monopoly with regard to mathematical power and elegance. There is still a certain awkwardness in that, for example, square roots can be taken only of positive numbers (or zero) and not of negative ones. From the mathematical point of view – and leaving aside, for the moment, any question of direct connection with the physical world – it turns out to be extremely convenient to be able to extract square roots of negative as well as positive numbers. Let us simply postulate, or ‘invent’ a square root for the number –1. We shall denote this by the symbol ‘i’, so we have:i2 = –1.The quantity i cannot, of course, be a real number since the product of a real number with itself is always positive (or zero, if the number is itself zero). For this reason the term imaginary has been conventionally applied to numbers whose squares are negative. However, it is important to stress the fact that these ‘imaginary’ numbers are no less real than the ‘real’ numbers that we have become accustomed to. As I have emphasized earlier, the relationship between such ‘real’ numbers and physical reality is not as direct or compelling as it may at first seem to be, involving, as it does, a mathematical idealization of infinite refinement for which there is no clear a priori justification from Nature.Having a square root for – 1, it is now no great effort to provide square roots for all the real numbers. For if a is a positive real number, then the quantityi × √ais a square root of the negative real number –a. (There is also one other square root, namely – i × √a.) What about i itself? Does this have a square root? It surely does. For it is easily checked that the quantity(1 + i) / √2(and also the negative of this quantity) squares to i. Does this number have a square root? Once again, the answer is yes; the square ofor its negative is indeed (1 + i)/√2.Notice that in forming such quantities we have allowed ourselves to add together real and imaginary numbers, as well as to multiply our numbers by arbitrary real numbers (or divide by non-zero real numbers, which is the same thing as multiplying by their reciprocals). The resulting objects are what are referred to as complex numbers. A complex number is a number of the forma + ibwhere a and b are real numbers, called the real part and the imaginary part respectively, of the complex number. The rules for adding and multiplying two such numbers follow the ordinary rules of (school) algebra, with the added rule that i2 = –1:(a + ib) + (c + id) = (a + c) + i (b + d)(a + ib) × (c + id) =(ac – bd) + i (ad + bc).A remarkable thing now happens! Our motivation for this system of numbers had been to provide the possibility that square roots can always be taken. It achieves this task, though this is itself not yet obvious. But it also does a great deal more: cube roots, fifth roots, ninety-ninth roots, nth roots, (1 + i)th roots, etc. can all be taken with impunity (as the great eighteenth century mathematician Leonhard Euler was able to show). As another example of the magic of complex numbers, let us examine the somewhat complicated-looking formulae of trigonometry that one has to learn in school; the sines and cosines of the sum of two anglessin (A + B) = sin A cos B + cos A sin B,cos (A + B) = cos A cos B – sin A sin B,are simply the imaginary and real parts, respectively, of the much simpler (and much more memorable!) complex equation*eiA + iB = eiA eiB.Here all we need to know is ‘Euler’s formula’ (apparently also obtained many years before Euler by the remarkable 16th century English mathematician Roger Cotes)eiA = cos A + i sin A,which we substitute into the equation above. The resulting expression iscos(A + B) + i sin(A + B) = (cos A + i sin A) (cos B + i sin B),and multiplying out the right-hand side we obtain the required trigonometrical relations.What is more, any algebraic equationa0 + a1z + a2z2 + a3z3 + . . . + anzn = 0(for which a0, a1, a2 . . . an are complex numbers, with an ≠ 0) can always be solved for some complex number z. For example,there is a complex number z satisfying the relationz102 + 999z33 – πz2 = – 417 +i,though this is by no means obvious! The general fact is sometimes referred to as ‘the fundamental theorem of algebra’. Various eighteenth century mathematicians had struggled to prove this result. Even Euler had not found a satisfactory general argument. Then, in 1831, the great mathematician and scientist Carl Friedrich Gauss gave a startlingly original line of argument and provided the first general proof. A key ingredient of this proof was to represent the complex numbers geometrically, and then to use a topological* argument.Actually, Gauss was not really the first to use a geometrical description of complex numbers. Wallis had done this, crudely, about two hundred years earlier, though he had not used it to nearly such powerful effect as had Gauss. The name normally attached to this geometrical representation of complex numbers belongs to Jean Robert Argand, a Swiss bookkeeper, who described it in 1806, although the Norwegian surveyor Caspar Wessel had, in fact, given a very complete description nine years earlier. In accordance with this conventional (but not altogether historically accurate) terminology I shall refer to the standard geometrical representation of complex numbers as the Argand plane.The Argand plane is an ordinary Euclidean plane with standard Cartesian coordinates x and y, where x marks off horizontal distance (positive to the right and negative to the left) and where y marks off vertical distance (positive upwards and negative downwards). The complex numberz = x + iyis then represented by the point of the Argand plane whose coordinates are(x, y)(see Fig. 3.8).Fig. 3.8. The Argand plane, depicting a complex number z = x + iy.Note that 0 (regarded as a complex number) is represented by the origin of coordinates, and 1 is represented as a particular point on the x-axis.The Argand plane simply provides us with a way of organizing our family of complex numbers into a geometrically useful picture. This kind of thing is not really something new to us. We are already familiar with the way that real numbers can be organized into a geometrical picture, namely the picture of a straight line that extends indefinitely in both directions. One particular point of the line is labelled 0 and another is labelled 1. The point 2 is placed so that its displacement from 1 is the same as the displacement of 1 from 0; the point 1/2 is the mid-point of 0 and 1; the point –1 is situated so that 0 lies mid-way between it and 1, etc., etc. The set of real numbers displayed in this way is referred to as the real line. For complex numbers we have, in effect, two real numbers to use as coordinates, namely a and b, for the complex number a + ib. These two numbers give us coordinates for points on a plane – the Argand plane. As an example, I have indicated in Fig. 3.9 approximately where the complex numbersu = 1 + i 1.3, v = – 2 + i, w = – 1.5 – i 0.4,should be placed.Fig. 3.9. Locations in the Argand plane of u – 1 + i 1.3, v = – 2 + i, and w = –1.5 – i 0.4.The basic algebraic operations of addition and multiplication of complex numbers now find a clear geometrical form. Let us consider addition first. Suppose u and ν are two complex numbers, represented on the Argand plane in accordance with the above scheme. Then their sum u + ν is represented as the ‘vector sum’ of the two points; that is to say, the point u + ν occurs at the place which completes the parallelogram formed by u, ν, and the origin 0. That this construction (see Fig. 3.10) actually gives us the sum is not very hard to see, but I omit the argument here.Fig. 3.10. The sum u + of two complex numbers u and ν is obtained by the parallelogram law.Fig. 3.11. The product uv of two complex numbers u and v is such that the triangle formed by 0, v, and uv is similar to that formed by 0, 1, and u. Equivalently: the distance of uv from 0 is the product of the distances of u and ν from 0, and the angle that uv makes with the real (horizontal) axis is the sum of the angles that u and υ make with this axis.The product uv also has a clear geometrical interpretation (see Fig. 3.11), which is perhaps a little harder to see. (Again I omit the argument.) The angle, subtended at the origin, between 1 and uv is the sum of the angles between 1 and u and between 1 and ν (all angles being measured in an anticlockwise sense), and the distance of uv from the origin is the product of the distances from the origin of u and v. This is equivalent to saying that the triangle formed by 0, v, and uv is similar (and similarly oriented) to the triangle formed by 0, 1, and u. (The energetic reader who is not familiar with these constructions may care to verify that they follow directly from the algebraic rules for adding and multiplying complex numbers that were given earlier, together with the above trigonometric identities.)CONSTRUCTION OF THE MANDELBROT SETWe are now in a position to see how the Mandelbrot set is defined. Let z be some arbitrarily chosen complex number. Whatever this complex number is, it will be represented as some point on the Argand plane. Now consider the mapping whereby z is replaced by a new complex number, given byz →z2 + cwhere c is another fixed (i.e. given) complex number. The number z2 + c will be represented by some new point in the Argand plane. For example, if c happened to be given as the number 1.63 – i4.2, then z would be mapped according toz → z2 + 1.63 – i 4.2so that, in particular, 3 would be replaced by32 + 1.63 – i 4.2 = 9 + 1.63 – i 4.2 = 10.63 – i 4.2and –2.7 + i 0.3 would be replaced by(–2.7 + i 0.3)2 + 1.63 – i 4.2= (–2.7)2 – (0.3)2 + 1.63 + i{2(𔃀.7)(0.3) – 4.2}= 8.83 – i 5.82.When such numbers get complicated, the calculations are best carried out by an electronic computer.Now, whatever c may be, the particular number 0 is replaced, under this scheme, by the actual given number c. What about c itself? This must be replaced by the number c2 + c. Suppose we continue this process and apply the replacement to the number c2 + c; then we obtain(c2 + c)2 + c = c4 + 2c3 + c2 + c.Let us iterate the replacement again, applying it next to the above number to obtain(c4 + 2c3 + c2 + c)2 + c = c8 + 4c7 + 6c6 + 6c5 + 5c4 + 2c3 + c2 + cand then again to this number, and so on. We obtain a sequence of complex numbers, starting with 0:0, c, c2 + c, c4 + 2c3 + c2 + c, . . .Fig. 3.12. A sequence of points in the Argand plane is bounded if there is some fixed circle that contains all the points. (This particular iteration starts with zero and has .)Now if we do this with certain choices of the given complex number c, the sequence of numbers that we get in this way never wanders very far from the origin in the Argand plane; more precisely, the sequence remains bounded for such choices of c which is to say that every member of the sequence lies within some fixed circle centred at the origin (see Fig. 3.12). A good example where this occurs is the case c – 0, since in this case, every member of the sequence is in fact 0. Another example of bounded behaviour occurs with c = –1, for then the sequence is: 0, –1, 0, –1, 0, –1,. . . ; and yet another example occurs with c = i, the sequence being 0, i, i –1, – i, i –1, –i, i –1, – i, . . . However, for various other complex numbers c the sequence wanders farther and farther from the origin to indefinite distance; i.e. the sequence is unbounded, and cannot be contained within any fixed circle. An example of this latter behaviour occurs when c – 1, for then the sequence is 0, 1, 2, 5, 26, 677, 458330, . . . ; this also happens when c = –3, the sequence being 0, – 3, 6, 33, 1086, . . . ; and also when c = i – 1, the sequence being O, i – 1, – i – 1, – 1 + 3i, – 9 – i5, 55 + i91, – 5257 + i10011, . . .The Mandelbrot set, that is to say, the black region of our world of Tor’Bled-Nam, is precisely that region of the Argand plane consisting of points c for which the sequence remains bounded. The white region consists of those points c for which the sequence is unbounded. The detailed pictures that we saw earlier were all drawn from the outputs of computers. The computer would systematically run through possible choices of the complex number c, where for each choice of c it would work out the sequence 0, c, c2 + c,. . . and decide, according to some appropriate criterion, whether the sequence is remaining bounded or not. If it is bounded, then the computer would arrange that a black spot appear on the screen at the point corresponding to c. If it is unbounded, then the computer would arrange for a white spot. Eventually, for every pixel in the range under consideration, the decision would be made by the computer as to whether the point would be coloured white or black.The complexity of the Mandelbrot set is very remarkable, particularly in view of the fact that the definition of this set is, as mathematical definitions go, a strikingly simple one. It is also the case that the general structure of this set is not very sensitive to the precise algebraic form of the mapping z → z2 + c that we have chosen. Many other iterated complex mappings (e.g. z → z3 + iz2 + c) will give extraordinarily similar structures (provided that we choose an appropriate number to start with – perhaps not 0, but a number whose value is characterized by a clear mathematical rule for each appropriate choice of mapping). There is, indeed, a kind of universal or absolute character to these ‘Mandelbrot’ structures, with regard to iterated complex maps. The study of such structures is a subject on its own, within mathematics, which is referred to as complex dynamical systems.PLATONIC REALITY OF MATHEMATICAL CONCEPTS?How ‘real’ are the objects of the mathematician’s world? From one point of view it seems that there can be nothing real about them at all. Mathematical objects are just concepts; they are the mental idealizations that mathematicians make, often stimulated by the appearance and seeming order of aspects of the world about us, but mental idealizations nevertheless. Can they be other than mere arbitrary constructions of the human mind? At the same time there often does appear to be some profound reality about these mathematical concepts, going quite beyond the mental deliberations of any particular mathematician. It is as though human thought is, instead, being guided towards some external truth – a truth which has a reality of its own, and which is revealed only partially to any one of us.The Mandelbrot set provides a striking example. Its wonderfully elaborate structure was not the invention of any one person, nor was it the design of a team of mathematicians. Benoit Mandelbrot himself, the Polish-American mathematician (and protagonist of fractal theory) who first3 studied the set, had no real prior conception of the fantastic elaboration inherent in it, although he knew that he was on the track of something very interesting. Indeed, when his first computer pictures began to emerge, he was under the impression that the fuzzy structures that he was seeing were the result of a computer malfunction (Mandelbrot 1986)! Only later did he become convinced that they were really there in the set itself. Moreover, the complete details of the complication of the structure of Mandelbrot’s set cannot really be fully comprehended by any one of us, nor can it be fully revealed by any computer. It would seem that this structure is not just part of our minds, but it has a reality of its own. Whichever mathematician or computer buff chooses to examine the set, approximations to the same fundamental mathematical structure will be found. It makes no real difference which computer is used for performing calculations (provided that the computer is in accurate working order), apart from the fact that differences in computer speed and storage, and graphic display capabilities, may lead to differences in the amount of fine detail that will be revealed and in the speed with which that detail is produced. The computer is being used in essentially the same way that the experimental physicist uses a piece of experimental apparatus to explore the structure of the physical world. The Mandelbrot set is not an invention of the human mind: it was a discovery. Like Mount Everest, the Mandelbrot set is just there!Likewise, the very system of complex numbers has a profound and timeless reality which goes quite beyond the mental constructions of any particular mathematician. The beginnings of an appreciation of complex numbers came about with the work of Gerolamo Cardano. He was an Italian, who lived from 1501 to 1576, a physician by trade, a gambler, and caster of horoscopes (once casting a horoscope for Christ), and he wrote an important and influential treatise on algebra ‘Ars Magna’ in 1545. In this he put forward the first complete expression for the solution (in terms of surds, i.e. nth roots) of a general cubic equation.* He had noticed, however, that in a certain class of cases – the ones referred to as ‘irreducible’, where the equation has three real solutions – he was forced to take, at a certain stage in his expression, the square root of a negative number. Although this was puzzling to him, he realized that if he allowed himself to take such square roots, and only if, then he could express the full answer (the final answer being always real). Later, in 1572, Raphael Bombelli, in a work entitled ‘I’Algebra’, extended Cardano’s work and began the study of the actual algebra of complex numbers.While at first it may seem that the introduction of such square roots of negative numbers is just a device – a mathematical invention designed to achieve a specific purpose – it later becomes clear that these objects are achieving far more than that for which they were originally designed. As I mentioned above, although the original purpose of introducing complex numbers was to enable square roots to be taken with impunity, by introducing such numbers we find that we get, as a bonus, the potentiality for taking any other kind of root or for solving any algebraic equation whatever. Later we find many other magical properties that these complex numbers possess, properties that we had no inkling about at first. These properties are just there. They were not put there by Cardano, nor by Bombelli, nor Wallis, nor Coates, nor Euler, nor Wessel, nor Gauss, despite the undoubted farsightedness of these, and other, great mathematicians; such magic was inherent in the very structure that they gradually uncovered. When Cardano introduced his complex numbers, he could have had no inkling of the many magical properties which were to follow – properties which go under various names, such as the Cauchy integral formula, the Riemann mapping theorem, the Lewy extension property. These, and many other remarkable facts, are properties of the very numbers, with no additional modifications whatever, that Cardano had first encountered in about 1539.Is mathematics invention or discovery? When mathematicians come upon their results are they just producing elaborate mental constructions which have no actual reality, but whose power and elegance is sufficient simply to fool even their inventors into believing that these mere mental constructions are ‘real’? Or are mathematicians really uncovering truths which are, in fact, already ‘there’ – truths whose existence is quite independent of the mathematicians’ activities? I think that, by now, it must be quite clear to the reader that I am an adherent of the second, rather than the first, view, at least with regard to such structures as complex numbers and the Mandelbrot set.Yet the matter is perhaps not quite so straightforward as this. As I have said, there are things in mathematics for which the term ‘discovery’ is indeed much more appropriate than ‘invention’, such as the examples just cited. These are the cases where much more comes out of the structure than is put into it in the first place. One may take the view that in such cases the mathematicians have stumbled upon ‘works of God’. However, there are other cases where the mathematical structure does not have such a compelling uniqueness, such as when, in the midst of a proof of some result, the mathematician finds the need to introduce some contrived and far from unique construction in order to achieve some very specific end. In such cases no more is likely to come out of the construction than was put into it in the first place, and the word ‘invention’ seems more appropriate than ‘discovery’. These are indeed just ‘works of man’. On this view, the true mathematical discoveries would, in a general way, be regarded as greater achievements or aspirations than would the ‘mere’ inventions.Such categorizations are not entirely dissimilar from those that one might use in the arts or in engineering. Great works of art are indeed ‘closer to God’ than are lesser ones. It is a feeling not uncommon amongst artists, that in their greatest works they are revealing eternal truths which have some kind of prior etherial existence,* while their lesser works might be more arbitrary, of the nature of mere mortal constructions. Likewise, an engineering innovation with a beautiful economy, where a great deal is achieved in the scope of the application of some simple, unexpected idea, might appropriately be described as a discovery rather than an invention.Having made these points, however, I cannot help feeling that, with mathematics, the case for believing in some kind of etherial, eternal existence, at least for the more profound mathematical concepts, is a good deal stronger than in those other cases. There is a compelling uniqueness and universality in such mathematical ideas which seems to be of quite a different order from that which one could expect in the arts or engineering. The view that mathematical concepts could exist in such a timeless, etherial sense was put forward in ancient times (c. 360 BC) by the great Greek philosopher Plato. Consequently, this view is frequently referred to as mathematical Platonism. It will have considerable importance for us later.In Chapter 1, I discussed at some length the point of view of strong AI, according to which mental phenomena are supposed to find their existence within the mathematical idea of an algorithm. In Chapter 2, I stressed the point that the concept of an algorithm is indeed a profound and ‘God-given’ notion. In this chapter I have been arguing that such ‘God-given’ mathematical ideas should have some kind of timeless existence, independent of our earthly selves. Does not this viewpoint lend some credence to the strong-AI point of view, by providing the possibility of an etherial type of existence for mental phenomena? Just conceivably so – and I shall even be speculating, later, in favour of a view not altogether dissimilar from this; but if mental phenomena can indeed find a home of this general kind, I do not believe that it can be with the concept of an algorithm. What would be needed would be something very much more subtle. The fact that algorithmic things constitute a very narrow and limited part of mathematics will be an important aspect of the discussions to follow. We shall begin to see something of the scope and subtlety of non-algorithmic mathematics in the next chapter.NOTES1. See Mandelbrot (1986). The particular sequence of magnifications that I have chosen has been adapted from those of Peitgen and Richter (1986), where many remarkable coloured pictures of the Mandelbrot set are to be found. For further striking illustrations, see Peitgen and Saupe (1988).2. As far as I am aware, it is a consistent, though unconventional, point of view to demand that there should always be some kind of rule determining what the nth digit actually is, for an arbitrary real number, although such a rule may not be effective nor even definable at all in a preassigned formal system (see Chapter 4). I hope it is consistent, since it is the point of view that I should most wish to adhere to myself!3. There is actually some dispute concerning who it was that first came across this set (see Brooks and Matelski 1981, Mandelbrot 1989); but the very fact that there can be such a dispute lends further support for the view that the finding of this set was more like a discovery than an invention.

4TRUTH, PROOF, AND INSIGHTHILBERT’S PROGRAMME FOR MATHEMATICSWHAT IS TRUTH? How do we form our judgements as to what is true and what is untrue about the world? Are we simply following some algorithm – no doubt favoured over other less effective possible algorithms by the powerful process of natural selection? Or might there be some other, possibly non-algorithmic, route -perhaps intuition, instinct, or insight – to the divining of truth? This seems a difficult question. Our judgements depend upon complicated interconnected combinations of sense-data, reasoning, and guesswork. Moreover, in many worldly situations there may not be general agreement about what is actually true and what is false. To simplify the question, let us consider only mathematical truth. How do we form our judgements – perhaps even our ‘certain’ knowledge – concerning mathematical questions? Here, at least, things should be more clear-cut. There should be no question as to what actually is true and what actually is false – or should there? What, indeed, is mathematical truth?The question of mathematical truth is a very old one, dating back to the times of the early Greek philosophers and mathematicians – and, no doubt, earlier. However, some very great clarifications and startling new insights have been obtained just over the past hundred years, or so. It is these new developments that we shall try to understand. The issues are quite fundamental, and they touch upon the very question of whether our thinking processes can indeed be entirely algorithmic in nature. It is important for us that we come to terms with them.In the late nineteenth century, mathematics had made great strides, partly because of the development of more and more powerful methods of mathematical proof. (David Hilbert and Georg Cantor, whom we have encountered before, and the great French mathematician Henri Poincaré, whom we shall encounter later, were three who were in the forefront of these developments.) Accordingly, mathematicians had been gaining confidence in the use of such powerful methods. Many of these methods involved the consideration of sets* with infinite numbers of members, and proofs were often successful for the very reason that it was possible to consider such sets as actual ‘things’ -completed existing wholes, with more than a mere potential existence. Many of these powerful ideas had sprung from Cantor’s highly original concept of infinite numbers, which he had developed consistently using infinite sets. (We caught a glimpse of this in the previous chapter.)However, this confidence was shattered when in 1902 the British logician and philosopher Bertrand Russell produced his now famous paradox (itself anticipated by Cantor, and a direct descendant of Cantor’s ‘diagonal slash’ argument). To understand Russell’s argument, we first need some feeling for what is involved in considering sets as completed wholes. We may imagine some set that is characterized in terms of a particular property. For example, the set of red things is characterized in terms of the property of redness: something belongs to that set if and only if it has redness. This allows us to turn things about, and talk about a property in terms of a single object, namely the entire set of things with that property. With this viewpoint, ‘redness’ is the set of all red things. (We may also conceive that some other sets are just ‘there’, their elements being characterized by no such simple property.)This idea of defining concepts in terms of sets was central to the procedure, introduced in 1884 by the influential German logician Gottlob Frege, whereby numbers can be defined in terms of sets. For example, what do we mean by the actual number 3? We know what the property of ‘threeness’ is, but what is 3 itself? Now, ‘threeness’ is a property of collections of objects, i.e. it is a property of sets: a set has this particular property ‘threeness’ if and only if the set has precisely three members. The set of medal winners in a particular Olympic event has this property of ‘threeness’, for example. So does the set of tyres on a tricycle, or the set of leaves on a normal clover, or the set of solutions to the equation x3 – 6x2 + 11x – 6 = 0. What, then, is Frege’s definition of the actual number 3? According to Frege, 3 must be a set of sets: the set of all sets with this property of ‘threeness’.1 Thus a set has three members if and only if it belongs to Frege’s set 3.This may seem a little circular, but it is not, really. We can define numbers generally as totalities of equivalent sets, where ‘equivalent’ here means ‘having elements that can be paired off one-to-one with each other’ (i.e. in ordinary terms this would be ‘having the same number of members’). The number 3 is then the particular one of these sets which has, as one of its members, a set containing, for example, just one apple, one orange, and one pear. Note that this is a quite different definition of ‘3’ from Church’s ‘’ given on p. 90. There are also other definitions which can be given and which are rather more popular these days.Now, what about the Russell paradox? It concerns a set R defined in the following way:R is the set of all sets which are not members of themselves.Thus, R is a certain collection of sets; and the criterion for a set X to belong to this collection is that the set X is itself not to be found amongst its own members.Is it absurd to suppose that a set might actually be a member of itself? Not really. Consider, for example, the set / of infinite sets (sets with infinitely many members). There are certainly infinitely many different infinite sets, so I is itself infinite. Thus I indeed belongs to itself! How is it, then, that Russell’s conception gives us a paradox? We ask: is Russell’s very set R a member of itself or is it not? If it is not a member of itself then it should belong to R, since R consists precisely of those sets which are not members of themselves. Thus, R belongs to R after all – a contradiction. On the other hand, if R is a member of itself, then since ‘itself is actually R, it belongs to that set whose members are characterized by not being members of themselves, i.e. it is not a member of itself after all – again a contradiction!*This consideration was not a flippant one. Russell was merely using, in a rather extreme form, the same type of very general mathematical set-theoretic reasoning that the mathematicians were beginning to employ in their proofs. Clearly things had got out of hand, and it became appropriate to be much more precise about what kind of reasoning was to be allowed and what was not. It was obviously necessary that the allowed reasoning must be free from contradiction and that it should permit only true statements to be derived from statements previously known to be true. Russell himself, together with his colleague Alfred North Whitehead, set about developing a highly formalized mathematical system of axioms and rules of procedure, the aim being that it should be possible to translate all types of correct mathematical reasoning into their scheme. The rules were carefully selected so as to prevent the paradoxical types of reasoning that led to Russell’s own paradox. The specific scheme that Russell and Whitehead produced was a monumental piece of work. However, it was very cumbersome, and it turned out to be rather limited in the types of mathematical reasoning that it actually incorporated. The great mathematician David Hilbert, whom we first encountered in Chapter 2, embarked upon a much more workable and comprehensive scheme. All correct mathematical types of reasoning, for any particular mathematical area, were to be included. Moreover, Hilbert intended that it would be possible to prove that the scheme was free from contradiction. Then mathematics would be placed, once and for all, on an unassailably secure foundation.However, the hopes of Hilbert and his followers were dashed when, in 1931, the brilliant 25-year-old Austrian mathematical logician Kurt Gödel produced a startling theorem which effectively destroyed the Hilbert programme. What Gödel showed was that any such precise (‘formal’) mathematical system of axioms and rules of procedure whatever, provided that it is broad enough to contain descriptions of simple arithmetical propositions (such as ‘Fermat’s last theorem’, considered in Chapter 2) and provided that it is free from contradiction, must contain some statements which are neither provable nor disprovable by the means allowed within the system. The truth of such statements is thus ‘undecidable’ by the approved procedures. In fact, Gödel was able to show that the very statement of the consistency of the axiom system itself, when coded into the form of a suitable arithmetical proposition, must be one such ‘undecidable’ proposition. It will be important for us to understand the nature of this ‘undecidability’. We shall see why Gödel’s argument cut to the very core of the Hilbert programme. We shall also see how Gödel’s argument enables us, by the use of insight, to go beyond the limitations of any particular formalized mathematical system under consideration. This understanding will be crucial for much of the discussion to follow.FORMAL MATHEMATICAL SYSTEMSIt will be necessary to be a bit more explicit about what we mean by a ‘formal mathematical system of axioms and rules of procedure’. We must suppose that there is some alphabet of symbols in terms of which our mathematical statements are to be expressed. These symbols must certainly be adequate to allow a notation for the natural numbers in order that ‘arithmetic’ can be incorporated into our system. We can, if desired, just use the usual Arabic notation 0, 1, 2, 3,. . ., 9, 10, 11, 12,. . . for numbers, although this makes the specification of the rules a little more complicated than they need be. A much simpler specification would result if we use, say, 0, 01, 011, 0111, 01111,. . . to denote the sequence of natural numbers (or, as a compromise, we could use the binary notation). However, since this could cause confusion in the discussion which follows, I shall just stick to the usual Arabic notation in my descriptions, whatever notation the system might actually use. We might need a ‘space’ symbol to separate the different ‘words’ or ‘numbers’ of our system, but since that might also be confusing, we could use just a comma (,) for that purpose where needed. We shall also require letters to denote arbitrary (‘variable’) natural numbers (or perhaps integers, or rationals, etc. – but let’s stick to natural numbers here), say t, u, v, w, x, y, z, t’, t”, t’”, . . . The primed letters t’, t”,. . . may be needed since we do not wish to put any definite limit on the number of variables that can occur in an expression. We regard the prime (‘) as a separate symbol of the formal system, so that the actual number of symbols remains finite. We shall need symbols for the basic arithmetical operations =, +, ×, etc., perhaps for various kinds of brackets (,),[,], and for the logical symbols such as & (‘and’), ⇒ (‘implies’), ν (‘or’), ⊜ (‘if and only if), ~ (‘not’, or ‘it is not so that. . .’). In addition, we shall want the logical ‘quantifiers’: the existential quantifier Ǝ (‘there exists . . . such that’) and the universal quantifier ∀ (‘for all. . . we have’). Then we can make statements such as ‘Fermat’s last theorem’.~Ǝ w, x, y, z [(x + 1)w+3 + (y + 1)w+3 = (z + 1)w+3](see Chapter 2, p. 76). (I could have written ‘0111’ for ‘3’, and perhaps used a notation for ‘raising to a power’ that might fit in better with the formalism; but, as I said, I am sticking to the conventional symbols so as not to introduce unnecessary confusion.) The above statement reads (ending at the first square bracket):‘It is not so that there exist natural numbers w, x, y, z suchthat. . .’.We can also rewrite Fermat’s last theorem using ∀:∀ w, x, y, z [~(x + 1)w + 3 + (y + 1)w + 3 = (z + 1)w + 3],which reads (ending after the ‘not’ symbol after the first bracket):‘For all natural numbers w, x, y, z it is not so that. . .’,which is logically the same thing as before.We need letters to denote entire propositions, and for that I shall use capital letters: P, Q, R, S,. . . Such a proposition might in fact be the Fermat assertion above:F = ~ Ǝ w, x, y, z [(x + 1)w + 3 + (y + 1)w + 3 = (z + 1)w + 3].A proposition might also depend on one or more variables; for example we might be interested in the Fermat assertion for some particular* power w + 3:G(w) = ~Ǝ x,y,z [(x + 1)w+3 + (y + 1)w+3 = (z + 1)w+3],so that G(0) asserts ‘no cube can be the sum of positive cubes’, G(1) asserts the same for fourth powers, and so on. (Note that ‘w’ is missing after Ǝ’.) The Fermat assertion is now that G(w) holds for all w:F = ∀w[G(w)].G( ) is an example of what is called a propositional function, i.e. a proposition which depends on one or more variables.The axioms of the system will constitute a finite list of general propositions whose truth, given the meanings of the symbols, is supposed to be self-evident. For example, for arbitrary propositions or propositional functions P, Q, R( ), we could have, amongst our axioms:(P & Q) ⇒ P,~(~P) ⇔ P,~Ǝx[R(x)] ⇔ ∀x[~R(x)],the ‘self-evident truth’ of which is readily ascertained from their meanings. (The first one asserts simply that: ‘if P and Q are both true, then P is true’; the second asserts the equivalence between ‘it is not so that P is false’ and ‘P is true’; the third is exemplified by the logical equivalence of the two ways of stating Fermat’s last theorem given above.) We could also include basic arithmetical axioms, such as∀ x,y[x + y = y + x]∀ x,y,z[(x + y) × z = (x × z) + (y × z)],although one might prefer to build up these arithmetical operations from something more primitive and deduce these statements as theorems. The rules of procedure would be (self-evident) things like:‘from P and P ⇒ Q we can deduce Q’‘from ∀ x[R(x)] we can deduce any proposition obtained by substituting a specific natural number for x in R(x)’.These are instructions telling us how we may derive new propositions from propositions already established.Now, starting from the axioms, and then applying the rules of procedure again and again, we can build up some long list of propositions. At any stage, we can bring any of the axioms into play again, and we can always keep re-using any of the propositions that we have already added to our lengthening list. The propositions of any such correctly assembled list are referred to as theorems (though many of them will be quite trivial, or uninteresting as mathematical statements). If we have a specific proposition P that we want to prove, then we try to find such a list, correctly assembled according to these rules, and which terminates with our specific proposition P. Such a list would provide us with a proof of P within the system; and P would, accordingly, then be a theorem.The idea of Hilbert’s programme was to find, for any well-defined area of mathematics, a list of axioms and rules of procedure sufficiently comprehensive that all forms of correct mathematical reasoning appropriate to that area would be incorporated. Let us fix our area of mathematics to be arithmetic (where the quantifiers Ǝ and ∀ are included, so that statements like that of Fermat’s last theorem can be made). There will be no advantage to us to consider any mathematical area more general than this here. Arithmetic is already general enough for Gödel’s procedure to apply. If we can accept that such a comprehensive system of axioms and rules of procedure has indeed been given to us for arithmetic, in according with Hilbert’s programme, then we shall be provided with a definite criterion for the ‘correctness’ of mathematical proof for any proposition in arithmetic. The hope had been that such a system of axioms and rules could be complete, in the sense that it would enable us in principle to decide the truth or falsity of any mathematical statement that can be formulated within the system.Hilbert’s hope was that for any string of symbols representing a mathematical proposition, say P, one should be able to prove either P or ~P, depending upon whether P is true or false. Here we must assume that the string is syntactically correct in its construction, where ‘syntactically correct’ essentially means ‘grammatically’ correct – i.e. satisfying all the notational rules of the formalism, such as brackets being paired off correctly, etc. – so that P has a well-defined true or false meaning. If Hilbert’s hope could be realized, this would even enable us to dispense with worrying about what the propositions mean altogether! P would just be a syntactically correct string of symbols. The string of symbols P would be assigned the truth-value  if P is a theorem (i.e. if P is provable within the system) and it would be assigned the truth-value  if, on the other hand, ~P is a theorem. For this to make sense, we require consistency, in addition to completeness. That is to say, there must be no string of symbols P for which both of P and ~P are theorems. Otherwise P could be  and  at the same time!The point of view that one can dispense with the meanings of mathematical statements, regarding them as nothing but strings of symbols in some formal mathematical system, is the mathematical standpoint of formalism. Some people like this idea, whereby mathematics becomes a kind of ‘meaningless game’. It is not an idea that appeals to me, however. It is indeed ‘meaning’ – not blind algorithmic computation – that gives mathematics its substance Fortunately, Gödel dealt formalism a devastating blow! Let us see how he did this.GÖDEL’S THEOREMPart of Gödel’s argument was very detailed and complicated. However, it is not necessary for us to examine the intricacies of that part. The central idea, on the other hand, was simple, beautiful, and profound. This part we shall be able to appreciate. The complicated part (which also contained much ingenuity) was to show in detail how one may actually code the individual rules of procedure of the formal system, and also the use of its various axioms, into arithmetical operations. (It was an aspect of the profound part, though, to realize that this was a fruitful thing to do!) In order to carry out this coding we need to find some convenient way of labelling propositions with natural numbers. One way would be simply to use some kind of ‘alphabetical’ ordering for all the strings of symbols of the formal system for each specific length, where there is an overall ordering according to the length of the string. (Thus, the strings of length one could be alphabetically ordered, followed by the strings of length two, alphabetically ordered, followed by the strings of length three, etc.) This is called lexicographical ordering.* In fact Gödel originally used a more complicated numbering system, but the distinctions are not important for us. We shall be particularly concerned with propositional functions which are dependent on a single variable, like G(w) above. Let the nth such propositional function (in the chosen ordering of strings of symbols), applied to w, bePn(w).We can allow our numbering to be a little ‘sloppy’ if we wish, so that some of these expressions may not be syntactically correct. (This makes the arithmetical coding very much easier than if we try to omit all such syntactically incorrect expressions.) If Pn(w) is syntactically correct, it will be some perfectly well-defined particular arithmetical statement concerning the two natural numbers n and w. Precisely which arithmetical statement it is will depend on the details of the particular numbering system that has been chosen. That belongs to the complicated part of the argument and will not concern us here. The strings of propositions which constitute a proof of some theorem in the system can also be labelled by natural numbers using the chosen ordering scheme. LetΠndenote the nth proof. (Again, we can use a ‘sloppy numbering’ whereby for some values of n the expression ‘Πn’ is not syntactically correct and thus proves no theorem.)Now consider the following propositional function, which depends on the natural number w:~Ǝx[Πx proves Pw(w)].The statement in the square brackets is given partly in words, but it is a perfectly precisely defined statement. It asserts that the xth proof is actually a proof of that proposition which is Pw( ) applied to the value w itself. Outside the square bracket the negated existential quantifier serves to remove one of the variables (‘there does not exist an x such that . . .’), so we end up with an arithmetical propositional function which depends on only the one variable w. The expression as a whole asserts that there is no proof of Pw(w). I shall assume that it is framed in a syntactically correct way (even if Pw(w) is not – in which case the statement would be true, since there can be no proof of a syntactically incorrect expression). In fact, because of the translations into arithmetic that we are supposing have been carried out, the above is actually some arithmetical statement concerning the natural number w (the part in square brackets being a well-defined arithmetical statement about two natural numbers x and w). It is not supposed to be obvious that the statement can be coded into arithmetic, but it can be. Showing that such statements can indeed be so coded is the major ‘hard work’ involved in the complicated part of Gödel’s argument. As before, precisely which arithmetical statement it is will depend on the details of the numbering systems, and it will depend very much on the detailed structure of the axioms and rules of our formal system. Since all that belongs to the complicated part, the details of it will not concern us here.We have numbered all propositional functions which depend on a single variable, so the one we have just written down must have been assigned a number. Let us write this number as k. Our propositional function is the kth one in the list. Thus~Ǝx[Πx proves Pw(w)] = Pk(w).Now examine this function for the particular w-value: w = k. We get~Ǝx[Πx proves Pk(k)] = Pk(k).The specific proposition Pk(k) is a perfectly well-defined (syntactically correct) arithmetical statement. Does it have a proof within our formal system? Does its negation ~Pk(k) have a proof? The answer to both these questions must be ‘no’. We can see this by examining the meaning underlying the Gödel procedure. Although Pk(k) is just an arithmetical proposition, we have constructed it so that it asserts what has been written on the left-hand side: ‘there is no proof, within the system, of the proposition Pk(k)’. If we have been careful in laying down our axioms and rules of procedure, and assuming that we have done our numbering right, then there cannot be any proof of this Pk(k) within the system. For if there were such a proof, then the meaning of the statement that Pk(k) actually asserts, namely that there is no proof, would be false, so Pk(k) would have to be false as an arithmetical proposition. Our formal system should not be so badly constructed that it actually allows false propositions to be proved! Thus, it must be the case that there is in fact no proof of Pk(k). But this is precisely what Pk(k) is trying to tell us. What Pk(k) asserts must therefore be a true statement, so Pk(k) must be true as an arithmetical proposition. We have found a true proposition which has no proof within the system!What about its negation ~Pk(k) It follows that we had also better not be able to find a proof of this either. We have just established that ~Pk(k) must be false (since Pk(k) is true), and we are not supposed to be able to prove false propositions within the system! Thus, neither Pk(k) nor ~Pk(k) is provable within our formal system. This establishes Gödel’s theorem.MATHEMATICAL INSIGHTNotice that something very remarkable has happened here. People often think of Gödel’s theorem as something negative -showing the necessary limitations of formalized mathematical reasoning. No matter how comprehensive we think we have been, there will always be some propositions which escape the net. But should the particular proposition Pk(k) worry us? In the course of the above argument, we have actually established that Pk(k) is a true statement! Somehow we have managed to see that Pk(k) is true despite the fact that it is not formally provable within the system. The strict mathematical formalists should indeed be worried, because by this very reasoning we have established that the formalist’s notion of ‘truth’ must be necessarily incomplete. Whatever (consistent) formal system is used for arithmetic, there are statements that we can see are true but which do not get assigned the truth-value  by the formalist’s proposed procedure, as described above. The way that a strict formalist might try to get around this would perhaps be not to talk about the concept of truth at all but merely refer to provability within some fixed formal system. However, this seems very limiting. One could not even frame the Gödel argument as given above, using this point of view, since essential parts of that argument make use of reasoning about what is actually true and what is not true.2 Some formalists take a more ‘pragmatic’ view, claiming not to be worried by statements such as Pk(k) because they are extremely complicated and uninteresting as propositions of arithmetic. Such people would assert:Yes, there is the odd statement, such as Pk(k), for which my notion of provability or  does not coincide with your instinctive notion of truth, but those statements will never come up in serious mathematics (at least not in the kind I am interested in) because such statements are absurdly complicated and unnatural as mathematics.It is indeed the case that propositions like Pk(k) would be extremely cumbersome and odd-looking as mathematical statements about numbers, when written out in full. However, in recent years, some reasonably simple statements of a very acceptable mathematical character have been put forward which are actually equivalent to Gödel-type propositions.3 These are unprovable from the normal axioms of arithmetic, yet follow from an Obviously true’ property that the axiom system itself has.The formalist’s professed lack of interest in ‘mathematical truth’ seems to me to be a very strange point of view to adopt for a philosophy of mathematics. Furthermore, it is not really all that pragmatic. When mathematicians carry out their forms of reasoning, they do not want to have to be continually checking to see whether or not their arguments can be formulated in terms of the axioms and rules of procedure of some complicated formal system. They only need to be sure that their arguments are valid ways of ascertaining truth. The Gödel argument is another such valid procedure, so it seems to me that Pk(k) is just as good a mathematical truth as any that can be obtained more conventionally using the axioms and rules of procedure that can be laid down beforehand.A procedure that suggests itself is the following. Let us accept that Pk(k), which for the present I shall simply denote by G0, is indeed a perfectly valid proposition; so we may simply adjoin it to our system, as an additional axiom. Of course, our new amended system will have its own Gödel proposition, say G1, which again is seen to be a perfectly valid statement about numbers. Accordingly we adjoin G1 to our system also. This gives us a further amended system which will have its own Gödel proposition G2 (again perfectly valid), and we can then adjoin this, obtaining the next Gödel proposition G3, which we also adjoin, and so on, repeating this process indefinitely. What about the resulting system when we allow ourselves to use the entire list G0, G1
G2, G3, . . . as additional axioms? Might it be that this is complete? Since we now have an unlimited (infinite) system of axioms, it is perhaps not clear that the Gödel procedure will apply. However, this continuing adjoining of Gödel propositions is a perfectly systematic scheme and can be rephrased as an ordinary finite logical system of axioms and rules of procedure. This system will have its own Gödel proposition, say Gω, which we can again adjoin, and then form the Gödel proposition Gω+1, of the resulting system. Repeating, as above, we obtain a list Gω, Gω+1,
Gω+2,
Gω+3, . . . of propositions, all perfectly valid statements about natural numbers, and which can all be adjoined to our formal system. This is again perfectly systematic, and it leads us to a new system encompassing the whole lot; but this again will have its Gödel proposition, say Gω+ω, which we can rewrite as Gω2, and the whole procedure can be started up again, so that we get a new infinite, but systematic, list of axioms Gω2,
Gω2+1,
Gω2+2, etc., leading to yet a new system – and a new Gödel proposition Gω3. Repeating the entire procedure, we get Gω4 and then Gω5 and so on. Now this procedure is entirely systematic and has its own Gödel proposition Gω2.Does this ever end? In a sense, no; but it leads us into some difficult mathematical considerations that cannot be gone into in detail here. The above procedure was discussed by Alan Turing in a paper4 in 1939. In fact, very remarkably, any true (but just universally quantified) proposition in arithmetic can be obtained by a repeated ‘Gödelization’ procedure of this type! See Feferman (1988). However, this does to some degree beg the question of how we actually decide whether a proposition is true or false. The critical issue, at each stage, is to see how to code the adjoining of an infinite family of Gödel propositions into providing a single additional axiom (or finite number of axioms). This requires that our infinite family can be systematized in some algorithmic way. To be sure that such a systematization correctly does what it is supposed to do, we shall need to employ insights from outside the system – just as we did in order to see that Pk(k) was a true proposition in the first place. It is these insights that cannot be systematized – and, indeed, must lie outside any algorithmic action!The insight whereby we concluded that the Gödel proposition Pk(k) is actually a true statement in arithmetic is an example of a general type of procedure known to logicians as a reflection principle: thus, by ‘reflecting’ upon the meaning of the axiom system and rules of procedure, and convincing oneself that these indeed provide valid ways of arriving at mathematical truths, one may be able to code this insight into further true mathematical statements that were not deducible from those very axioms and rules. The derivation of the truth of Pk(k), as outlined above, depended upon such a principle. Another reflection principle, relevant to the original Gödel argument (though not given above), depends upon deducing new mathematical truths from the fact that an axiom system, that we already believe to be valid for obtaining mathematical truths, is actually consistent. Reflection principles often involve reasoning about infinite sets, and one must always be careful when using them, that one is not getting too close to the type of argument that could lead one into a Russell-type paradox. Reflection principles provide the very antithesis of formalist reasoning. If one is careful, they enable one to leap outside the rigid confinements of any formal system to obtain new mathematical insights that did not seem to be available before. There could be many perfectly acceptable results in our mathematical literature whose proofs require insights that lie far from the original rules and axioms of the standard formal systems for arithmetic. All this shows that the mental procedures whereby mathematicians arrive at their judgements of truth are not simply rooted in the procedures of some specific formal system. We see the validity of the Gödel proposition Pk(k) though we cannot derive it from the axioms. The type of ‘seeing’ that is involved in a reflection principle requires a mathematical insight that is not the result of the purely algorithmic operations that could be coded into some mathematical formal system. This matter will be returned to in Chapter 10.The reader may notice a certain similarity between the argument establishing the truth yet ‘unprovability’ of Pk(k) and the argument of Russell’s paradox. There is a similarity also with Turing’s argument establishing the non-existence of a Turing machine which will solve the halting problem. These similarities are not accidental. There is a strong thread of historical connection between the three. Turing found his argument after studying the work of Gödel. Gödel himself was well aware of the Russell paradox, and was able to transform paradoxical reasoning of this kind, which stretches the use of logic too far, into a valid mathematical argument. (All these arguments have their origins in Cantor’s ‘diagonal slash’, described in the previous chapter, p. 110).Why should we accept the Gödel and Turing arguments, whereas we have had to reject the reasoning leading to the Russell paradox? The former are much more clear-cut, and are unexceptionable as mathematical arguments, whereas the Russell paradox depends upon more nebulous reasoning involving ‘enormous’ sets. But it must be admitted that the distinctions are not really as clear as one would like them to be. The attempt to make such distinctions clear was a strong motive behind the whole idea of formalism. Gödel’s argument shows that the strict formalist viewpoint does not really hold together; yet it does not lead us to a wholly reliable alternative point of view. To my mind the issue is still unresolved. The procedure that is actually adopted* in contemporary mathematics, for avoiding the type of reasoning with ‘enormous’ sets that leads to Russell’s paradox, is not entirely satisfactory. Moreover it tends still to be stated in distinctly formalistic terms – or, alternatively, in terms that do not give us full confidence that contradictions cannot arise.Be that as it may, it seems to me that it is a clear consequence of the Gödel argument that the concept of mathematical truth cannot be encapsulated in any formalistic scheme. Mathematical truth is something that goes beyond mere formalism. This is perhaps clear even without Gödel’s theorem. For how are we to decide what axioms or rules of procedure to adopt in any case when trying to set up a formal system? Our guide in deciding on the rules to adopt must always be our intuitive understanding of what is ‘self-evidently true’, given the ‘meanings’ of the symbols of the system. How are we to decide which formal systems are sensible ones to adopt – in accordance, that is, with our intuitive feelings about ‘self-evidence’ and ‘meaning’ – and which are not? The notion of self-consistency is certainly not adequate for this. One can have many self-consistent systems which are not ‘sensible’ in this sense, where the axioms and rules of procedure have meanings that we would reject as false, or perhaps no meaning at all. ‘Self-evidence’ and ‘meaning’ are concepts which would still be needed, even without Gödel’s theorem.However, without Gödel’s theorem it might have been possible to imagine that the intuitive notions of ‘self-evidence’ and ‘meaning’ could have been employed just once and for all, merely to set up the formal system in the first place, and thereafter dispensed with as part of clear mathematical argument for determining truth. Then, in accordance with a formalist view, these ‘vague’ intuitive notions would have had roles to play as part of the mathematician’s preliminary thinking, as a guide towards finding the appropriate formal argument; but they would play no part in the actual demonstration of mathematical truth. Gödel’s theorem shows that this point of view is not really a tenable one in a fundamental philosophy of mathematics. The notion of mathematical truth goes beyond the whole concept of formalism. There is something absolute and ‘God-given’ about mathematical truth. This is what mathematical Platonism, as discussed at the end of the last chapter, is about. Any particular formal system has a provisional and ‘man-made’ quality about it. Such systems indeed have very valuable roles to play in mathematical discussions, but they can supply only a partial (or approximate) guide to truth. Real mathematical truth goes beyond mere man-made constructions.PLATONISM OR INTUITIONISM?I have pointed out two opposing schools of mathematical philosophy, siding strongly with the Platonist rather than the formalist view. I have actually been rather simplistic in my distinctions. There are many refinements of viewpoint that can be made. For example, one can argue under the heading of ‘Platonism’ whether the objects of mathematical thought have any kind of actual ‘existence’ or whether it is just the concept of mathematical ‘truth’ which is absolute. I have not chosen to make an issue of such distinctions here. In my own mind, the absoluteness of mathematical truth and the Platonic existence of mathematical concepts are essentially the same thing. The ‘existence’ that must be attributed to the Mandelbrot set, for example, is a feature of its ‘absolute’ nature. Whether a point of the Argand plane does, or does not, belong to the Mandelbrot set is an absolute question, independent of which mathematician, or which computer, is examining it. It is the Mandelbrot set’s ‘mathematician-independence’ that gives it its Platonic existence. Moreover, its finest details lie beyond what is accessible to us by use of computers. Those devices can yield only approximations to a structure that has a deeper and ‘computer-independent’ existence of its own. I do appreciate, however, that there can be many other viewpoints that are reasonable to hold on this question. We need not worry too much about these distinctions here.There are also differences of viewpoint concerning the lengths to which one may be prepared to carry one’s Platonism – if, indeed one claims to be a Platonist. Gödel, himself, was a very strong Platonist. The types of mathematical statement that I have been considering so far are rather ‘mild’ ones as such things go.5 More controversial statements can arise, particularly in the theory of sets. When all the ramifications of set theory are considered, one comes across sets which are so wildly enormous and nebulously constructed that even a fairly determined Platonist such as myself may begin to have doubts that their existence, or otherwise, is indeed an ‘absolute’ matter.6 There may come a stage at which the sets have such convoluted and conceptually dubious definitions that the question of the truth or falsity of mathematical statements concerning them may begin to take on a somewhat ‘matter-of-opinion’ quality rather than a ‘God-given’ one. Whether one is prepared to go the whole way with Platonism, along with Gödel, and demand that the truth or falsity of mathematical statements concerning such enormous sets is always an absolute or ‘Platonic’ matter, or whether one stops somewhere short of this and demands an absolute truth or falsity only when the sets are reasonably constructive and not so wildly enormous, is not a matter that will have a great relevance to our discussion here. The sets (finite or infinite) that will have significance for us are, by the standards of the ones that I have just been referring to, ridiculously tiny! Thus the distinctions between these various Platonistic views will not greatly concern us.There are, however, other mathematical standpoints, such as that known as intuitionism (and another called finitism), which go to the other extreme, where one refuses to accept the completed existence of any infinite set whatever.* Intuitionism was initiated, in 1924, by the Dutch mathematician L. E. J. Brouwer as an alternative response – distinct from that of formalism – to the paradoxes (such as Russell’s) that can arise when too free a use of infinite sets is made in mathematical reasoning. The roots of such a viewpoint can be traced back to Aristotle, who had been Plato’s pupil but who had rejected Plato’s views about the absolute existence of mathematical entities and about the acceptability of infinite sets. According to intuitionism, sets (infinite or otherwise) are not thought of as having an ‘existence’ in themselves but are thought of merely in terms of the rules which might determine their membership.A characteristic feature of Brouwer’s intuitionism is the rejection of ‘the law of the excluded middle’. This law asserts that the denial of the negation of a statement is equivalent to the assertion of that statement. (In symbols: ~(~P) ⇔ P, a relation we encountered above.) Perhaps Aristotle might have been unhappy with the denial of something so logically ‘obvious’ as this! In ordinary ‘common-sense’ terms, the law of the excluded middle may be regarded as a self-evident truth: if it is false that something is not true, then that thing is surely true! (This law is the basis of the mathematical procedure of ‘reductio ad absurdum’, cf. p. 78.) But the intuitionists find themselves able to deny this law. This is basically because they are taking a different attitude to the concept of existence, demanding that a definite (mental) construction be presented before it is accepted that a mathematical object actually exists. Thus, to an intuitionist, ‘existence’ means ‘constructive existence’. In a mathematical argument which proceeds by reductio ad absurdum one puts forward some hypothesis with the intention of showing that its consequences lead one into a contradiction, this contradiction providing the desired proof that the hypothesis in question is false. The hypothesis could take the form of a statement that a mathematical entity with certain required properties does not exist. When this leads to a contradiction, one infers, in ordinary mathematics, that the required entity does indeed exist. But such an argument, in itself, provides no means for actually constructing such an entity. To an intuitionist, this kind of existence is no existence at all; and it is in this kind of sense that they refuse to accept the law of the excluded middle and the procedure of reductio ad absurdum. Indeed, Brouwer was profoundly dissatisfied by such a non-constructive ‘existence’.7 Without an actual construction, he asserted, such a concept of existence is meaningless. In Brouwerian logic, one cannot deduce from the falsity of the non-existence of some object that the object actually exists!In my own view, although there is something commendable about seeking constructiveness in mathematical existence, Brouwer’s point of view of intuitionism is much too extreme. Brouwer first put forward his ideas in 1924, which was more than ten years before the work of Church and Turing. Now that the concept of constructiveness – in terms of Turing’s idea of computability – can be studied within the conventional framework of mathematical philosophy, there is no need to go to the extremes to which Brouwer would wish to take us. We can discuss constructiveness as a separate issue from the question of mathematical existence. If we go along with intuitionism, we must deny ourselves the use of very powerful kinds of argument within mathematics, and the subject becomes somewhat stifled and impotent.I do not wish to dwell on the various difficulties and seeming absurdities that the intuitionistic point of view leads one into; but it will perhaps be helpful if I refer to just a few of the problems. An example often referred to by Brouwer concerns the decimal expansion of π:3.141592653589793 . . .Does there exist a succession of twenty consecutive sevens somewhere in this expansion, i.e.π = 3.141592653589793 . . . 77777777777777777777 . . .,or does there not? In ordinary mathematical terms, all that we can say, as of now, is that either there does or there does not – and we do not know which! This would seem to be a harmless enough statement. However, the intuitionists would actually deny that one can validly say ‘either there exists a succession of twenty consecutive sevens somewhere in the decimal expansion of π, or else there does not’ – unless and until one has (in some constructive way acceptable to the intuitionists) either established that there is indeed such a succession, or else established that there is none! A direct calculation could suffice to show that a succession of twenty consecutive sevens actually does exist somewhere in the decimal expansion of π, but some sort of mathematical theorem would be needed to establish that there is no such succession. No computer has yet proceeded far enough in the computation of π to determine that there is indeed such a succession. One’s expectation on probabilistic grounds would be that such a succession does actually exist, but that even if a computer were to produce digits consistently at the rate of, say, 1010 per second, it would be likely to take something of the order of between one hundred and one thousand years to find the sequence! It seems to me to be much more likely that, rather than by direct computation, the existence of such a sequence will someday be established mathematically (probably as a corollary of some much more powerful and interesting result) – although perhaps not in a way acceptable to the intuitionists!This particular problem is of no real mathematical interest. It is given only as an example which is easy to state. In Brouwer’s extreme form of intuitionism, he would assert that, at the present time, the assertion ‘there exists a succession of twenty consecutive sevens somewhere in the decimal expansion of π’ is neither true nor false. If at some later date the appropriate result is established one way or the other, by computation or by (intuitionistic) mathematical proof, then the assertion would become ‘true’, or else ‘false’, as the case may be. A similar example would be ‘Fermat’s last theorem’. According to Brouwer’s extreme intuitionism, this also is neither true nor false at the present time, but it might become one or the other at some later date. To me, such subjectiveness and time-dependence of mathematical truth is abhorrent. It is, indeed, a very subjective matter whether, or when, a mathematical result might be accepted as officially ‘proved’. Mathematical truth should not rest on such society-dependent criteria. Also, to have a concept of mathematical truth which changes with time is, to say the least, most awkward and unsatisfactory for a mathematics which one hopes to be able to employ reliably for a description of the physical world. Not all intuitionists would take such a strong line as Brouwer’s. Nevertheless the intuitionist point of view is a distinctly awkward one, even for those sympathetic to the aims of constructivism. Few present-day mathematicians would go along with intuitionism wholeheartedly, if only because it is very limiting with regard to the type of mathematical reasoning one is allowed to use.I have briefly described the three main streams of present-day mathematical philosophy: formalism, Platonism, and intuitionism. I have made no secret of the fact that my sympathies lie strongly with the Platonistic view that mathematical truth is absolute, external, and eternal, and not based on man-made criteria; and that mathematical objects have a timeless existence of their own, not dependent on human society nor on particular physical objects. I have tried to make my case for this view in this section, in the previous section, and at the end of Chapter 3. I hope that the reader is prepared to go most of the way with me on this. It will be important for a good deal of what we shall encounter later.GÖDEL-TYPE THEOREMS FROM TURING’S RESULTIn my presentation of Gödel’s theorem, I have omitted many details, and I have also left out what was historically perhaps the most important part of his argument; that referring to the ‘un-decidability’ of the consistency of the axioms. My purpose here has not been to emphasize this ‘axiom-consistency-provability problem’, so important to Hilbert and his contemporaries, but to show that a specific Gödel proposition – neither provable nor disprovable using the axioms and rules of the formal system under consideration – is clearly seen, using our insights into the meanings of the operations in question, to be a true proposition!I have mentioned that Turing developed his own later argument establishing the insolubility of the halting problem after studying the work of Gödel. The two arguments had a good deal in common and, indeed, key aspects of Gödel’s result can be directly derived using Turing’s procedure. Let us see how this works, and thereby obtain a somewhat different insight into what lies behind Gödel’s theorem.An essential property of a formal mathematical system is that it should be a computable matter to decide whether or not a given string of symbols constitutes a proof, within the system, of a given mathematical assertion. The whole point of formalizing the notion of mathematical proof, after all, is that no further judgements have to be made about what is valid reasoning and what is not. It must be possible to check in a completely mechanical and previously determined way whether or not a putative proof is indeed a proof; that is, there must be an algorithm for checking proofs. On the other hand, we do not demand that it should necessarily be an algorithmic matter to find proofs (or disproofs) of suggested mathematical statements.In fact, it turns out that there always is an algorithm for finding a proof within any formal system whenever some proof exists. For we must suppose that our system is formulated in terms of some symbolic language, this language being expressible in terms of some finite ‘alphabet’ of symbols. As before, let us order our strings of symbols lexicographically, which we recall means alphabetically for each fixed length of string, taking all strings of length one to be ordered first, those of length two next, then length three, and so on (p. 138). Thus we have all the correctly constructed proofs numerically ordered according to this lexicographical scheme. Having our list of proofs, we also have a list of all the theorems of the formal system. For the theorems are precisely the propositions that appear as last lines of correctly constructed proofs. The listing is perfectly computable: for we can consider the lexicographical list of all strings of symbols of the system, whether or not they make sense as proofs, and then test the first string with our proof-testing algorithm to see if it is a proof and discard it if it is not; then we test the second in the same way and discard it if it is not a proof, and then the third, then the fourth, and so on. By this means, if there is a proof, we shall eventually find it somewhere along the list.Thus if Hilbert had been successful in finding his mathematical system – a system of axioms and rules of procedure strong enough to enable one to decide, by formal proof, the truth or falsity of any mathematical proposition correctly formulated within the system – then there would be a general algorithmic method of deciding the truth of any such proposition. Why is this? Because if, by the procedure outlined above, we eventually come across the proposition we are looking for as the final line in the proof, then we have proved this proposition. If, instead, we eventually come across the negation of our proposition as the final line, then we have disproved it. If Hilbert’s scheme were complete, either one or the other of these eventualities would always occur (and, if consistent, they would never both occur together). Thus our mechanical procedure would always terminate at some stage and we should have a universal algorithm for deciding the truth or otherwise of all propositions of the system. This would contradict Turing’s result, as presented in Chapter 2, that there is no general algorithm for deciding mathematical propositions. Consequently we have, in effect, proved Gödel’s theorem that no scheme of Hilbert’s intended type can be complete in the sense that we have been discussing.In fact Gödel’s theorem is more specific than this, since the type of formal system that Gödel was concerned with was required to be adequate only for propositions of arithmetic, not for propositions of mathematics in general. Can we arrange that all the necessary operations of Turing machines can be carried out using just arithmetic? To put this another way, can all the computable functions of natural numbers (i.e. recursive, or algorithmic functions – the results of Turing machine action) be expressed in terms of ordinary arithmetic? In fact it is almost true that we can, but not quite. We need one extra operation to be adjoined to the standard rules of arithmetic and logic (including Ǝ and ∀). This operation simply selects‘the smallest natural number x such that K(x) is true’,where K( ) is any given arithmetically calculable propositional function – for which it is assumed that there is such a number, i.e. that Ǝx[K(x)] is true. (If there were no such number, then our operation would ‘run on forever’* trying to locate the required non-existent x.) In any case, the foregoing argument does establish, on the basis of Turing’s result, that Hilbert’s programme of reducing whole branches of mathematics to calculations within some formal system is indeed untenable.As it stands, this procedure does not so readily show that we have a Gödel proposition (like Pk(k)) that is true, but not provable within the system. However, if we recall the argument given in Chapter 2 on ‘how to outdo an algorithm’ (cf. p. 83), we shall see that we can do something very similar. In that argument we were able to show that, given any algorithm for deciding whether a Turing machine action stops, we can produce a Turing machine action that we see does not stop, yet the algorithm cannot. (Recall that we insisted that the algorithm must correctly inform us when a Turing machine action will stop though it can sometimes fail to tell us that the Turing machine action will not stop – by running on forever itself.) Thus, as with the situation with Gödel’s theorem above, we have a proposition that we can see, by the use of insight, must actually be true (not-stopping of the Turing machine action), but the given algorithmic action is not capable of telling us this.RECURSIVELY ENUMERABLE SETSThere is a way of describing the basic ingredients of Turing’s and Gödel’s results in a graphic way, in terms of the language of set theory. This enables us to get away from arbitrary descriptions in terms of specific symbolisms or formal systems, so that the essential issues are brought out. We shall consider only sets (finite or infinite) of the natural numbers 0, 1, 2, 3, 4,. . ., so we shall be examining collections of these, such as {4, 5, 8}, {0, 57, 100003}, {6}, {0}, {1, 2, 3, 4,. . ., 9999}, {1, 2, 3, 4,. . .}, {0, 2, 4, 6, 8,. . .}, or even the entire set ∅ = {0, 1, 2, 3, 4,. . .} or the empty set ∅ = {}. We shall be concerned only with computability questions, namely: ‘Which kinds of sets of natural numbers can be generated by algorithms and which cannot?’In order to address such issues we may, if we like, think of each single natural number n as denoting a specific string of symbols in a particular formal system. This would be the ‘nth’ string of symbols, say Qn, according to some lexicographical ordering of (the ‘syntactically correctly’ expressed) propositions in the system. Then each natural number represents a proposition. The set of all propositions of the formal system would be represented as the entire set ℕ and, for example, the theorems of the formal system would be thought of as constituting some smaller set of natural numbers, say the set P. However, the details of any particular numbering system for propositions are not important. All that we should need, in order to set up a correspondence between natural numbers and propositions, would be a known algorithm for obtaining each proposition Qn (written in the appropriate symbolic notation) from its corresponding natural number n, and another known algorithm for obtaining n from Qn. Taking these two known algorithms as given, we are at liberty to identify the set of natural numbers ℕ with the set of propositions of a specific formal system.Let us choose a formal system which is consistent and broad enough to include all actions of all Turing machines – and, moreover, ‘sensible’ in the sense that its axioms and rules of procedure are things that may be taken to be ‘self-evidently true’ Now, some of the propositions Q0, Q1, Q2, Q3,, . . . of the formal system will actually have proofs within the system. These ‘provable’ propositions will have numbers which constitute some set in ℕ in fact the set P of ‘theorems’ considered above. We have, in effect, already seen that there is an algorithm for generating, one after the other, all the propositions with proofs in some given formal system. (As outlined earlier, the ‘nth proof’ Πn is obtained algorithmically from n. All we have to do is look at the last line of the nth proof to find the ‘nth proposition provable within the system’, i.e. the nth ‘theorem’.) Thus we have an algorithm for generating the elements of P one after the other (perhaps with repetitions – that makes no difference).A set, such as P, which can be generated in this way by some algorithm, is called recursively enumerable. Note that the set of propositions which are disprovable within the system – i.e. propositions whose negations are provable – is likewise recursively enumerable, for we can simply enumerate the provable propositions, taking their negations as we go along. There are many other subsets of ℕ which are recursively enumerable, and we need make no reference to our formal system in order to define them. Simple examples of recursively enumerable sets are the set of even numbers{0, 2, 4, 6, 8,. . .},the set of squares{0, 1, 4, 9, 16,. . .},and the set of primes{2, 3, 5, 7, 11,. . .}.Clearly we can generate each of these sets by means of an algorithm. In each of these three examples it will also be the case that the complement of the set – that is, the set of natural numbers not in the set – is recursively enumerable. The complementary sets in these three cases are, respectively:{1, 3, 5, 7, 9,. . .};{2, 3, 5, 6, 7, 8, 10,. . .};and{0, 1, 4, 6, 8, 9, 10, 12,. . .}.It would be a simple matter to provide an algorithm for these complementary sets also. Indeed, we can algorithmically decide, for any given natural number n, whether or not it is an even number, or whether or not it is a square, or whether or not it is a prime. This provides us with an algorithm for generating both the set and the complementary set, for we can run through the natural numbers in turn and decide, in each case, whether it belongs to the original set or to the complementary set. A set which has the property that both it and its complementary set are recursively enumerable is called a recursive set. Clearly the complement of a recursive set is also a recursive set.Now, are there any sets which are recursively enumerable but not recursive? Let us pause, for a moment, to note what that would entail. Since the elements of such a set can be generated by an algorithm, we shall have a means of deciding, for an element suspected of being in the set – and which, let us suppose for the moment, is actually in the set – that, indeed, it is in the set. All we need do is to allow our algorithm to run over all the elements of the set until it eventually finds the particular element that we are examining. But suppose our suspected element is actually not in the set. In that case our algorithm will do us no good at all, for it will just keep running forever, never coming to a decision. For that we need an algorithm for generating the complementary set. If that finds our suspect then we know for sure that the element is not in the set. With both algorithms we should be in good shape. We could simply alternate between the two algorithms and catch the suspect either way. That happy situation, however, is what happens with a recursive set. Here our set is supposed to be merely recursively enumerable but not recursive: our suggested algorithm for generating the complementary set does not exist! Thus we are presented with the curious situation that we can algorithmically decide, for an element in the set, that it is actually in the set; but we cannot guarantee, by means of any algorithm, to decide this question for elements that happen not to be in the set!Does such a curious situation ever arise? Are there, indeed, any recursively enumerable sets that are not recursive? Well, what about the set P? Is that a recursive set? We know that it is recursively enumerable, so what we have to decide is whether the complementary set is also recursively enumerable. In fact it is not! How can we tell that? Well, recall that the actions of Turing machines are supposed to be among the operations allowed within our formal system. We denote the nth Turing machine by Tn. Then the statement‘Tn(n) stops’is a proposition – let us write it S(n) – that we can express in our formal system, for each natural number n. The proposition S(n) will be true for some values of n, and false for others. The set of all S(n), as n runs through the natural numbers 0, 1, 2, 3,. . ., will be represented as some subset S of ℕ. Now, recall Turing’s fundamental result (Chapter 2, p. 78) that there is no algorithm that asserts ‘Tn(n) does not stop’ in precisely these cases in which Tn(n) in fact does not stop. This shows that the set of false S(n) is not recursively enumerable.We observe that the part of S that lies in P consists precisely of those S(n) which are true. Why is this? Certainly if any particular S(n) is provable, then it must be true (because our formal system has been chosen to be ‘sensible’!), so the part of S that lies in P must consist solely of true propositions S(n). Moreover, no true proposition S(n) can lie outside P, for if Tn(n) stops then we can provide a proof within the system that it actually does so.*Now, suppose that the complement of P were recursively enumerable. Then we should have some algorithm for generating the elements of this complementary set. We can run this algorithm and note down every proposition S(n) that we come across. These are all the false S(n), so our procedure would actually give us a recursive enumeration of the set of false S(n). But we noted above that the false S(n) are not recursively enumerable. This contradiction establishes that the complement of P cannot be recursively enumerated after all; so the set P is not recursive, which is what we have sought to establish.These properties actually demonstrate that our formal system cannot be complete: i.e. there must be propositions which are neither provable nor disprovable within the system. For if there were no such ‘undecidable’ propositions, the complement of the set P would have to be the set of disprovable propositions (anything not provable would have to be disprovable). But we have seen that the disprovable propositions constitute a recursively enumerable set, so this would make P recursive. However, P is not recursive – a contradiction which establishes the required incompleteness. This is the main thrust of Gödel’s theorem.Now what about the subset T of ℕ which represents the true propositions of our formal system? Is T recursive? Is T recursively enumerable? Is the complement of T recursively enumerable? In fact the answer to all these questions is ‘No’. One way of seeing this is to note that the false propositions of the form‘Tn(n) stops’cannot be generated by an algorithm, as we have noted above. Therefore the false propositions as a whole cannot be generated by an algorithm, since any such algorithm would, in particular, enumerate all the above false ‘Tn(n) stops’ propositions. Similarly, the set of all true propositions cannot be generated by an algorithm (since any such algorithm could be trivially modified to yield all the false propositions, simply by making it take the negation of each proposition that it generates). Since the true propositions are thus not recursively enumerable (and nor are the false ones) they constitute a vastly more complicated and profound array than do the propositions provable within the system. This again illustrates aspects of Gödel’s theorem: that the concept of mathematical truth is only partly accessible by the means of formal argument.There are certain simple classes of true arithmetical proposition that do form recursively enumerable sets, however. For example, true propositions of the formƎ w, x. . ., z[f(w, x,. . ., z) = 0],where f( ) is some function constructed from the ordinary arithmetical operations of adding, subtracting, multiplying, and raising to a power, constitute a recursively enumerable set (which I shall label A), as is not too hard to see.8 An instance of a proposition of this form – although we do not know if it is true–is the negation of ‘Fermat’s last theorem’, for which we can take f( ) to be given byf(w, x, y, z) = (x + 1)w + 3 + (y + 1)w + 3 – (z + 1)w + 3.However, the set A turns out not to be recursive (a fact which is not so easy to see – though it is a consequence of Gödel’s actual original argument). Thus we are not presented with any algorithmic means which could, even in principle, decide the truth or otherwise of ‘Fermat’s last theorem’!In Fig. 4.1, I have tried schematically to represent a recursive set as a region with a nice simple boundary, so that one can imagine it being a direct matter to tell whether or not some given point belongs to the set. Each point in the picture is to be thought of as representing a natural number. The complementary set is then also represented as a simple-looking region. In Fig. 4.2, I have tried to represent a recursively enumerable but non-recursive set as a set with a complicated boundary, where the set on one side of the boundary – the recursively enumerable side – is supposed to look simpler than that on the other. The figures are very schematic, and are not intended to be in any sense ‘geometrically accurate’. In particular, there is no significance in the fact that these pictures have been represented as though on a flat two-dimensional plane! In Fig. 4.3 I have schematically indicated how the regions P, T, and A lie within the set ℕ.Fig. 4.1. Highly schematic representation of a recursive set.Fig. 4.2. Highly schematic representation of a recursively enumerable set (black region) which is not recursive. The idea is that the white region is defined only as ‘what’s left’ when the computably generated black region is removed; and it is not a computable matter to ascertain that a point is actually in the white region itself.IS THE MANDELBROT SET RECURSIVE?Non-recursive sets must have the property that they are complicated in a very essential way. Their complication must, in some sense, defy all attempts at systematization, otherwise that very systematization would lead to some appropriate algorithmic procedure. For a non-recursive set, there is no general algorithmic way of deciding whether or not an element (or ‘point’) belongs to the set. Now we witnessed, at the beginning of Chapter 3, a certain extraordinarily complicated-looking set, namely the Mandelbrot set. Although the rules which provide its definition are surprisingly simple, the set itself exhibits an endless variety of highly elaborate structure. Could this be an example of a non-recursive set, truly exhibited before our mortal eyes?The reader will not be slow to point out, however, that this paradigm of complication has been conjured up, for our eyes to see, by the magic of modern high-speed electronic computer technology. Are not electronic computers the very embodiments of algorithmic action? Indeed, that is certainly true, but we must bear in mind the way that the computer actually produces these pictures. To test whether a point of the Argand plane – a complex number c – belongs to the Mandelbrot set (coloured black) or to the complementary set (coloured white), the computer would start with 0, then apply the mapFig. 4.3. Highly schematic representation of various sets of propositions. The set P of propositions that are provable within the system is, like A, recursively enumerable but not recursive; the set T of true propositions is not even recursively enumerable.z → z2 + cto z = 0 to obtain c, and then to z = c to obtain c2 + c and then to z = c2 + c to obtain c4 + 2c3 + c2 + c, and so on. If this sequence 0, c, c2 + c, c4 + 2c3 + c2 + c,. . . remains bounded, then the point represented by c is coloured black; otherwise it is coloured white. How does the machine tell whether or not such a sequence remains bounded? In principle, the question involves knowing what happens after an infinite number of terms of the sequence! That, by itself, is not a computable matter. Fortunately, there are ways of telling after only a finite number of terms when the sequence has become unbounded. (In fact, as soon as it reaches the circle of radius 1 + √2 centred at the origin, then one can be sure that the sequence is unbounded.)Thus, in a certain sense, the complement of the Mandelbrot set (i.e. the white region) is recursively enumerable. If the complex number c is in the white region, then there is an algorithm for ascertaining that fact. What about the Mandelbrot set itself – the black region? Is there an algorithm for telling for sure that a point suspected to be in the black region is in fact in the black region? The answer to this question seems to be unknown, at present.9 I have consulted various colleagues and experts, and none seems to be aware of such an algorithm. Nor had they come across any demonstration that no such algorithm exists. At least, there appears to be no known algorithm for the black region. Perhaps the complement of the Mandelbrot set is, indeed, an example of a recursively enumerable set which is not recursive!Before exploring this suggestion further, it will be necessary to address certain issues that I have glossed over. These issues will have some importance to us in our later discussions of computability in physics. I have actually been somewhat inexact in the preceding discussion. I have applied such terms as ‘recursively enumerable’ and ‘recursive’ to sets of points in the Argand plane, i.e. to sets of complex numbers. These terms should strictly only be used for the natural numbers or for other countable sets. We have seen in Chapter 3 (p. 110) that the real numbers are not countable, and so the complex numbers cannot be countable either – since real numbers can be regarded as particular kinds of complex numbers, namely complex numbers with vanishing imaginary parts (cf. p. 115). In fact there are precisely ‘as many’ complex numbers as real ones, namely ‘C’ of them. (To establish a one-to-one relation between the complex numbers and real numbers, one can, roughly speaking, take the decimal expansions of the real and imaginary parts of each complex number and interleave them according to the odd and even digits of the corresponding real number: e.g. the complex number 3.6781. . . + i512.975 . . . would correspond to the real number 50132.6977851. . .)One way to evade this problem would be to refer only to computable complex numbers, for we saw in Chapter 3 that the computable real numbers – and therefore also the computable complex numbers – are indeed countable. However, there is a severe difficulty with this: there is in fact no general algorithm for deciding whether two computable numbers, given in terms of their respective algorithms, are equal to each other or not! (We can algorithmically form their difference, but we cannot algorith-mically decide whether or not this difference is zero. Imagine two algorithms generating the digits 0.99999 . . . and 1.00000 . . ., respectively, but we might never know if the 9s, or the 0s, are going to continue indefinitely, so that the two numbers are equal, or whether some other digit will eventually appear, and the numbers are unequal.) Thus, we might never know whether these numbers are equal. One implication of this is that even with such a simple set as the unit disc in the Argand plane (the set of points whose distance from the origin is not greater than one unit, i.e. the black region in Fig. 4.4) there would be no algorithm for deciding for sure whether a complex number actually lies on the disc. The problem does not arise with the points in the interior (or with points outside the disc) but with points which lie on the very edge of the disc – i.e. on the unit circle itself. The unit circle is considered to be part of the disc. Suppose that we are simply given an algorithm which generates the digits of the real and imaginary part of some complex number. If we suspect that this complex number actually lies on the unit circle, we cannot necessarily ascertain this fact. There is no algorithm for deciding whether the computable numberFig. 4.4. The unit disc should surely count as ‘recursive’, but this requires an appropriate viewpoint.x2 + y2is actually equal to 1 or not, this being the criterion for deciding whether or not the computable complex number x + iy lies on the unit circle.Clearly this is not what we want. The unit disc certainly ought to count as recursive. There are not many sets simpler than the unit disc! One way around this problem might be to ignore the boundary. For points actually in the interior or actually in the exterior, an algorithm for ascertaining these facts certainly exists. (Simply generate the digits of x2 + y2 one after the other, and eventually we find a digit other than 9 after the decimal point in 0.99999 . . . or other than 0 in 1.00000 . . .) In this sense the unit disc is recursive. But the point of view is rather an awkward one for mathematics since one often needs to phrase arguments in terms of what actually happens at boundaries. It is possible that such a point of view might be appropriate for physics, on the other hand. We shall need to reconsider this kind of issue again later.There is another closely related point of view that one might adopt and this is not to refer to computable complex numbers at all. Instead of trying to enumerate the complex numbers inside or outside the set in question, we simply ask for an algorithm which decides, given the complex number, whether it lies in the set or whether it lies in the complement of the set. By ‘given’, I mean that, for each complex number that we are testing, successive digits of the real and imaginary parts are presented to use one after the other, for as long as we please – perhaps by some magical means. I do not require that there be any algorithm, known or unknown, for presenting these digits. A set of complex numbers would be considered to be ‘recursively enumerable’ if a single algorithm exists such that whenever it is presented with such a succession of digits in this way it would eventually say ‘yes’, after a finite number of steps, if and only if the complex number actually lies in that set. As with the first point of view suggested above, it turns out that this point of view ‘ignores’ boundaries. Thus the interior of the unit disc and the exterior of the unit disc would each count as recursively enumerable in this sense, whereas the boundary itself would not.It is not altogether clear to me that either of these viewpoints is really what is needed.10 When applied to the Mandelbrot set, the philosophy of ‘ignoring the boundary’ may miss a lot of the complication of the set. This set consists partly of ‘blobs’ – regions with interiors – and partly of ‘tendrils’. The most extreme complication seems to lie in the tendrils, which can meander most wildly. However the tendrils do not lie in the interior of the set, and so they would be ‘ignored’ if we adopt either one of these two philosophies. Even so, it is still not clear whether the Mandelbrot set is ‘recursive’, where the blobs only are considered. The question seems to rest on a certain unproved conjecture concerning the Mandelbrot set: is it what is called ‘locally connected’? I do not propose to explain the meaning or relevance of that term here. I merely wish to indicate that these are difficult issues, and they raise questions concerning the Mandelbrot set which are still not resolved, and some of which lie at the forefront of some current mathematical research.There are also other points of view that one can adopt, in order to get around the problem that the complex numbers are not countable. Rather than consider all computable complex numbers, one may consider a suitable subset of such numbers having the property that it is a computable matter to decide whether or not two of them are equal. A simple such subset would be the ‘rational’ complex numbers for which the real and imaginary parts of the numbers are both taken to be rational numbers. I do not think that this would give much in the way of tendrils on the Mandelbrot set, however, this point of view being very restrictive. Somewhat more satisfactory would be to consider the algebraic numbers – those complex numbers which are solutions of algebraic equations with integer coefficients. For example, all the solutions for z of129z7 – 33z5 + 725z4 + 16z3 – 2z – 3 = 0are algebraic numbers. Algebraic numbers are countable and computable, and it is actually a computable matter to decide whether or not two of them are equal. (It turns out that many of them lie on the boundary of the unit circle and on the tendrils of the Mandelbrot set.) We can, if desired, phrase the question as to whether or not the Mandelbrot set is recursive in terms of them.It may be that algebraic numbers would be appropriate in the case of the two sets just considered, but they do not really resolve all our difficulties in general. For consider the set (the black region of Fig. 4.5) defined by the relationy ≥ exfor x + iy (= z) in the Argand plane. The interior of the set and the interior of the complement of the set are both recursively enumerable according to any of the viewpoints expressed above, but (as follows from a famous theorem due to F. Lindemann, proved in 1882) the boundary, y = ex, contains only one algebraic point, namely the point z = i. The algebraic numbers are no help to us in exploring the algorithmic nature of the boundary in this case! It would not be hard to find another subclass of computable numbers which would suffice in this particular case, but one is left with the strong feeling that the correct viewpoint has not yet been arrived at.Fig. 4.5. The set defined by the exponential relation y ≥ ex should also count as ‘recursive’.SOME EXAMPLES OF NON-RECURSIVE MATHEMATICSThere are very many areas of mathematics where problems arise which are not recursive. Thus, we may be presented with a class of problems to which the answer in each case is either ‘yes’ or ‘no’, but for which no general algorithm exists for deciding which of these two is actually the case. Some of these classes of problem are remarkably simple-looking.First, consider the problem of finding integer solutions of systems of algebraic equations with, integer coefficients. Such equations are known as Diophantine equations (after the Greek mathematician Diophantos, who lived in the third century BC and who studied equations of this type). Such a set of equations might bez3 – y – 1 = 0, yz2 – 2x – 2 = 0, y2 – 2xz + z + 1 = 0and the problem is to decide whether or not they can be solved for integer values of x, y and z. In fact, in this particular case they can, a solution being given byx = 13, y = 7, z = 2.However, there is no algorithm for deciding this question for an arbitrary set* of Diophantine equations: Diophantine arithmetic, despite the elementary nature of its ingredients, is part of non-algorithmic mathematics!(A somewhat less elementary example is the topological equivalence of manifolds. I mention this only briefly, because it has some conceivable relevance to the issues discussed in Chapter 8. To understand what a ‘manifold’ is consider first a loop of string, which is a manifold of just one dimension, and then consider a closed surface, a manifold of two dimensions. Next try to imagine a kind of ‘surface’ which can have three or a higher number of dimensions. ‘Topological equivalence’ of two manifolds means that one of them can be deformed into the other by a continuous motion – without tearing or gluing. Thus a spherical surface and the surface of a cube are topologically equivalent, whereas they are both topologically in equivalent to the surface of a ring or a teacup – the latter two being actually topologically equivalent to each other. Now, for two-dimensional manifolds there is an algorithm for deciding whether or not two of them are topologically equivalent – amounting, in effect, to counting the number of ‘handles’ that each surface has. For three dimensions, the answer to the question is not known, at the time of writing, but for four or more dimensions, there is no algorithm for deciding equivalence. The four-dimensional case is conceivably of some relevance to physics, since according to Einstein’s general relativity, space and time together constitute a 4-manifold; see Chapter 5, p. 268. It has been suggested by Geroch and Hartle 1986 that this non-algorithmic property might have relevance to ‘quantum gravity’; cf. also Chapter 8.)Let us consider a different type of problem, called the word problem.11 Suppose that we have some alphabet of symbols, and we consider various strings of these symbols, referred to as words. The words need not in themselves have any meaning, but we shall be given a certain (finite) list of ‘equalities’ between them which we are allowed to use in order to derive further such ‘equalities’. This is done by making substitutions of words from the initial list into other (normally longer) words which contain them as portions. Each such portion may be replaced by another portion which is deemed to be equal to it according to the list. The problem is then to decide, for some given pair of words, whether or not they are ‘equal’ according to these rules.As an example, we might have, for our initial list:EAT = ATATE = ALATER = LOWPAN = PILLOWCARP = ME.From these we can derive, for example,LAP = LEAPby use of successive substitutions from the second, the first, and again the second of the relations from the initial list:LAP = LATEP = LEATEP = LEAP.The problem now is, given some pair of words, can we get from one to the other simply using such substitutions? Can we, for example, get from CATERPILLAR to MAN, or, say, from CARPET to MEAT? The answer in the first case happens to be ‘yes’, while in the second it is ‘no’. When the answer is ‘yes’, the normal way to show this would be simply to exhibit a string of equalities where each word is obtained from the preceding one by use of an allowed relation. Thus (indicating the letters about to be changed in bold type, and the letters which have just been changed in italics):CATERPILLAR = CARPILLAR = CARPILLATER = CARPILLOW = CARPAN = MEAN = MEATEN = MATEN = MAN.How can we tell that it is impossible to get from CARPET to MEAT by means of the allowed rules? For this, we need to think a little more, but it is not hard to see, in a variety of different ways. The simplest appears to be the following: in every ‘equality’ in our initial list, the number of As plus the number of Ws plus the number of Ms is the same on each side. Thus the total number of As, Ws, and Ms cannot change throughout any succession of allowed substitutions. However, for CARPET this number is 1 whereas for MEAT it is 2. Consequently, there is no way of getting from CARPET to MEAT by allowed substitutions.Notice that when the two words are ‘equal’ we can show this simply by exhibiting an allowed formal string of symbols, using the rules that we had been given; whereas in the case where they are ‘unequal’, we had to resort to arguments about the rules that we had been given. There is a clear algorithm that we can use to establish ‘equality’ between words whenever the words are in fact ‘equal’. All we need do is to make a lexicographical listing of all the possible sequences of words, and then strike from this list any such string for which there is a pair of consecutive words where the second does not follow from the first by an allowed rule. The remaining sequences will provide all the sought-for ‘equalities’ between words. However, there is no such obvious algorithm, in general, for deciding when two words are not ‘equal’, and we may have to resort to ‘intelligence’ in order to establish that fact. (Indeed, it took me some while before I noticed the above ‘trick’ for establishing that CARPET and MEAT are not ‘equal’. With another example, quite a different kind of ‘trick’ might be needed. Intelligence, incidentally, is useful – although not necessary – also for establishing the existence of an ‘equality’.)In fact, for the particular list of five ‘equalities’ that constitute the initial list in the above case, it is not unduly difficult to provide an algorithm for ascertaining that two words are ‘unequal’ when they are indeed ‘unequal’. However, in order to find the algorithm that works in this case we need to exercise a certain amount of intelligence! Indeed, it turns out that there is no single algorithm which can be used universally for all possible choices of initial list. In this sense there is no algorithmic solution to the word problem. The general word problem belongs to non-recursive mathematics!There are even certain particular selections of initial list for which there is no algorithm for deciding when two words are unequal. One such is given by(This list is adapted from one given in 1955 by G. S. Tseitin and Dana Scott; see Gardner 1958, p. 144.) Thus this particular word problem by itself is an example of non-recursive mathematics, in the sense that using this particular initial list we cannot algorithmically decide whether or not two given words are ‘equal’.Fig. 4.6. Two examples of periodic tilings of the plane, each using a single shape (found by Marjorie Rice in 1976).The general word problem arose from considerations of formalized mathematical logic (‘formal systems’ etc., as we considered earlier). The initial list plays the role of an axiom system and the substitution rule for words, the role of the formal rules of procedure. The proof of non-recursiveness for the word problem arises from such considerations.As a final example of a problem in mathematics which is non-recursive, let us consider the question of covering the Euclidean plane with polygonal shapes, where we are given a finite number of different such shapes and we ask whether it is possible to cover the plane completely, without gaps or overlaps, using just these shapes and no others. Such an arrangement of shapes is called a tiling of the plane. We are familiar with the fact that such tilings are possible using just squares, or just equilateral triangles, or just regular hexagons (as illustrated in Fig. 10.2, on p. 563), but not using just regular pentagons. Many other single shapes will tile the plane, such as each of the two irregular pentagons illustrated in Fig. 4.6. With a pair of shapes, the tilings can become more elaborate. Two simple examples are given in Fig. 4.7. All these examples, so far, have the property that they are periodic; which means that they are exactly repetitive in two independent directions. In mathematical terms, we say that there is a period parallelogram – a parallelogram which, when marked in some way and then repeated again and again in the two directions parallel to its sides, will reproduce the given tiling pattern. An example is shown in Fig. 4.8, where a periodic tiling with a thorn-shaped tile is depicted on the left, and related to a period parallelogram whose periodic tiling is indicated on the right.Fig. 4.7. Two examples of periodic tilings of the plane, each using two shapes.Fig. 4.8. A periodic tiling, illustrated in relation to its period parallelogram.Now there are many tilings of the plane which are not periodic. Figure 4.9 depicts three non-periodic ‘spiral’ tilings, with the same thorn-shaped tile as in Fig. 4.8. This particular tile shape is known as a ‘versatile’ (for obvious reasons!), and it was devised by B. Grünbaum and G. C. Shephard (1981, 1987), apparently based on an earlier shape due to H. Voderberg. Note that the versatile will tile both periodically and non-periodically. This property is shared by many other single tile shapes and sets of tile shapes. Are there single tiles or sets of tiles which will tile the plane only non-periodically? The answer to this question is ‘yes’. In Fig. 4.10, I have depicted a set of six tiles constructed by the American mathematician Raphael Robinson (1971) which will tile the entire plane, but only in a non-periodic way.It is worthwhile to go into a little of the history of how this non-periodic set of tiles came about (cf. Grünbaum and Shephard 1987). In 1961 the Chinese-American logician Hao Wang addressed the question of whether or not there is a decision procedure for the tiling problem, that is to say, is there an algorithm for deciding whether or not a given finite set of different polygonal shapes will tile the entire plane!* He was able to show that there indeed would be such a decision procedure if it could be shown that every finite set of distinct tiles which will in some way tile the plane, will in fact also tile the plane periodically. I think that it was probably felt, at that time, that it would be unlikely that a set violating this condition – i.e. an ‘aperiodic’ set of tiles – could exist. However, in 1966, following some of the leads that Hao Wang had suggested, Robert Berger was able to show that there is in fact no decision procedure for the tiling problem: the tiling problem is also part of non-recursive mathematics!12Fig. 4.9. Three non-periodic ‘spiral’ tilings, using the same ‘versatile’ shape that was used in Fig. 4.8.Thus it follows from Hao Wang’s earlier result that an aperiodic set of tiles must exist, and Berger was indeed able to exhibit the first aperiodic set of tiles. However, owing to the complication of this line of argument, his set involved an inordinately large number of different tiles – originally 20 426. By the use of some additional ingenuity, Berger was able to reduce his number to 104. Then in 1971, Raphael Robinson was able to get the number down to the six depicted in Fig. 4.10.Another aperiodic set of six tiles is depicted in Fig. 4.11. This set I produced myself in about 1973 following a quite independent line of thought. (I shall return to this matter in Chapter 10 where an array tiled with these shapes is depicted in Fig. 10.3, p. 563.) After Robinson’s aperiodic set of six was brought to my attention, I began thinking about reducing the number; and by various operations of slicing and re-gluing, I was able to reduce it to two. Two alternative schemes are depicted in Fig. 4.12. The necessarily non-periodic patterns exhibited by the completed tilings have many remarkable properties, including a seemingly crystallographically impossible quasi-periodic structure with fivefold symmetry. I shall return to these matters later.Fig. 4.10. Raphael Robinson’s six tiles which tile the plane only non-periodically.Fig. 4.11. Another set of six tiles which tile the plane only non-periodically.It is perhaps remarkable that such an apparently ‘trivial’ area of mathematics – namely covering the plane with congruent shapes – which seems almost like ‘child’s play’ should in fact be part of non-recursive mathematics. In fact there are many difficult and unsolved problems in this area. It is not known, for example, if there is an aperiodic set consisting of a single tile.The tiling problem, as treated by Wang, Berger, and Robinson, used tiles based on squares. I am here allowing polygons of general shape, and one needs some adequately computable way of displaying the individual tiles. One way of doing this would be to give their vertices as points in the Argand plane, and these points may perfectly adequately be given as algebraic numbers.IS THE MANDELBROT SET LIKE NON-RECURSIVE MATHEMATICS?Let us now return to our earlier discussion of the Mandelbrot set. I am going to assume, for the purposes of an illustration, that the Mandelbrot set is, in some appropriate sense, non-recursive. Since its complement is recursively enumerable, this would mean that the set itself would not be recursively enumerable. I think that it is likely that the form of the Mandelbrot set has some lessons to teach us as to the nature of non-recursive sets and non-recursive mathematics.Fig. 4.12. Two pairs, each of which will tile only non-periodically (‘Penrose tiles’); and regions of the plane tiled with each pair.Let us return to Fig. 3.2, which we encountered early in Chapter 3. Notice that most of the set seems to be taken up with a large heart-shaped region, which I have labelled A, in Fig. 4.13. The shape is referred to as a cardioid and its interior region can be defined mathematically as the set of points c of the Argand plane which arise in the formc = z – z2,where z is a complex number whose distance from the origin is less than 1/2. This set is certainly recursively enumerable in the sense suggested earlier: an algorithm exists which, when applied to a point of the interior of the region, will ascertain that the point is indeed in that interior region. The actual algorithm is easily obtained from the above formula.Now consider the disc-like region just to the left of the main cardioid (region B in Fig. 4.13). Its interior is the set of pointsc = z – 1where z has distance from the origin less than 1/4. This region is indeed the interior of a disc – the set of points inside an exact circle. Again this region is recursively enumerable in the above sense. What about the other ‘warts’ on the cardioid? Consider the two next largest warts. These are the roughly circular blobs which appear approximately at the top and bottom of the cardioid in Fig. 3.2 and are marked C1, C2 in Fig. 4.13. They can be given in terms of the setFig. 4.13. The major parts of the interior of the Mandelbrot set can be defined by simple algorithmic equations.c3 + 2c2 + (1 – z)c + (1 – z)2 = 0,where now z ranges over the region which has distance 1/8 from the origin. In fact this equation provides us with not merely these two blobs (together) but also with the ‘baby’ cardioid-like shape which appears off to the left in Fig. 3.2 – the main region of Fig. 3.1 – and is the region marked C3 in Fig. 4.13. Again, these regions (together or separately) constitute recursively enumerable sets (in the sense suggested earlier) by virtue of the existence of the above formula.Despite the suggestion that I have been making that the Mandelbrot set may be non-recursive, we have been able to clean out the largest areas of the set already with some perfectly well-defined and not-too-complicated algorithms. It seems that this process should continue. All the most evident regions in the set – and certainly the overwhelming percentage of its area (if not all of it) – can be dealt with algorithmically. If, as I am supposing, the complete set is actually not recursive, then the regions that cannot be reached by our algorithms must be very delicate and hard to find. Moreover, when we have located such a region, the chances are that we can then see how to improve our algorithms so that those particular regions can also be reached. Yet then there would be other such regions (if my supposition of non-recursiveness is correct), hiding yet more deeply in the obscurities of subtlety and complication, that even our improved algorithm cannot reach. Again, by prodigious efforts of insight, ingenuity, and industry we might be able to locate such a region; but there would be yet others that would still escape, and so on.I think that this is not unlike the way that mathematics often proceeds in areas where the problems are difficult, and presumably non-recursive. The most common problems that one is likely to come across in some specific area can often be handled by simple algorithmic procedures – procedures which may have been known for centuries. But some will escape the net, and more sophisticated procedures are needed to handle them. The ones that still escape would, of course, particularly intrigue the mathematicians and would goad them on to develop ever more powerful methods. These would need to be based upon deeper and deeper insights into the nature of the mathematics involved. Perhaps there is something of this in our understanding of the physical world.In the word problems and tiling problems considered above, one can begin to catch a glimpse of this kind of thing (although these are not areas where the mathematical machinery has yet developed very far). We were able to use a very simple argument in one particular case to show that a certain word cannot be obtained from another by the allowed rules. It is not hard to imagine that much more sophisticated lines of reasoning can be brought into play to deal with more awkward cases. The likelihood would then be that these new lines of reasoning can be developed into an algorithmic procedure. We know that no one procedure can suffice for all instances of the word problem, but the examples which escape would need to be very carefully and subtly constructed. Indeed, as soon as we know how these examples are constructed – as soon as we know for sure that a particular case has eluded our algorithm – then we can improve our algorithm to include that case also. Only pairs of words that are not ‘equal’ can escape, so as soon as we know that they have escaped, we know they are not ‘equal’, and that fact can be tagged on to our algorithm. Our improved insight will lead to an improved algorithm!COMPLEXITY THEORYThe arguments that I have given above, and in the preceding chapters, concerning the nature, existence, and limitations of algorithms have been very much at the ‘in principle’ level. I have not discussed at all the question of whether the algorithms arising are likely to be in any way practical. Even for problems where it is clear that algorithms exist and how such algorithms can be constructed, it may require much ingenuity and hard work to develop such algorithms into something usable. Sometimes a little insight and ingenuity will lead to considerable reductions in the complexity of an algorithm and sometimes to absolutely enormous improvements in its speed. These questions are often very detailed and technical, and a great deal of work has been done in many different contexts in recent years in the construction, understanding, and improvement of algorithms – a rapidly expanding and developing field of endeavour. It would not be pertinent for me to attempt to enter into a detailed discussion of such questions. However, there are various general things that are known, or conjectured, concerning certain absolute limitations on how much the speed of an algorithm can be increased. It turns out that even among mathematical problems that are algorithmic in nature, there are some classes of problem that are intrinsically vastly more difficult to solve algorithmically than others. The difficult ones can be solved only by very slow algorithms (or, perhaps, with algorithms which require an inordinately large amount of storage space, etc.). The theory which is concerned with questions of this kind is called complexity theory.Complexity theory is concerned not so much with the difficulty of solving single problems algorithmically, but with infinite families of problems where there would be a general algorithm for finding answers to all the problems of one single family. The different problems in the family would have different ‘sizes’, where the size of a problem is measured by some natural number n. (I shall have more to say in a moment about how this number n actually characterizes the size of the problem.) The length of time – or more correctly, the number of elementary steps – that the algorithm would need for each particular problem of the class would be some natural number N which depends on n. To be a little more precise, let us say that among all of the problems of some particular size n, the greatest number of steps that the algorithm takes is N. Now, as n gets larger and larger, the number N is likely to get larger and larger too. In fact, N is likely to get large very much more rapidly than n. For example, N might be approximately proportional to n2, or to n3 or perhaps to 2n (which, for large n, is very much greater than each of n, n2, n3, n4, and n5 – greater, indeed, than nr for every fixed number r), or N
might even be approximately proportional to, say, 22n (which is much greater still).Of course, the number of ‘steps’ might depend on the type of computing machine on which the algorithm is to be run. If the computing machine is a Turing machine of the type described in Chapter 2, where there is just a single tape – which is rather inefficient – then the number N might increase more rapidly (i.e. the machine might run more slowly) than if two or more tapes are allowed. To avoid uncertainties of this kind, a broad categorization is made of the possible ways in which N gets large as a function of n, so that no matter what type of Turing machine is used, the measure of the rate of increase of N will always fall into the same category. One such category, referred to as P (which stands for ‘polynomial time’), includes all rates which are, at most, fixed multiples* of one of n, n2, n3, n4, n5,. . . That is to say, for any problem lying in the category P (where by a ‘problem’ I really mean a family of problems with a general algorithm for solving them), we haveN ≤ K × nr,the numbers K and r being constants (independent of n). This means that N is no larger than some multiple of n raised to some fixed power.A simple type of problem that certainly belongs to P is that of multiplying two numbers together. To explain this, I should first describe how the number n characterizes the size of the particular pair of numbers that are to be multiplied. We can imagine that each number is written in the binary notation and that n/2 is simply the number of binary digits of each number, giving a total of n binary digits – i.e. n bits – in all. (If one of the numbers is longer than the other, we can simply start off the shorter one with a succession of zeros to make it up to the length of the longer one.) For example, if n = 14, we could be considering1011010 × 0011011(which is 1011010 × 11011, but with added zeros on the shorter figure). The most direct way of carrying out this multiplication is just to write it out:recalling that, in the binary system, 0 × 0 = 0, 0 × 1 = 0, 1 × 0 = 0, 1 × 1 = 1, 0 + 0 = 0, 0 + 1 = 1, 1 + 0 = 1, 1 + 1 = 10. The number of individual binary multiplications is (n/2) × (n/2) = n2/4, and there can be up to (n2/4) – (n/2) individual binary additions (including carrying). This makes (n2/2) – (n/2) individual arithmetical operations in all – and we should include a few extra for the logical steps involved in the carrying. The total number of steps is essentially N = n2/2 (ignoring the lower order terms) which is certainly polynomial.13For a class of problems in general, we take the measure n of the ‘size’ of the problem to be the total number of binary digits (or bits) needed to specify the free data of the problem of that particular size. This means that, for given n, there will be up to 2n different instances of the problem for that given size (because each digit can be one of two possibilities, 0 or 1, and there are n digits in all), and these have to be coped with uniformly by the algorithm, in not more than N steps.There are many examples of (classes of) problems which are not in P. For example, in order to perform the operation of computing 22r from the natural number r we would need about 2n steps even just to write down the answer, let alone to perform the calculation, n being the number of binary digits in the binary representation of r. The operation of computing 222r takes some thing like 22r steps just to write down, etc.! These are much bigger than polynomials and so certainly not in P.More interesting are problems where the answers can be written down, and even checked for correctness, in polynomial time. There is an important category of (algorithmically soluble classes of) problems characterized by this property. They are referred to as NP (classes of) problems. More precisely, if an individual problem of a class of problems in NP has a solution, then the algorithm will give that solution, and it must be possible to check in polynomial time that the proposed solution is indeed a solution. In the cases where the problem has no solution, the algorithm will say so, but one is not required to check – in polynomial time or otherwise – that there is indeed no solution.14NP problems arise in many contexts, both within mathematics itself and in the practical world. I shall just give one simple mathematical example: the problem of finding what is called a ‘Hamiltonian circuit’ in a graph (a rather daunting name for an extremely simple idea). By a ‘graph’ is meant a finite collection of points, or ‘vertices’, a certain number of pairs of which are connected together by lines – called the ‘edges’ of the graph. (We are not interested in geometrical or ‘distance’ properties here, but only in which vertex is connected to which. Thus it does not really matter whether the vertices are all represented on one plane – assuming that we don’t mind the edges crossing over one another – or in three-dimensional space.) A Hamiltonian circuit is simply a closed-up route (or loop) consisting solely of edges of the graph, and which passes exactly once through each vertex. An example of a graph, with a Hamiltonian circuit drawn on it, is depicted in Fig. 4.14. The Hamiltonian circuit problem is to decide, for any given graph, whether or not a Hamiltonian circuit exists, and to display one explicitly whenever one does exist.There are various ways of presenting a graph in terms of binary digits. It does not matter a great deal which method is used. One procedure would be to number the vertices 1, 2, 3, 4, 5,. . ., and then to list the pairs in some appropriate fixed order:(1, 2), (1, 3), (2, 3), (1, 4), (2, 4), (3, 4), (1, 5), (2, 5), (3, 5), (4, 5), (1, 6), . . .Fig. 4.14. A graph with a Hamiltonian circuit indicated (slightly darker lines). There is just one other Hamiltonian circuit, which the reader may care to locate.Then we make an exactly matching list of ‘0’s and ‘1’s where we put a ‘1’ whenever the pair corresponds to an edge of the graph and a ‘0’ whenever it does not. Thus the binary sequence10010110110. . .would designate that vertex 1 is joined to vertex 2, to vertex 4, and to vertex 5,. . . vertex 3 is joined to vertex 4 and to vertex 5,. . ., vertex 4 is joined to vertex 5, . . . etc. (as in Fig. 4.14). The Hamiltonian circuit could be given, if desired, just as a sub-collection of these edges, which would be described as a binary sequence with many more zeros than before. The checking procedure is something that can be achieved much more rapidly than finding the Hamiltonian circuit in the first place. All that one needs to do is check that the proposed circuit is indeed a circuit, that its edges indeed belong to those of the original graph, and that each vertex of the graph is used exactly twice – once at the end of each of two edges. This checking procedure is something that can be easily achieved in polynomial time.In fact this problem is not only NP, but what is known as NP-complete. This means that any other NP problem can be converted into it in polynomial time – so that if someone were clever enough to find an algorithm which solves the Hamiltonian circuit problem in polynomial time, i.e. to show that the Hamiltonian circuit problem is actually in P, then it would follow that all NP problems are actually in P! Such a thing would have momentous implications. In a general way, problems which are in P are regarded as being ‘tractable’ (i.e. ‘soluble in an acceptable length of time’), for reasonably large n, on a fast modern computer, while problems in NP which are not in P are regarded as being ‘intractable’ (i.e. though soluble in principle, they are ‘insoluble in practice’) for reasonably large n – no matter what increases in operational computer speed, of any foreseeable kind, are envisaged. (The actual time that would be taken, for large n, rapidly becomes longer than the age of the universe for an NP problem not in P, which is not much use for a practical problem!) Any clever algorithm for solving the Hamiltonian circuit problem in polynomial time could be converted into an algorithm for solving any other NP problem whatever, in polynomial time!Another problem which is NP-complete15 is the ‘travelling salesman problem’, which is rather like the Hamiltonian circuit problem except that the various edges have numbers attached to them, and one seeks that Hamiltonian circuit for which the sum of the numbers (the ‘distance’ travelled by the salesman) is a minimum. Again a polynomial time solution of the travelling salesman problem would lead to polynomial time solutions to all other NP problems. (If such a solution were found, it would make headline news! For, in particular, there are the secret code systems, that have been introduced over the past several years, which depend on a problem of factorization of large integers, this being another NP problem. If that problem could be solved in polynomial time, then such codes could probably be cracked by the use of powerful modern computers, but if not, then the codes appear to be safe. See Gardner 1989.)It is commonly believed by the experts that it is actually impossible, with any Turing machine-like device, to solve an NP-complete problem in polynomial time, and that, consequently, P and NP are not the same. Very likely this belief is correct, but as yet no-one has been able to prove it. This remains the most important unsolved problem of complexity theory.COMPLEXITY AND COMPUTABILITY IN PHYSICAL THINGSComplexity theory is important for our considerations in this book because it raises another issue, somewhat separate from the question of whether or not things are algorithmic: namely, whether or not things that are known to be algorithmic are actually algorithmic in a useful way. In the later chapters, I shall have less to say concerning matters of complexity theory than of computability. For I am inclined to think (though, no doubt, on quite inadequate grounds) that unlike the basic question of computability itself, the issues of complexity theory are not quite the central ones in relation to mental phenomena. Moreover, I feel that the questions of practicality of algorithms are being only barely touched by complexity theory as it stands today.However, I could well be wrong about the role of complexity. As I shall remark later (in Chapter 9, p. 519), the complexity theory for actual physical objects could perhaps be different in significant ways from that which we have just been discussing. For this possible difference to become manifest, it would be necessary to harness some of the magical properties of quantum mechanics – a mysterious yet powerfully accurate theory of the behaviour of atoms and molecules, and of many other phenomena, some of which are important on a much larger scale. We shall be coming to terms with this theory in Chapter 6. According to a recent set of ideas introduced by David Deutsch (1985), it is possible in principle to construct a ‘quantum computer’ for which there are (classes of) problems which are not in P, yet which could be solved by that device in polynomial time. It is not at all clear, as of now, how an actual physical device could be constructed which behaves (reliably) as a quantum computer – and, moreover, the particular class of problem so far considered is decidedly artificial – but the theoretical possibility that a quantum physical device may be able to improve on a Turing machine seems to be with us.Can it be that a human brain, which I am taking for this discussion to be a ‘physical device’, albeit one of amazing subtlety and delicacy of design, as well as of complication, is itself taking advantage of the magic of quantum theory? Do we yet understand the ways in which quantum effects might be used beneficially in the solving of problems or the forming of judgements? Is it conceivable that we might have to go even ‘beyond’ present-day quantum theory to make use of such possible advantages? Is it really likely that actual physical devices might be able to improve upon the complexity theory for Turing machines? And what about the computability theory for actual physical devices?To address such questions we must turn away from matters purely mathematical and ask, in the following chapters, how the physical world actually behaves!NOTES1. In considering sets whose members may again be sets we must be careful to distinguish between the members of that set and the members of the members of that set. For example, suppose S is the set of non-empty subsets of a certain other set T, where the members of T are one apple and one orange. T has the property of ‘twoness’ not ‘threeness’, but S actually has the property ‘threeness’; for the three members of S are: a set containing just one apple, a set containing just one orange, and a set containing one apple and one orange – three sets in all, these being the three members of S. Likewise, the set whose only member is the empty set possesses the property of ‘oneness’ not of ‘zeroness’ – it has one member, namely the empty set! The empty set itself has zero members, of course.2. In fact the reasoning in Gödel’s theorem can be presented in such a way that it does not depend upon a full external concept of ‘truth’ for propositions such as Pk(k). However, it still depends upon an interpretation of the actual ‘meaning’ of some of the symbols: in particular that ‘~Ǝ’ really means ‘there is no (natural number). . . such that. . .’3. In the following, small letters represent natural numbers and capital ones, finite sets of natural numbers. Let m → [n, k, r] stand for the statement ‘If X = {0, 1,. . ., m), each of whose k-element subsets are assigned to r boxes, then there is a ‘large’ subset Y of X with at least n-elements such that, all k-element subsets of Y go into the same box.’ Here ‘large’ means that Y has more elements than that natural number which is the smallest element of Y. Consider the proposition: ‘For any choice of k, r, and n there exists an m0 such that, for all m greater than m0, the statement m → [n, k, r] always holds true’. This proposition has been shown by J. Paris and L. Harrington (1977) to be equivalent to a Gödel-type proposition for the standard (Peano) axioms for arithmetic, unprovable from those axioms, yet asserting something about those axioms which is ‘Obviously true’ (namely, in this case, that propositions deducible from the axioms are themselves true).4. The title was ‘Systems of logic based on ordinals’, and some readers will be familiar with the notation for Cantor’s ordinal numbers that I have been employing in the subscripts. The hierarchy of logical systems that one obtains by the procedure that I have described above is characterized by computable ordinal numbers.There are some mathematical theorems that are quite natural and easy to state which, if one attempted to prove them using the standard (Peano) rules of arithmetic, would require using the above ‘Gödelization’ procedure to an outrageously huge degree (extending the procedure enormously beyond what I have outlined above). The mathematical proofs of these theorems, are not at all of the kind which depend on any vague or questionable reasoning that would seem to lie outside the procedures of normal mathematical argument. See Smorynski (1983).5. The continuum hypothesis that was referred to in Chapter 3, p. 112 (and which states that C = ℵ1) is the most ‘extreme’ mathematical statement that we have encountered here (although much more extreme statements than this are often considered). The continuum hypothesis is additionally interesting because Gödel himself, together with Paul J. Cohen, established that the continuum hypothesis is actually independent of the standard axioms and rules of procedure of set theory. Thus, one’s attitude to the status of the continuum hypothesis distinguishes between the formalist and Platonist point of view. To a formalist, the continuum hypothesis is ‘undecidable’ since it cannot be established or refuted using the standard (Zermelo–Frankel) formal system, and it is ‘meaningless’ to call it either ‘true’ or ‘false’. However, to a good Platonist, the continuum hypothesis is indeed either true or false, but to establish which is the case will require some new forms of reasoning – actually going beyond even employing Gödel-type propositions for the Zermelo–Frankel formal system. (Cohen (1966), himself suggested a reflection principle which makes the continuum hypothesis ‘obviously false’!)6. For a vivid and fairly non-technical account of these matters, see Rucker (1984).7. Brouwer himself seems to have started on this line of thought partly because of nagging worries about a ‘non-constructiveness’ in his proof of one of his own theorems, ‘the Brouwer fixed point theorem’ of topology. The theorem asserts that if you take a disc – that is, a circle together with its interior – and move it in a continuous way to inside the region where it was originally located, then there is at least one point of the disc – called a fixed point – which ends up exactly where it started. One may have no idea exactly where this point is, or whether there might be many such points, it is merely the existence of some such point that the theorem asserts. (As mathematical existence theorems go, this is actually a fairly ‘constructive’ one. Of a different order of non-constructiveness are existence theorems which depend on what is known as the ‘Axiom of Choice’ or ‘Zorn’s lemma’ (cf. Cohen 1966, Rucker 1984).) In Brouwer’s case, the difficulty is similar to the following: if f is a real-valued continuous function of a real variable which takes both positive and negative values, find a place where f vanishes. The usual procedure involves repeatedly bisecting the interval where f changes sign, but it may not be ‘constructive’ in Brouwer’s required sense, to decide whether the intermediate values of f are positive, negative, or zero.8. We enumerate the sets {v, w, x, . . ., z), where v represents the function f according to some lexicographical scheme. We check (recursively) at each stage to see whether f(w, x,. . ., z) = 0 and retain the proposition Ǝw, x,. . . z[f(w, x, . . . z) = 0] only if so.9. I have recently been informed by Leonore Blum that (stimulated by my comments in the initial hardback printing of this book) she has ascertained that the (complement of the) Mandelbrot set is indeed non-recursive, as I conjecture in the text, in the particular sense referred to in note 10 below.10. There is a new theory of computability for real-valued functions of real numbers (as opposed to the conventional natural-number-valued functions of natural numbers), due to Blum, Shub, and Smale (1989), the details of which have come to my attention only very recently. This theory will also apply to complex-valued functions, and it could have a significant bearing on the issues raised in the text.11. This particular problem is more correctly called ‘the word problem for semigroups’. There are also other forms of the word problem, where the rules are slightly different. These will not concern us here.12. Hanf (1974) and Myers (1974) have shown, moreover, that there is a single set (of a great number of tiles) which will tile the plane only in a non-computable way.13. In fact, by the use of ingenuity, this number of steps can be reduced to something of the order of n log n log log n for large n – which is, of course, still in P. See Knuth (1981) for further information about such matters.14. More correctly, the classes P, NP, and NP-complete (see p. 186) are defined for problems of just a yes/no type (e.g. given a, b, and c, is it true that a × b = c?), but the descriptions given in the text are adequate for our purposes.15. Strictly we need a yes/no version of this, such as: is there a route for the salesman of distance less than so and so? (see note 14 above).

5THE CLASSICAL WORLDTHE STATUS OF PHYSICAL THEORYWHAT NEED WE know of the workings of Nature in order to appreciate how consciousness may be part of it? Does it really matter what are the laws that govern the constituent elements of bodies and brains? If our conscious perceptions are merely the enacting of algorithms, as many AI supporters would have us believe, then it would not be of much relevance what these laws actually are. Any device which is capable of acting out an algorithm would be as good as any other. Perhaps, on the other hand, there is more to our feelings of awareness than mere algorithms. Perhaps the detailed way in which we are constituted is indeed of relevance, as are the precise physical laws that actually govern the substance of which we are composed. Perhaps we shall need to understand whatever profound quality it is that underlies the very nature of matter, and decrees the way in which all matter must behave. Physics is not yet at such a point. There are many mysteries to be unravelled and many deep insights yet to be gained. Yet, most physicists and physiologists would judge that we already know enough about those physical laws that are relevant to the workings of such an ordinary-sized object as a human brain. While it is undoubtedly the case that the brain is exceptionally complicated as a physical system, and a vast amount about its detailed structure and relevant operation is not yet known, few would claim that it is in the physical principles underlying its behaviour that there is any significant lack of understanding.I shall later argue an unconventional case that, on the contrary, we do not yet understand physics sufficiently well that the functioning of our brains can be adequately described in terms of it, even in principle. To make this case, it will be necessary for me first to provide some overview of the status of present physical theory. This chapter is concerned with what is called ‘classical physics’, which includes both Newton’s mechanics and Einstein’s relativity. ‘Classical’, here, means essentially the theories that held sway before the arrival, in about 1925 (through the inspired work of such physicists as Planck, Einstein, Bohr, Heisenberg, Schrödinger, de Broglie, Born, Jordan, Pauli, and Dirac), of quantum theory – a theory of uncertainty, indeterminism, and mystery, describing the behaviour of molecules, atoms, and subatomic particles. Classical theory is, on the other hand, deterministic, so the future is always completely fixed by the past. Even so, classical physics has much about it that is mysterious, despite the fact that the understanding that has been achieved over the centuries has led us to a picture of quite phenomenal accuracy. We shall also have to examine the quantum theory (in Chapter 6), for I believe that, contrary to what appears to be a majority view among physiologists, quantum phenomena are likely to be of importance in the operation of the brain – but that is a matter for chapters to follow.What science has so far achieved has been dramatic. We have only to look about us to witness the extraordinary power that our understandings of Nature have helped us to obtain. The technology of the modern world has derived, in good measure, from a great wealth of empirical experience. However, it is physical theory that underlies our technology in a much more fundamental way, and it is physical theory that we shall be concerned with here. The theories that are now available to us have an accuracy which is quite remarkable. But it is not just their accuracy that has been their strength. It is also the fact that they have been found to be extraordinarily amenable to a precise and detailed mathematical treatment. It is these facts together that have yielded us a science of truly impressive power.A good deal of this physical theory is not particularly recent. If one event can be singled out above all others, it is the publication, in 1687, of the Principia of Isaac Newton. This momentous work demonstrated how, from a few basic physical principles, one may comprehend, and often predict with striking accuracy, a great deal of how physical objects actually behave. (Much of the Principia was also concerned with remarkable developments in mathematical technique, though more practical methods were provided later by Euler and others.) Newton’s own work, as he readily admitted, owed much to the achievements of earlier thinkers, the names of Galileo Galilei, René Descartes, and Johannes Kepler being pre-eminent among these. Yet there were important underlying concepts from more ancient thinkers still, such as the geometrical ideas of Plato, Eudoxos, Euclid, Archimedes, and Apollonios. I shall have more to say about these later.Departures from the basic scheme of Newton’s dynamics were to come later. First, there was the electromagnetic theory of James Clerk Maxwell, developed in the mid-nineteenth century. This encompassed not only the classical behaviour of electric and magnetic fields, but also that of light.1 This remarkable theory will be the subject of our attentions later in this chapter. Maxwell’s theory is of considerable importance to present-day technology, and there is no doubt that electromagnetic phenomena have relevance to the workings of our brains. What is less clear, however, is that there can be any significance for our thinking processes in the two great theories of relativity associated with the name of Albert Einstein. The special theory of relativity, which developed from a study of Maxwell’s equations, was put forward by Henri Poincaré, Hendrick Antoon Lorentz, and Einstein (and later given an elegant geometrical description by Hermann Minkowski) to explain the puzzling behaviour of bodies when they move at speeds close to that of light. Einstein’s famous equation ‘E = mc2’ was part of this theory. But the theory’s impact on technology has been very slight so far (except where it impinges on nuclear physics), and its relevance to the workings of our brains would seem to be peripheral at best. On the other hand, special relativity tells us something deep about physical reality, in relation to the nature of time. We shall see in the next chapters that this leads to some profound puzzles concerning quantum theory which could have importance in relation to our perceived ‘flow of time’. Moreover, we shall need to understand the special theory before we can properly appreciate Einstein’s general theory of relativity – the theory which uses curved space–time to describe gravity. The impact of this theory on technology has been almost non-existent* so far, and it would seem fanciful in the extreme to suggest any relevance to the workings of our brains! But, remarkably, it is indeed the general theory that will have the greater relevance to our later deliberations, most particularly in Chapters 7 and 8, where we shall need to venture out to the farthest reaches of space and time in order to glean something of the changes that I claim are necessary before a properly coherent picture of quantum theory can come to light – but more of that later!These are the broad areas of classical physics. What of quantum physics? Unlike relativity theory, quantum theory is beginning to have a really significant impact on technology. This is partly owing to the understandings that it has provided, in certain technologically important areas such as chemistry and metallurgy. Indeed, some would say that these areas have actually become subsumed into physics, by virtue of the detailed new insights that the quantum theory has given us. In addition, there are quite new phenomena that quantum theory has provided us with, the most familiar of which being, I suppose, the laser. Might not some essential aspects of quantum theory also be playing crucial roles in the physics that underlies our thought processes?What about physical understandings of more recent origin? Some readers may have come across excitedly-expressed ideas, involving such names as ‘quarks’ (cf. p. 200), ‘GUT’ (Grand Unified Theories), the ‘inflationary scenario’ (see end-note 13 on p. 449), ‘supersymmetry’, ‘(super) string theory’, etc. How do such new schemes compare with those that I have just been referring to? Shall we need to know about them also? I believe that in order to put things in a more appropriate perspective, I should formulate three broad categories of basic physical theory. I label these as follows:1. SUPERB,2. USEFUL,       3. TENTATIVE.Into the SUPERB category must go all those that I have been discussing in the paragraphs preceding this one. To qualify as SUPERB, I do not deem it necessary that the theory should apply without refutation to the phenomena of the world, but I do require that the range and accuracy with which it applies should, in some appropriate sense, be phenomenal. The way that I am using the term ‘superb’, it is an extraordinary remarkable fact that there are any theories in this category at all! I am not aware of any basic theory in any other science which could properly enter this category. Perhaps the theory of natural selection, as proposed by Darwin and Wallace, comes closest, but it is still some good way off.The most ancient of the SUPERB theories is the Euclidean geometry that we learn something of at school. The ancients may not have regarded it as a physical theory at all, but that is indeed what it was: a sublime and superbly accurate theory of physical space – and of the geometry of rigid bodies. Why do I refer to Euclidean geometry as a physical theory rather than a branch of mathematics? Ironically, one of the clearest reasons for taking that view is that we now know that Euclidean geometry is not entirely accurate as a description of the physical space that we actually inhabit! Einstein’s general relativity now tells us that space(–time) is actually ‘curved’ (i.e. not exactly Euclidean) in the presence of a gravitational field. But that fact does not detract from Euclidean geometry’s characterization as SUPERB. Over a metre’s range, deviations from Euclidean flatness are tiny indeed, errors in treating the geometry as Euclidean amounting to less than the diameter of an atom of hydrogen!It is reasonable to say that the theory of statics (which concerns bodies not in motion), as developed into a beautiful science by Archimedes, Pappos, and Stevin, would also have qualified as SUPERB. This theory is now subsumed by Newtonian mechanics. The profound ideas of dynamics (bodies in motion) introduced by Galileo in around 1600, and developed into a magnificent and comprehensive theory by Newton, must undoubtedly come into the SUPERB category. As applied to the motions of planets and moons, the observed accuracy of this theory is phenomenal – better than one part in ten million. The same Newtonian scheme applies here on earth – and out among the stars and galaxies – to some comparable accuracy. Maxwell’s theory, likewise, is accurately valid over an extraordinary range, reaching inwards to the tiny scale of atoms and sub-atomic particles, and outwards, also, to that of galaxies, some million million million million million million times larger! (At the very small end of the scale, Maxwell’s equations must be combined appropriately with the rules of quantum mechanics.) It surely also must qualify as SUPERB.Einstein’s special relativity (anticipated by Poincaré and elegantly reformulated by Minkowski) gives a wonderfully accurate description of phenomena in which the speeds of objects are allowed to come close to that of light – speeds at which Newton’s descriptions at last begin to falter. Einstein’s supremely beautiful and original theory of general relativity generalizes Newton’s dynamical theory (of gravity) and improves upon its accuracy, inheriting all the remarkable precision of that theory concerning the motions of planets and moons. In addition, it explains various detailed observational facts which are incompatible with the older Newtonian scheme. One of these (the ‘binary pulsar’, cf. p. 273) shows Einstein’s theory to be accurate to about one part in 1014. Both relativity theories – the second of which subsumes the first – must indeed be classified as SUPERB (for reasons of their mathematical elegance, almost as much as of their accuracy).The range of phenomena which are explained according to the strangely beautiful and revolutionary theory of quantum mechanics, and the accuracy with which it agrees with experiment, clearly tells us that quantum theory, also, must certainly qualify as SUPERB. No observational discrepancies with that theory are known – yet its strength goes far beyond this, in the number of hitherto inexplicable phenomena that the theory now explains. The laws of chemistry, the stability of atoms, the sharpness of spectral lines (cf. p. 295) and their very specific observed patterns, the curious phenomenon of superconductivity (zero electrical resistance), and the behaviour of lasers are just a few amongst these.I am setting high standards for the category SUPERB, but this is what we have become accustomed to in physics. Now, what about the more recent theories? In my opinion there is only one of them which can possibly qualify as SUPERB and this is not a particularly recent one: a theory called quantum electrodynamics (or QED), which emerged from the work of Jordan, Heisenberg, and Pauli, was formulated by Dirac in 1926–1934, and made workable by Bethe, Feynman, Schwinger, and Tomonaga in 1947–1948. This theory arose as a combination of the principles of quantum mechanics with special relativity, incorporating Maxwell’s equations and a fundamental equation governing the motion and spin of electrons, due to Dirac. The theory as a whole does not have the compelling elegance or consistency of the earlier SUPERB theories, but it qualifies by virtue of its truly phenomenal accuracy. A particularly noteworthy implication is the value of the magnetic moment of an electron. (Electrons behave like tiny magnets of spinning electric charge. The term ‘magnetic moment’ refers to the strength of this tiny magnet.) The value 1.001159 65246 (in appropriate units – with an allowance for error of about 20 in the last two digits) is what is calculated from QED for this magnetic moment, whereas the most recent experimental value is 1.001159652193 (with a possible error of about 10 in the last two digits). As Feynman has pointed out, this kind of accuracy could determine the distance between New York and Los Angeles to within the width of a human hair! We shall not need to know about this theory here, but for completeness, I shall briefly mention some of its essential features towards the end of the next chapter.*There are some current theories that I would place in the USEFUL category. Two of these we shall not need here, but they are worthy of a mention. The first is the Gell–Mann–Zweig quark model for the sub-atomic particles called hadrons (the protons, neutrons, mesons, etc. which constitute atomic nuclei – or, more correctly, the ‘strongly interacting’ particles) and the detailed (later) theory of their interactions, referred to as quantum chromodynamics, or QCD. The idea is that all hadrons are made up of constituents known as ‘quarks’ which interact with one another by a certain generalization of Maxwell’s theory (called ‘Yang–Mills theory’). Second, there is a theory (due to Glashow, Salam, Ward, and Weinberg – again using Yang–Mills theory) which combines electromagnetic forces with the ‘weak’ interactions that are responsible for radioactive decay. This theory incorporates a description of the so-called leptons (electrons, muons, neutrinos; also W- and Z-particles – the ‘weakly interacting’ particles). There is some good experimental support for both theories. However, they are, for various reasons, rather more untidy than one would wish (as is QED, but these are more so), and their observed accuracy and predictive power, at present, falls a very long way short of the ‘phenomenal’ standard required for their inclusion in the SUPERB category. These two theories together (the second including QED) are sometimes referred to as the standard model.Finally, there is a theory of another type which I believe also belongs at least to the USEFUL category. This is what is called the theory of the big bang origin of the universe.* This theory will have an important role to play in the discussions of Chapters 7 and 8.I do not think that anything else makes it into the USEFUL2 category. There are many currently (or recently) popular ideas. The names of some of them are: ‘Kaluza–Klein’ theories, ‘supersymmetry’ (or ‘supergravity’), and the still extremely fashionable ‘string’ (or ‘superstring’) theories, in addition to the ‘GUT’ theories (and certain ideas derived from them, such as the ‘inflationary scenario’, cf. note 13 on p. 449). All these are, in my opinion, firmly in the category TENTATIVE. (See Barrow 1988, Close 1983, Davies and Brown 1988, Squires 1985.) The important distinction between the categories USEFUL and TENTATIVE is the lack of any significant experimental support for the theories in the latter category.3 This is not to say that perhaps one of them might not be raised, dramatically, into the USEFUL or even SUPERB categories. Some of these theories indeed contain original ideas of some notable promise, but they remain ideas, as of now, without experimental support. The TENTATIVE category is a very broad-ranging one. The ideas involved in some of them could well contain the seeds of a new substantial advance in understanding, while some of the others strike me as being definitely misguided or contrived. (I was tempted to split off a fourth category from the respectable TENTATIVE one, and refer to it as, say, MISGUIDED – but then I thought better of it, since I do not want to lose half of my friends!)One should not be surprised that the main SUPERB theories are ancient ones. Throughout history there must have been very many more theories that would have fallen in the TENTATIVE category, but most of these have been forgotten. Likewise, in the USEFUL category must have been many which have since faded; but there are also some which have been subsumed by theories which later came into their own as SUPERB. Let us consider a few examples. Before Copernicus, Kepler, and Newton produced a much better scheme, there was a wonderfully elaborate theory of planetary motion that the ancient Greeks had put forward, known as the Ptolemaic system. According to this scheme the motions of the planets are governed by a complicated composition of circular motions. It had been quite effective for making predictions, but became more and more over-complicated as greater accuracy was needed. The Ptolemaic system seems very artificial to us today. This was a good example of a USEFUL theory (for about twenty centuries, in fact!) which subsequently faded altogether as a physical theory, though it played an organizational role of definite historical importance. For a good example of a USEFUL theory of the ultimately successful kind we may look, instead, to Kepler’s brilliant conception of elliptical planetary motion. Another example was Mendeleev’s periodic table of the chemical elements. In themselves they did not provide predictive schemes of the required ‘phenomenal’ character, but they later became ‘correct’ deductions within the SUPERB theories which grew out of them (Newtonian dynamics and quantum theory, respectively).In the sections and chapters which follow, I shall not have much to tell about current theories which are merely USEFUL or TENTATIVE. There is enough to say about those which are SUPERB. It is indeed fortunate that we have such theories and can, in so remarkably complete a way, comprehend the world in which we live. Eventually we must try to decide whether even these theories are rich enough to govern the actions of our brains and minds. I shall broach this question in due course; but for now let us consider the SUPERB theories as we know them, and try to ponder upon their relevance to our purposes here.EUCLIDEAN GEOMETRYEuclidean geometry is simply that subject that we learn as ‘geometry’ at school. However, I expect that most people think of it as mathematics, rather than as a physical theory. It is also mathematics, of course, but Euclidean geometry is by no means the only conceivable mathematical geometry. The particular geometry that has been handed down to us by Euclid describes very accurately the physical space of the world in which we live, but this is not a logical necessity – it is just a (nearly exact) observed feature of the physical world.Indeed, there is another geometry called Lobachevskian (or hyperbolic) geometry* which is very like Euclidean geometry in most ways, but with certain intriguing differences. For example, recall that in Euclidean geometry the sum of the angles of any triangle is always 180°. In Lobachevskian geometry, this sum is always less than 180°, the difference being proportional to the area of the triangle (see Fig. 5.1).The remarkable Dutch artist Maurits C. Escher has produced some very fine and accurate representations of this geometry. One of his prints is reproduced in Fig. 5.2. Each black fish is to be thought of as being of the same size and shape as each other black fish, according to Lobachevskian geometry, and similarly for the white ones. The geometry cannot be represented completely accurately in the ordinary Euclidean plane, hence the apparent crowding just inside the circular boundary. Imagine yourself to be located inside the pattern but somewhere close to this boundary, then the Lobachevskian geometry is supposed to seem just the same to you as it would be in the middle, or anywhere else. What appears to be this ‘boundary’ of the pattern, according to this Euclidean representation, is really ‘at infinity’ in the Lobachevskian geometry. The actual boundary circle should not be thought of as part of the Lobachevsky space at all – and nor should any of the Euclidean region which lies outside this circle. (This ingenious representation of the Lobachevsky plane is due to Poincaré. It has the special virtue that very small shapes are not distorted in the representation – only the sizes are changed.) The ‘straight lines’ of the geometry (along certain of which Escher’s fish are pointing) are circles meeting this boundary circle at right angles.Fig. 5.1. (a) A triangle in Euclidean space, (b) a triangle in Lobachevskian space.Fig. 5.2. Escher’s depiction of Lobachevskian space. (Think of all the black fish as congruent and all the white fish as congruent.)It could very well be the case that Lobachevskian geometry is actually true of our world on a cosmological scale (see Chapter 7, p. 420). However, the proportionality constant between the angle deficit for a triangle, and its area, would have to be exceedingly tiny in this case, and Euclidean geometry would be an excellent approximation to this geometry at any ordinary scale. In fact, as we shall be seeing later in this chapter, Einstein’s general theory of relativity tells us that the geometry of our world does deviate from Euclidean geometry (though in an ‘irregular’ way that is more complicated than Lobachevskian geometry) at scales considerably less remote than cosmological ones, though the deviations are still exceedingly small at the ordinary scales of our direct experiences.The fact that Euclidean geometry seems so accurately to reflect the structure of the ‘space’ of our world has fooled us (or our ancestors!) into thinking that this geometry is a logical necessity, or into thinking that we have an innate a priori intuitive grasp that Euclidean geometry must apply to the world in which we live. (Even the great philosopher Immanuel Kant claimed this.) This real break with Euclidean geometry only came with Einstein’s general relativity, which was put forward many years later. Far from Euclidean geometry being a logical necessity, it is an empirical observational fact that this geometry applies so accurately – though not quite exactly – to the structure of our physical space! Euclidean geometry was indeed, all along, a (SUPERB) physical theory. This was in addition to its being an elegant and logical piece of pure mathematics.In a sense, this was not so far from the philosophical viewpoint espoused by Plato (c. 360 BC; this was some fifty years before Euclid’s Elements, his famous books on geometry). In Plato’s view, the objects of pure geometry – straight lines, circles, triangles, planes, etc. – were only approximately realized in terms of the world of actual physical things. Those mathematically precise objects of pure geometry inhabited, instead, a different world – Plato’s ideal world of mathematical concepts. Plato’s world consists not of tangible objects, but of ‘mathematical things’. This world is accessible to us not in the ordinary physical way but, instead, via the intellect. One’s mind makes contact with Plato’s world whenever it contemplates a mathematical truth, perceiving it by the exercise of mathematical reasoning and insight. This ideal world was regarded as distinct and more perfect than the material world of our external experiences, but just as real. (Recall our discussions of Chapters 3 and 4, pp. 127, 146 on the Platonic reality of mathematical concepts.) Thus, whereas the objects of pure Euclidean geometry can be studied by thought, and many properties of this ideal thereby derived, it would not be a necessity for the ‘imperfect’ physical world of external experience to adhere to this ideal exactly. By some miraculous insight Plato seems to have foreseen, on the basis of what must have been very sparse evidence indeed at that time, that: on the one hand, mathematics must be studied and understood for its own sake, and one must not demand completely accurate applicability to the objects of physical experience; on the other hand, the workings of the actual external world can ultimately be understood only in terms of precise mathematics – which means in terms of Plato’s ideal world ‘accessible via the intellect’!Plato founded an Academy in Athens aimed at the furtherance of such ideas. Among the elite who rose out of its membership was the exceedingly influential and famous philosopher Aristotle. But here we shall be concerned with another member of this Academy – somewhat less well-known than Aristotle, but in my view, a much finer scientist – one of the very great thinkers of antiquity: the mathematician and astronomer Eudoxos.There is a profound and subtle ingredient to Euclidean geometry – indeed a most essential one – that we hardly think of as geometry at all nowadays! (Mathematicians would tend to call this ingredient ‘analysis’ rather than ‘geometry’.) This was the introduction, in effect, of real numbers. Euclidean geometry refers to lengths and angles. To understand this geometry, we must appreciate what kind of ‘numbers’ are needed to describe such lengths and angles. The central new idea was put forward in the fourth century BC by Eudoxos (c. 408–355 BC).* Greek geometry had been in a ‘crisis’ owing to the discovery by the Pythagoreans that numbers such as √2 (needed in order to express the length of the diagonal of a square in terms of its side) cannot be expressed as a fraction (cf. Chapter 3, p. 105). It had been important to the Greeks to be able to formulate their geometrical measures (ratios) in terms of (ratios of) integers, in order that geometrical magnitudes could be studied according to the laws of arithmetic. Basically Eudoxos’s idea was to provide a method of describing ratios of lengths (i.e. real numbers!) in terms of integers. He was able to provide criteria, stated in terms of integer operations, for deciding when one length ratio exceeds another, or whether the two are actually to be regarded as exactly equal.The idea was roughly as follows: If a, b, c, and d are four lengths, then a criterion for ascertaining that the ratio alb is greater than the ratio c/d is that there exist integers M and N such that a added to itself N times exceeds b added to itself M times, whilst also d added to itself M times exceeds c added to itself N
times.* A corresponding criterion can be used to ascertain that a/b is less than c/d. The sought-for criterion for the equality a/b = c/d is now simply that neither of these other two criteria can be satisfied!A fully precise abstract mathematical theory of real numbers was not developed until the nineteenth century, by mathematicians such as Dedekind and Weierstrass. But their procedure actually followed very similar lines to that which Eudoxos had already discovered some twenty-two centuries earlier! There is no need to describe this modern development here. This modern theory was vaguely hinted at on p. 107 in Chapter 3, but for ease of presentation I preferred, in that chapter, to base the discussion of real numbers on the more familiar decimal expansions. (These expansions were, in effect, introduced by Stevin in 1585.) It should be noted that decimal notation, though familiar to us, was actually unknown to the Greeks.There is an important difference, however, between Eudoxos’s proposal and that of Dedekind and Weierstrass. The ancient Greeks thought of the real numbers as things given – in terms of (ratios of) geometrical magnitudes – that is, as properties of ‘actual’ space. It was necessary for the Greeks to be able to describe geometrical magnitudes in terms of arithmetic in order to be able to argue rigorously about them, and also about their sums and their products – essential ingredients of so many of the marvellous geometrical theorems of the ancients. (In Fig. 5.3 I have given, by way of illustration, the remarkable theorem of Ptolemy - though Ptolemy discovered it a good deal later than the time of Eudoxos – relating the distances between four points on a circle, which nicely illustrates how both sums and products are needed.) Eudoxos’s criteria proved to be extraordinarily fruitful, and, in particular, they enabled the Greeks to compute areas and volumes in a rigorous way.However, for the mathematicians of the nineteenth century – and, indeed, for the mathematicians of today – the role of geometry has changed. To the ancient Greeks, and to Eudoxos in particular, ‘real’ numbers were things to be extracted from the geometry of physical space. Now we prefer to think of the real numbers as logically more primitive than geometry. This allows us to construct all sorts of different types of geometry, each starting from the concept of number. (The key idea was that of coordinate geometry, introduced in the seventeenth century by Fermat and Descartes. Coordinates can be used to define other types of geometry.) Any such ‘geometry’ must be logically consistent, but need not have direct relevance to the physical space of our experiences. The particular physical geometry that we do seem to perceive in an idealization of experience (e.g. depending upon our extrapolations to indefinitely large or small sizes, cf. Chapter 3, p. 113), but experiments are now accurate enough that we must take it that our ‘experienced’ geometry does actually differ from the Euclidean ideal (cf. p. 272), and it is consistent with what Einstein’s general theory of relativity tells us it should be. However, despite the changes in our view of the geometry of the physical world that have now come about, Eudoxos’s twenty-three-century-old concept of real number has actually remained essentially unchanged and forms as much an essential ingredient of Einstein’s theory as of Euclid’s. Indeed it is an essential ingredient of all serious physical theories to this day!Fig. 5.3. Ptolemy’s theorem.The fifth book of Euclid’s Elements was basically an exposition of the ‘theory of proportion’, described above, that Eudoxos introduced. This was deeply important to the work as a whole. Indeed, the entire Elements, first published in about 300 BC, must be rated as one of the most profoundly influential works of all time. It set the stage for almost all scientific and mathematical thinking thereafter. Its methods were deductive, starting from clearly stated axioms that were supposed to be ‘self-evident’ properties of space; and numerous consequences were derived, many of which were striking and important, and not at all self-evident. There is no doubt that Euclid’s work was profoundly significant for the development of subsequent scientific thought.The greatest mathematician of antiquity was undoubtedly Archimedes (287–212 BC). Using Eudoxos’s theory of proportion in ingenious ways, he worked out the areas and volumes of many different kinds of shape, such as the sphere, or more complex ones involving parabolas or spirals. Today, we would use calculus to do this, but this was some nineteen centuries before calculus, as such, was finally introduced by Newton and Leibniz! (One could say that a good half – the ‘integral’ half – of the calculus was already known to Archimedes!) The degree of mathematical rigour that Archimedes achieved in his arguments was impeccable, even by modern standards. His writings deeply influenced many later mathematicians and scientists, most notably Galileo and Newton. Archimedes also introduced the (SUPERB?) physical theory of statics (i.e. the laws governing bodies in equilibrium, such as the law of the lever and the laws of floating bodies) and developed it as a deductive science, in a way similar to that in which Euclid had developed the science of geometrical space and the geometry of rigid bodies.A contemporary of Archimedes whom I must also mention is Apollonios (c. 262–200 BC), a very great geometer of profound insights and ingenuity, whose study of the theory of conic sections (i.e. ellipses, parabolas, and hyperbolas) had a very important influence on Kepler and Newton. These shapes turned out, quite remarkably, to be just what were needed for the descriptions of planetary orbits!THE DYNAMICS OF GALILEO AND NEWTONThe profound breakthrough that the seventeenth century brought to science was the understanding of motion. The ancient Greeks had a marvellous understanding of things static – rigid geometrical shapes, or bodies in equilibrium (i.e. with all forces balanced, so there is no motion) – but they had no good conception of the laws governing the way that bodies actually move. What they lacked was a good theory of dynamics, i.e. a theory of the beautiful way in which Nature actually controls the change in location of bodies from one moment to the next. Part (but by no means all) of the reason for this was an absence of any sufficiently accurate means of keeping time, i.e. of a reasonably good ‘clock’. Such a clock is needed so that changes in position can be accurately timed, and so that the speeds and accelerations of bodies can be well ascertained. Thus, Galileo’s observation in 1583 that a pendulum could be used as a reliable means of keeping time had a far-reaching importance for him (and for the development of science as a whole!) since the timing of motion could then be made precise.4 Some fifty-five years later, with the publication of Galileo’s Discorsi in 1638, the new subject of dynamics was launched – and the transformation from ancient mysticism to modern science had begun!Let me single out just four of the most important physical ideas that Galileo introduced. The first was that a force acting on a body determines acceleration, not velocity. What do the terms ‘acceleration’ and ‘velocity’ actually mean? The velocity of a particle – or point on some body – is the rate of change, with respect to time, of the position of that point. Velocity is normally taken to be a vector quantity, which is to say that its direction has to be taken account of as well as its magnitude (otherwise we use the term ‘speed’; see Fig. 5.4). Acceleration (again a vector quantity) is the rate of change of this velocity with respect to time – so acceleration is actually the rate of change of the rate of change of position with respect to time! (This would have been difficult for the ancients to come to terms with, lacking both adequate ‘clocks’ and the relevant mathematical ideas concerning ‘rates of change’.) Galileo ascertained that the force on a body (in his case, the force of gravity) controls the acceleration of that body but does not control its velocity directly – as the ancients, such as Aristotle, had believed.Fig. 5.4. Velocity, speed, and acceleration.In particular, if there is no force then the velocity is constant – hence, unchanging motion in a straight line would result from an absence of a force (which is Newton’s first law). Bodies in free motion continue uniformly on their way, and need no force to keep them going. Indeed, one consequence of the dynamical laws that Galileo and Newton developed was that that uniform straight-line motion is physically completely indistinguishable from the state of rest (i.e. of absence of motion): there is no local way of telling uniform motion from rest! Galileo was particularly clear on this point (clearer even than was Newton) and gave a very graphic description in terms of a ship at sea (cf. Drake 1953, pp. 186–7):Shut yourself up with some friend in the main cabin below decks on some large ship, and have with you there some flies, butterflies, and other small flying animals. Have a large bowl of water with some fish in it; hang up a bottle that empties drop by drop into a wide vessel beneath it. With the ship standing still, observe carefully how the little animals fly with equal speed to all sides of the cabin. The fish swim indifferently in all directions; the drops fall into the vessel beneath; . . . When you have observed all these things carefully . . . have the ship proceed with any speed you like, so long as the motion is uniform and not fluctuating this way and that. You will discover not the least change in all the effects named, nor could you tell from any of them whether the ship was moving or standing still. . . . The droplets will fall as before into the vessel beneath without dropping toward the stern, although while the drops are in the air the ship runs many spans. The fish in their water will swim toward the front of their bowl with no more effort than toward the back, and will go with equal ease to bait placed anywhere around the edges of the bowl. Finally the butterflies and flies will continue their flights indifferently toward every side, nor will it ever happen that they are concentrated toward the stern, as if tired out from keeping up with the course of the ship, from which they will have been separated during long intervals by keeping themselves in the air.This remarkable fact, called the Principle of Galilean relativity, is actually crucial in order that the Copernican point of view can make dynamical sense. Niccolai Copernicus (1473–1543, and the ancient Greek astronomer Aristarchos, c. 310–230 BC – not to be confused with Aristotle! – eighteen centuries before him) had put forward the picture in which the sun remains at rest while the earth, as well as being in rotation about its own axis, moves in orbit about the sun. Why are we not aware of this motion, which would amount to some 100000 kilometres per hour? Before Galileo presented his dynamical theory, this indeed posed a genuine and deep puzzle for the Copernican point of view. If the earlier ‘Aristotelian’ view of dynamics had been correct, in which the actual velocity of a system in its motion through space would affect its dynamical behaviour, then the earth’s motion would surely be something very directly evident to us. Galilean relativity makes it clear how the earth can be in motion, yet this motion is not something that we can directly perceive.*Note that, with Galilean relativity, there is no local physical meaning to be attached to the concept of ‘at rest’. This already has a remarkable implication with regard to the way that space and time should be viewed. The picture that we instinctively have about space and time is that ‘space’ constitutes a kind of arena in which physical events take place. A physical object may be at one point in space at one moment and at either the same or a different point in space at a later moment. We imagine that somehow the points in space persist from one moment to the next, so that it has meaning to say whether or not an object has actually changed its spatial location. But Galilean relativity tells us that there is no absolute meaning to the ‘state of rest’, so there is no meaning to be attached to ‘the same point in space at two different times’. Which point of the Euclidean three-dimensional space of physical experience at one time is the ‘same’ point of our Euclidean threed-imensional space at another time? There is no way to say. It seems that we must have a completely new Euclidean space for each moment of time! The way to make sense of this is to consider a four-dimensional space-time picture of physical reality (see Fig. 5.5). The three-dimensional Euclidean spaces corresponding to different times are indeed regarded as separate from one another, but all these spaces are joined together to make the complete picture of a four-dimensional space-time. The histories of particles moving in uniform straight-line motion are described as straight lines (called world-lines) in the space-time. I shall return to the question of space-time, and the relativity of motion, later in the context of Einsteinian relativity. We shall find that the argument for four-dimensionality has a considerably greater force in that case.Fig. 5.5. Galilean space–time: particles in uniform motion are depicted as straight lines.The third of these great insights of Galileo was a beginning of an understanding of conservation of energy. Galileo was mainly concerned with the motion of objects under gravity. He noticed that if a body is released from rest, then whether it simply drops freely, or swings on a pendulum of arbitary length, or slides down a smooth inclined plane, its speed of motion always depends only upon the distance that it has reached below its point of release. Moreover this speed is always just sufficient to return it to the height from which it started. As we should now say, the energy stored in its height above the ground (gravitational potential energy) can be converted into the energy of its motion (kinetic energy, which depends upon the body’s speed) and back again, but the energy as a whole is neither lost nor gained.The law of energy conservation is a very important physical principle. It is not an independent physical requirement, but a consequence of the dynamical laws of Newton that we shall be coming to shortly. Increasingly comprehensive formulations of this law were made over the centuries by Descartes, Huygens, Leibniz, Euler and Kelvin. We shall come back to it later in this chapter and in Chapter 7. It turns out that when combined with Galileo’s relativity principle, energy conservation yields further conservation laws of considerable importance: conservation of mass and of momentum. The momentum of a particle is the product of its mass with its velocity. Familiar examples of momentum conservation occur with rocket propulsion, where the increase of forward momentum of the rocket exactly balances the backward momentum of the (less massive, but compensatingly swifter) exhaust gases. The recoil of a gun is also a manifestation of momentum conservation. A further consequence of Newton’s laws is conservation of angular momentum which describes the persistence of spin of a system. The earth’s spin about its axis and a tennis ball’s spin are both maintained through conservation of their angular momentum. Each constituent particle of any body contributes to that body’s total angular momentum, where the magnitude of any particle’s contribution is the product of its momentum with its perpendicular distance out from the centre. (As a consequence, the angular speed of a freely rotating object may be increased by making it more compact. This leads to a striking, but familiar, action often performed by skaters and trapeze artists. The act of drawing in the arms or the legs, as the case may be, causes the rotation rate spontaneously to increase, simply because of angular momentum conservation!) We shall find that mass, energy, momentum, and angular momentum are concepts that will have importance for us later.Finally, I should remind the reader of Galileo’s prophetic insight that, in the absence of atmospheric friction, all bodies fall at the same rate under gravity. (The reader may recall the famous story of Galileo’s dropping various objects simultaneously from the Leaning Tower of Pisa.) Three centuries later, this very insight led Einstein to generalize the relativity principle to accelerating systems of reference, and it provided the cornerstone of his extraordinary general-relativistic theory of gravity, as we shall see, near the end of this chapter.Upon the impressive foundations that Galileo had laid, Newton was able to erect a cathedral of superb grandeur. Newton gave three laws governing the behaviour of material objects. The first and second laws were essentially those given by Galileo: if no force acts on a body, it continues to move uniformly in a straight line; if a force does act on it, then its mass times its acceleration (i.e. the rate of change of its momentum) is equal to that force. One of Newton’s own special insights was to realize the need for a third law: the force that body A exerts on body B is precisely equal and opposite to the force that body B exerts on body A (‘to every action there is always opposed an equal reaction’). This provided the basic framework. The ‘Newtonian universe’ consists of particles moving around in a space which is subject to the laws of Euclid’s geometry. The accelerations of these particles are determined by the forces which act upon them. The force on each particle is obtained by adding together (using the vector addition law; see Fig. 5.6) all the separate contributions to the force on that particle, arising from all the other particles. In order to make the system well defined, some definite rule is needed to tell us what the force on particle A should be that arises from some other particle B. Normally we require that this force acts in a direct line between A and B (see Fig. 5.7). If the force is a gravitational force, then it acts attractively between A and B and its strength is proportional to the product of the two masses and to the reciprocal of the square of the distance between them: the inverse square law. For other types of force, there might be a dependence on position different from this, and the force might depend on the particles according to some quality they possess different from their masses.Fig. 5.6. The parallelogram law of vector addition.The great Johannes Kepler (1571–1630), a contemporary of Galileo’s, had noticed that the planets’ orbits about the sun are elliptical rather than circular (with the sun being always at a focus, not the centre, of the ellipse) and he formulated two other laws governing the rates at which these ellipses are described. Newton was able to show that Kepler’s three laws follow from his own general scheme of things (with an attractive inverse square law of force). Not only this, but he also obtained all sorts of detailed corrections to Kepler’s elliptical orbits, as well as other effects, such as the precession of the equinoxes (a slow movement of the direction of the earth’s rotation axis, which had been noticed by the Greeks over the centuries). In order to achieve all this, Newton had to develop many mathematical techniques – in addition to differential calculus. The phenomenal success of his efforts owed much to his supreme mathematical skills and to his equally superb physical insights.Fig. 5.7. The force between two particles is taken to be in a direct line between the two (and by Newton’s third law, the force on A due to B is always equal and opposite to the force on B due to A).THE MECHANISTIC WORLD OF NEWTONIAN DYNAMICSWith a specific law of force (such as the inverse square law of gravitation), the Newtonian scheme translates to a precise and determinate system of dynamical equations. If the positions, velocities, and masses of the various particles are specified at one time, then their positions and velocities (and their masses – these being taken to be constant) are mathematically determined for all later times. This form of determinism, as satisfied by the world of Newtonian mechanics, had (and still has) a profound influence on philosophical thought. Let us try to examine the nature of this Newtonian determinism a little more closely. What can it tell us about the question of ‘free will’? Could a strictly Newtonian world contain minds? Can a Newtonian world even contain computing machines?Let us try to be reasonably specific about this ‘Newtonian’ model of the world. We can suppose, for example, that the constituent particles of matter are all taken to be exact mathematical points, i.e. with no spatial extent whatever. As an alternative, we might take them all to be rigid spherical balls. In either case, we shall have to suppose that the laws of force are known to us, like the inverse square law of attraction of Newton’s gravitational theory. We shall want to model other forces of nature also, such as electric and magnetic forces (first studied in detail by William Gilbert in 1600), or the strong nuclear forces which are now known to bind particles (protons and neutrons) together to form atomic nuclei. Electric forces are like gravitational ones in that they, also, satisfy the inverse square law, but for which similar particles repel each other (rather than attract, as in the gravitational case), and here it is not the masses of the particles that govern the strength of electric forces between them, but their electric charges. Magnetic forces are also ‘inverse square’ like electric ones,* but nuclear forces have a quite different dependence on distance, being extremely large at the very close separations that occur within the atomic nucleus, but negligible at greater distances.Suppose that we adopt the rigid spherical ball picture, requiring that when two of the spheres collide they simply rebound perfectly elastically. That is to say, they separate again without any loss of energy (or of total momentum), as if they were perfect billiard balls. We also have to specify exactly how the forces are to act between one ball and another. For simplicity, we can assume that the force that each ball exerts on each other ball is along the line joining their centres, and its magnitude is a specified function of the length of this line. (For Newtonian gravity this assumption automatically holds true, by a remarkable theorem due to Newton; and for other laws of force it can be imposed as a consistent requirement.) Provided that the balls collide only in pairs, and no triple or higher-order collisions occur, then everything is well defined, and the outcome depends in a continuous way on the initial state (i.e. sufficiently tiny changes in the initial state lead to only small changes in the outcome). The behaviour of glancing collisions is continuous with the behaviour when the balls just miss one another. There is, however, a problem with what to do with triple or higher-order collisions. For example, if three balls A, B, and C come together at once, it makes a difference whether we consider A and B to come together just first, and C then to collide with B immediately afterwards, or whether we consider A and C to come together first, and B then to collide with A immediately afterwards (see Fig. 5.8). In our model, there is indeterminism whenever exact triple collisions occur! If we like, we can simply rule out exact triple or higher-order collisions as ‘infinitely improbable’. This provides a reasonably consistent scheme, but the potential problem of triple collisions means that the resulting behaviour may not depend in a continuous way on the initial state.Fig. 5.8. A triple collision. The resulting behaviour depends critically upon which particles come together first, so the outcome depends discontinuously on the input.This is a little unsatisfactory, and we may prefer a picture in terms of point particles. But in order to avoid certain theoretical difficulties raised by the point-particle model (infinite forces and infinite energies, when particles come towards coincidence), one must make other assumptions, such as that the forces between the particles always become very strongly repulsive at short distances. In this way we can ensure that no pair of particles ever actually do collide. (This also allows us to avoid the problem of deciding how point particles are supposed to behave when they collide!) However, for ease of visualization I shall prefer to phrase the discussion which follows entirely in terms of the rigid spheres. It seems that this kind of ‘billiard-ball’ picture is essentially the model of reality that a good many people have at the backs of their minds!Now (ignoring the multiple collision problem), the Newtonian5 billiard-ball picture of reality is indeed a deterministic model. The word ‘deterministic’ is to be taken in the sense that physical behaviour is mathematically completely determined for all times in the future (or past) by the positions and velocities of all the balls (assumed finite in number, say, to avoid certain problems), at any one time. It seems, then, that there is no room for a ‘mind’ to be influencing the behaviour of material things by the action of its ‘free will’ in this billiard-ball world. If we believe in ‘free will’, it would seem that we are forced to doubt that our actual world can be made up in this way.The vexed question of ‘free will’ hovers at the background, throughout this book – though for most of what I shall have to say, it will remain only in the background. It will have one specific, but minor, role to play later in this chapter (in relation to the issue of faster-than-light signalling in relativity). The question of free will is addressed directly in Chapter 10, and there the reader will doubtless be disappointed by what I have to contribute. I do indeed believe that there is a real issue here, rather than an imagined one, but it is profound and hard to formulate adequately. The issue of determinism in physical theory is important, but I believe that it is only part of the story. The world might, for example, be deterministic but non-computable. Thus, the future might be determined by the present in a way that is in principle non-calculable. In Chapter 10, I shall try to present arguments to show that the action of our conscious minds is indeed non-algorithmic (i.e. non-computable). Accordingly, the free will that we believe ourselves to be capable of would have to be intimately tied in with some non-computable ingredient in the laws that govern the world in which we actually live. It is an interesting question – whether or not one accepts this viewpoint with regard to free will – whether a given physical theory (such as Newton’s) is indeed computable, not just whether it is deterministic. Computability is a different question from determinism – and the fact that it is a different question is something that I am trying to emphasize in this book.IS LIFE IN THE BILLIARD-BALL WORLD COMPUTABLE?Let me first illustrate, with an admittedly absurdly artificial example, that computability and determinism are different, by exhibiting a ‘toy model universe’ which is deterministic but not computable. Let the ‘state’ of this universe at any one ‘time’ be described as a pair of natural numbers (m, n). Let Tu be a fixed universal Turing machine, say the specific one defined in Chapter 2 (p. 73). To decide what the state of this universe is to be at the next ‘instant of time’, we must ask whether the action of Tu on m ultimately stops or does not stop (i.e. whether Tu(m) ≠  or Tu(m) = , in the notation of Chapter 2, p. 76). If it stops, then the state at this next ‘instant’ is to be (m + 1, n). If it does not stop, then it is to be (n + 1, m). We saw in Chapter 2 that there is no algorithm for the halting problem for Turing machines. It follows that there can be no algorithm for predicting the ‘future’ in this model universe, despite the fact that it is completely deterministic!6Of course, this is not a model to be taken seriously, but it shows that there is a question to be answered. We may ask of any deterministic physical theory whether or not it is computable. Indeed, is the Newtonian billiard-ball world computable?The issue of physical computability depends partly on what kind of question we are proposing to ask of the system. I can think of a number of questions that could be asked for which my guess would be, for the Newtonian billiard-ball model, that it is not a computable (i.e. algorithmic) matter to ascertain the answer. One such question might be: does ball A ever collide with ball B? The idea would be that, as initial data, the positions and velocities of all the balls are given to us at some particular time (t = 0), and the problem is to work out from this data whether or not A and B will ever collide at any later time (t > 0). To make the problem specific (although not particularly realistic), we may assume that all the balls have equal radius and equal mass and that there is, say, an inverse square law of force acting between each pair of balls. One reason for guessing that this particular question is not one that can be resolved algorithmically is that the model is somewhat like a ‘billiard-ball model for a computation’ that was introduced by Edward Fredkin and Tommaso Toffoli (1982). In their model (instead of having an inverse square law of force) the balls are constrained by various ‘walls’, but they bounce elastically off one another in a similar way to the Newtonian balls that I have just been describing (see Fig. 5.9). In the Fredkin-Toffoli model, all the basic logical operations of a computer can be performed by the balls. Any Turing machine computation can be imitated: the particular choice of Turing machine Tn defines the configuration of ‘walls’, etc. of the Fredkin–Toffoli machine; then an initial state of balls in motion codes the information of the input tape, and the output tape of the Turing machine is coded by the final state of the balls. Thus, in particular, one can pose the question: does such-and-such a Turing machine computation ever stop? ‘Stopping’ might be phrased in terms of ball A eventually colliding with ball B. The fact that this question cannot be answered algorithmically (p. 78), at least suggests that the Newtonian question ‘does ball A ever collide with ball B?’, that I initially posed, cannot be answered algorithmically either.Fig. 5.9. A ‘switch’ (suggested by A. Ressler) in the Fredkin–Toffoli billiard-ball computer. If a ball enters at B then a ball subsequently leaves at D or at E depending upon whether another ball enters at A (where the entries at A and B are assumed to be simultaneous).In fact, the Newtonian problem is a much more awkward one than that put forward by Fredkin and Toffoli. They were able to specify the states of their model in terms of discrete parameters (i.e. in terms of ‘on or off’ statements like ‘either the ball is in the channel or it is not’). But in the full Newtonian problem the initial positions and velocities of the balls have to be specified with infinite precision, in terms of coordinates which are real numbers, rather than in this discrete way. Thus, we are again faced with all the problems that we had to consider, when in Chapter 4 we addressed the question of whether the Mandelbrot set is recursive. What does ‘computable’ mean when continuously varying parameters are allowed for the input and output data?7 The problem can, for the moment, be alleviated by supposing that all the initial position and velocity coordinates are given by rational numbers (although it cannot be expected that these coordinates will remain rational for later rational values of the time t). Recall that a rational number is a quotient of two integers, and is therefore specified in discrete finite terms. Using rational numbers, we can approximate, as closely as we like, whatever initial data sets we are choosing to examine. It is not altogether unreasonable to guess that, with rational initial data, there may be no algorithm for deciding whether or not A and B will eventually collide.This, however, is not really what one would mean by an assertion such as: ‘the Newtonian billiard-ball world is not computable’. The particular model that I have been comparing our Newtonian billiard-ball world with, namely the Fredkin–Toffoli ‘billiard-ball computer’, does indeed proceed according to a computation. That, after all, was the essential point of Fredkin and Toffoli’s idea – that their model should behave like a (universal) computer! The kind of issue that I am trying to raise is whether it is conceivable that a human brain can, by the harnessing of appropriate ‘non-computable’ physical laws, do ‘better’, in some sense, than a Turing machine. It is of no use trying to harness something like:‘If ball A never meets ball B then the answer toyour problem is “No”.’One might have to wait forever in order to ascertain for sure that the balls in question never do meet! That, of course, is just the kind of way that Turing machines do behave.In fact there would seem to be clear indications that, in an appropriate sense, the Newtonian billiard-ball world is computable (at least if we ignore the problem of multiple collisions). The way in which one would normally try to compute the behaviour of such a world would be to make some approximation. We might imagine that the centres of the balls are specified as lying on some grid of points, where the lattice points on the grid are just those whose coordinates are measured off, say, in hundredths of a unit. The time, also, is taken as ‘discrete’: all the allowed time-moments are to be multiples of some small unit (denoted by Δt, say). This gives rise to certain discrete possibilities for the ‘velocities’ (differences in position lattice-point values at two successive allowed time-moments, divided by Δt). The appropriate approximations to the accelerations are computed using the force law, and these are used to enable the ‘velocities’, and hence the new lattice-point positions to be computed to the required degree of approximation at the next allowed time-moment. The calculation proceeds for as many time-moments as will preserve the desired accuracy. It may well be that not many such time-moments can be computed before all accuracy is lost. The procedure is then to start again with a considerably finer space grid and somewhat finer division of allowed time-moments. This allows a greater accuracy to be achieved and the calculation can be carried farther into the future than previously, before accuracy is lost. With a finer space-grid still, and a finer division of time interval, the accuracy can be still further improved and the calculation projected still farther into the future. In this way, the Newtonian billiard-ball world can be computed as closely as desired (ignoring multiple collisions) – and, in this sense we can say that the Newtonian world is indeed computable.There is a sense, however, in which this world is ‘non-computable’ in practice. This arises from the fact that the accuracy with which the initial data can be known is always limited. In fact there is a very considerable ‘instability’ inherent in this kind of problem. A very tiny change in the initial data may rapidly give rise to an absolutely enormous change in the resulting behaviour. (Anyone who has tried to pocket a billiard ball, by hitting it with an intermediate ball that has to be hit first, will know what I mean!) This is particularly apparent when (successive) collisions are involved, but such instabilities in behaviour can also occur with Newtonian gravitational action at a distance (with more than two bodies). The term ‘chaos’, or ‘chaotic behaviour’ is often used for this type of instability. Chaotic behaviour is important, for example, with regard to the weather. Although the Newtonian equations governing the elements are well known, long-term weather prediction is notoriously unreliable!This is not at all the kind of ‘non-computability’ that can be ‘harnessed’ in any way. It is just that since there is a limit on the accuracy with which the initial state can be known, the future state cannot reliably be computed from the initial one. In effect, a random element has been introduced into the future behaviour, but this is all. If the brain is indeed calling upon useful non-computable elements in physical laws, they must be of a completely different and much more positive character from this. Accordingly, I shall not refer to this kind of ‘chaotic’ behaviour as ‘non-computability’ at all, preferring to use the term ‘unpredictability’. The presence of unpredictability is a very general phenomenon in the kind of deterministic laws that actually arise in (classical) physics, as we shall shortly see. Unpredictability is surely something that one would wish to minimize rather than ‘harness’ in the construction of a thinking machine!In order to discuss questions of computability and unpredictability in a general way, it will be helpful to adopt a more comprehensive viewpoint than before, with regard to physical laws. This will enable us to consider not only the scheme of Newtonian mechanics, but also the later theories that have come to supersede it. We shall need to catch some glimpse of the remarkable Hamiltonian formulation of mechanics.HAMILTONIAN MECHANICSThe successes of Newtonian mechanics resulted not only from its superb applicability to the physical world, but also from the richness of the mathematical theory that it gave rise to. It is remarkable that all the SUPERB theories of Nature have proved to be extraordinarily fertile as sources of mathematical ideas. There is a deep and beautiful mystery in this fact: that these superbly accurate theories are also extraordinarily fruitful simply as mathematics. No doubt this is telling us something profound about the connections between the real world of our physical experiences and the Platonic world of mathematics. (I shall attempt to address this issue later, in Chapter 10, p. 556.) Newtonian mechanics is perhaps supreme in this respect, since its birth yielded the calculus. Moreover, the specific Newtonian scheme has given rise to a remarkable body of mathematical ideas known as classical mechanics. The names of many of the great mathematicians of the eighteenth and nineteenth centuries are associated with its development: Euler, Lagrange, Laplace, Liouville, Poisson, Jacobi, Ostrogradski, Hamilton. What is called ‘Hamiltonian theory’8 summarizes much of this work, and a taste of this will be sufficient for our purposes here. The versatile and original Irish mathematician William Rowan Hamilton (1805–1865) – who was also responsible for the Hamiltonian circuits discussed on p. 185) – had developed this form of the theory in a way that emphasized an analogy with wave propagation. This hint of a relation between waves and particles – and the form of the Hamilton equations themselves–was highly important for the later development of quantum mechanics. I shall return to that aspect of things in the next chapter.One novel ingredient of the Hamiltonian scheme lies in the ‘variables’ that one uses in the description of a physical system. Up until now, the positions of particles were taken as primary, the velocities being simply the rate of change of position with respect to time. Recall (p. 217) that in the specification of the initial state of a Newtonian system we needed the positions and the velocities of all the particles in order that the subsequent behaviour be determinate. With the Hamiltonian formulation we must select the momenta of the particles rather than the velocities. (We noted on p. 214 that the momentum of a particle is just its velocity multiplied by its mass.) This might seem a small change in itself, but the important thing is that the position and momentum of each particle are to be treated as though they are independent quantities, more or less on an equal footing with one another. Thus one ‘pretends’, at first, that the momenta of the various particles have nothing to do with the rates of change of their respective position variables, but are just a separate set of variables, so we can imagine that they ‘could’ have been quite independent of the position motions. In the Hamiltonian formulation, we now have two sets of equations. One of these tells us how the momenta of the various particles are changing with time, and the other tells us how the positions are changing with time. In each case, the rates of change are determined by the various positions and momenta at that time.Roughly speaking, the first set of Hamilton’s equations states Newton’s crucial second law of motion (rate of change of momentum = force) while the second set of equations is telling us what the momenta actually are, in terms of the velocities (in effect, rate of change of position = momentum ÷ mass). Recall that the laws of motion of Galilei–Newton were described in terms of accelerations, i.e. rates of change of rates of change of position (i.e. ‘second order’ equations). Now, we need only to talk about rates of change of things (‘first order’ equations) rather than rates of change of rates of change of things. All these equations are derived from just one important quantity: the Hamiltonian function H, which is the expression for the total energy of the system in terms of all the position and momentum variables.The Hamiltonian formulation provides a very elegant and symmetrical description of mechanics. Just to see what they look like, let us write down the equations here, even though many readers will not be familiar with the calculus notions required for a full understanding–which will not be needed here. All we really want to appreciate, as far as calculus is concerned, is that the ‘dot’ appearing on the left-hand side of each equation stands for rate of change with respect to time (of momentum, in the first case, and position, in the second):Here the index i is being used simply to distinguish all the different momentum coordinates p1, p2, p3, p4, . . . and all the different position coordinates x1, x2, x3, x4, . . . For n unconstrained particles we shall have 3n momentum coordinates and 3n position coordinates (one, each, for the three independent directions in space). The symbol ∂ refers to ‘partial differentiation’ (‘taking derivatives while holding all the other variables constant’), and H
is the Hamiltonian function, as described above. (If you don’t know about ‘differentiation’, don’t worry. Just think of the right-hand sides of these equations as some perfectly well-defined mathematical expressions, written in terms of the xis and pis.)The coordinates x1, x2, . . . and p1, p2, . . . are actually allowed to be more general things than just ordinary Cartesian coordinates for particles (i.e. with the xis being ordinary distances, measured off in three different directions at right angles). Some of the coordinates xis could be angles, for example (in which case, the corresponding pis would be angular momenta, cf. p. 214, rather than momenta), or some other completely general measure. Remarkably, the Hamiltonian equations still hold in exactly the same form. In fact, with suitable choices of H, Hamilton’s equations still hold true for any system of classical equations whatever, not just for Newton’s equations. In particular, this will be the case for the Maxwell(–Lorentz) theory that we shall be considering shortly. Hamilton’s equations also hold true for special relativity. Even general relativity can, if due care is exercised, be subsumed into the Hamiltonian framework. Moreover, as we shall see later with Schrödinger’s equation (p. 372), this Hamiltonian framework provides the taking-off point for the equations of quantum mechanics. Such unity of form in the structure of dynamical equations, despite all the revolutionary changes that have occurred in physical theories over the past century or so, is truly remarkable!PHASE SPACEThe form of the Hamiltonian equations allows us to ‘visualize’ the evolution of a classical system in a very powerful and general way. Try to imagine a ‘space’ of a large number of dimensions, one dimension for each of the coordinates x1, x2, . . ., p1, p2, . . . (Mathematical spaces often have many more than three dimensions.) This space is called phase space (see Fig. 5.10). For n unconstrained particles, this will be a space of 6n dimensions (three position coordinates and three momentum coordinates for each particle). The reader may well worry that even for a single
particle this is already twice as many dimensions as she or he would normally be used to visualizing! The secret is not to be put off by this. Whereas six dimensions are, indeed, more dimensions than can be readily (!) pictured, it would actually not be of much use to us if we were in fact able to picture it. For just a room full of air molecules, the number of phase-space dimensions might be something likeFig. 5.10. Phase space. A single point Q of phase space represents the entire state of some physical system, including the instantaneous motions of all of its parts.10 000 000 000 000 000 000 000 000 000.There is not much hope in trying to obtain an accurate visualization of a space that big! Thus, the trick is not even to try – even in the case of the phase space for a single particle. Just think of some vague kind of three-dimensional (or even just two-dimensional) region. Have another look at Fig. 5.10. That will do.Now, how are we to visualize Hamilton’s equations in terms of phase space? First, we should bear in mind what a single point Q of phase space actually represents. It corresponds to a particular set of values for all the position coordinates x1, x2, . . . and for all the momentum coordinates p1, p2, . . . That is to say, Q represents our entire physical system, with a particular state of motion specified for every single one of its constituent particles. Hamilton’s equations tell us what the rates of change of all these coordinates are, when we know their present values; i.e. it governs how all the individual particles are to move. Translated into phase-space language, the equations are telling us how a single point Q in phase space must move, given the present location of Q in phase space. Thus, at each point of phase space, we have a little arrow – more correctly, a vector – which tells us the way that Q is moving, in order to describe the evolution of our entire system in time. The whole arrangement of arrows constitutes what is known as a vector field (Fig. 5.11). Hamilton’s equations thus define a vector field on phase space.Let us see how physical determinism is to be interpreted in terms of phase space. For initial data at time t = 0, we would have a particular set of values specified for all the position and momentum coordinates; that is to say, we have a particular choice of point Q in phase space. To find the evolution of the system in time, we simply follow the arrows. Thus, the entire evolution of our system with time – no matter how complicated that system might be – is described in phase space as just a single point moving along following the particular arrows that it encounters. We can think of the arrows as indicating the ‘velocity’ of our point Q in phase space. For a ‘long’ arrow, Q moves along swiftly, but if the arrow is ‘short’, Q’s motion will be sluggish. To see what our physical system is doing at time t, we simply look to see where Q has moved to, by that time, by following arrows in this way. Clearly this is a deterministic procedure. The way that Q moves is completely determined by the Hamiltonian vector field.Fig. 5.11. A vector field on phase space, representing time-evolution according to Hamilton’s equations.What about computability? If we start from a computable point in phase space (i.e. from a point all of whose position and momentum coordinates are computable numbers, cf. Chapter 3, p. 107) and we wait for a computable time t, do we necessarily end up at a point which can be computably obtained from t and the values of the coordinates at the starting point? The answer would certainly depend upon the choice of Hamiltonian function H. In fact there would be physical constants appearing in H, such as Newton’s gravitational constant or the speed of light – the exact values of these ones would depend on the choice of units, but others might be pure numbers – and it would be necessary to make sure that these constants are computable numbers if one is to have hope of obtaining an affirmative answer. If we assume that this is the case, then my guess would be that, for the usual Hamiltonians that are normally encountered in physics, the answer would indeed be in the affirmative. This is merely a guess, however, and the question is an interesting one that I hope will be examined further in the future.On the other hand, it seems to me that, for reasons similar to those that I raised briefly in connection with the billiard-ball world, this is not quite the relevant issue. It would require infinite precision for the coordinates of a phase-space point – i.e. all the decimal places! – in order for it to make sense to say that the point is non-computable. (A number described by a finite decimal is always computable.) A finite portion of a decimal expansion of a number tells us nothing about the computability of the entire expansion of that number. But all physical measurements have a definite limitation on how accurately they can be performed, and can only give information about a finite number of decimal places. Does this nullify the whole concept of ‘computable number’ as applied to physical measurements?Indeed, a device which could, in any useful way, take advantage of some (hypothetical) non-computable element in physical laws should presumably not have to rely on making measurements of unlimited precision. But it may be that I am taking too strict a line here. Suppose we have a physical device that, for known theoretical reasons, imitates some interesting non-algorithmic mathematical process. The exact behaviour of the device, if this behaviour could always be ascertained precisely, would then yield the correct answers to a succession of mathematically interesting yes/no questions for which there can be no algorithm (like those considered in Chapter 4). Any given algorithm would fail at some stage, and at that stage, the device would give us something new. The device might indeed involve examining some physical parameter to greater and greater accuracy, where more and more accuracy would be needed in order to go further and further down the list of questions. However, we do get something new from our device at a finite stage of accuracy, at least until we find an improved algorithm for the sequence of questions; then we should have to go to a greater accuracy in order to be able to achieve something that our improved algorithm cannot tell us.Nevertheless, it would still seem that ever-increasing accuracy in a physical parameter is an awkward and unsatisfactory way of coding information. Much preferable would be to acquire our information in a discrete (or ‘digital’) form. Answers to questions further and further down a list could then be achieved by examining more and more of the discrete units, or perhaps by examining a fixed set of discrete units again and again, where the required unlimited information could be spread over longer and longer time intervals. (We could imagine these discrete units to be built up from parts, each capable of an ‘on’ or ‘off’ state, like the Os and Is of the Turing machine descriptions given in Chapter 2.) For this, we seem to require devices of some kind which can take up (distinguishably) discrete states and which, after evolving according to the dynamical laws, would again each take up one of a set of discrete states. If this were the case we could avoid having to examine each device to arbitrarily high accuracy.Now, do Hamiltonian systems actually behave like this? A necessary ingredient would be some kind of stability of behaviour, so that it would be a clear-cut matter to ascertain which of these discrete states our device is actually in. Once it is in one of these states we shall want it to stay there (at least for a significant period of time) and not to drift from one of these states into another. Moreover, if the system arrives in these states a little inaccurately, we do not want these inaccuracies to build up; indeed, we really require that such inaccuracies should die away with time. Now our proposed device would have to be made up of particles (or other sub-units) which need to be described in terms of continuous parameters, and each distinguishable ‘discrete’ state would have to cover some range of these continuous parameters. (For example, one possible way of representing discrete alternatives would be to have a particle which can lie in one box or another. To specify that the particle indeed lies in one of the boxes we need to say that the position coordinates of the particle lie within some range.) What this means, in terms of phase space, is that each of our ‘discrete’ alternatives must correspond to a region in phase space, so that different phase-space points lying in the same region would correspond to the same one of these alternatives for our device (Fig. 5.12).Now suppose that the device starts off with its phase-space point in some region R0 corresponding to a particular one of these alternatives. We think of R0 as being dragged along the Hamiltonian vector field as time proceeds, until at time t the region becomes Rt. In picturing this, we are imagining, simultaneously, the time-evolution of our system for all possible starting states corresponding to this same alternative. (See Fig. 5.13.) The question of stability (in the sense we are interested in here) is whether, as t increases, the region Rt remains localized or whether it begins to spread out over the phase space. If such regions remain localized as time progresses, then we have a measure of stability for our system. Points of phase space which are close together (so that they correspond to detailed physical states of the system which closely resemble one another) will remain close together in phase space, and inaccuracies in their specification will not become magnified with time. Any undue spreading would entail an effective unpredictability in the behaviour of the system.Fig. 5.12. A region in phase space corresponds to a range of possible values for the positions and momenta of all the particles. Such a region might represent a distinguishable state (i.e. an ‘alternative’) for some device.Fig. 5.13. As time evolves, a phase state region R0 is dragged along by the vector field to a new region Rt. This could represent the time-evolution of a particular alternative for our device.What can be said about Hamiltonian systems generally? Do regions in phase space tend to spread with time or do they not? It might seem that for a problem of such generality, very little could be said. However, it turns out that there is a very beautiful theorem, due to the distinguished French mathematician Joseph Liouville (1809–1882), that tells us that the volume of any region of the phase space must remain constant under any Hamiltonian evolution. (Of course, since our phase space has some high dimension, this has to be a ‘volume’ in the appropriate high-dimensional sense.) Thus the volume of each Rt must be the same as the volume of the original R0. At first sight this would seem to answer our stability question in the affirmative. For the size – in the sense of this phase-space volume – of our region cannot grow, so it would seem that our region cannot spread itself out over the phase space.However, this is deceptive, and on reflection we see that the very reverse is likely to be the case! In Fig. 5.14 I have tried to indicate the sort of behaviour that one would expect, in general. We can imagine that the initial region R0 is a small ‘reasonably’ shaped region, more roundish in shape than spindly – indicating that the states that belong to R0 can be characterized in some way that does not require an unreasonable precision. However, as time unfolds, the region Rt begins to distort and stretch – at first being perhaps somewhat amoeba-like, but then stretching out to great distances in the phase space and winding about backwards and forwards in a very complicated way. The volume indeed remains the same, but this same small volume can get very thinly spread out over huge regions of the phase space. For a somewhat analogous situation, think of a small drop of ink placed in a large container of water. Whereas the actual volume of material in the ink remains unchanged, it eventually becomes thinly spread over the entire contents of the container. In the phase-space case, the region Rt is likely to behave in a similar way. It may not spread itself over the whole of phase space (which is the extreme situation referred to as ‘ergodic’), but it is likely to spread over an enormously larger region than it started from. (For further discussion, see Davies 1974.)Fig. 5.14. Despite the fact that Liouville’s theorem tells us that phase-space volume does not change with time-evolution, this volume will normally effectively spread outwards because of the extreme complication of this evolution.The trouble is that preservation of volume does not at all imply preservation of shape: small regions will tend to get distorted, and this distortion gets magnified over large distances. The problem is a much more serious one in high dimensions than in low dimensions, since there are so many more ‘directions’ in which the region can locally spread. In fact, far from being a ‘help’, in keeping the region Rt under control, Liouville’s theorem actually presents us with a fundamental problem! Without Liouville’s theorem, one might envisage that this undoubted tendency for a region to spread out in phase space could, in suitable circumstances, be compensated by a reduction in overall volume. However, the theorem tells us that this is impossible, and we have to face up to this striking implication – a universal feature of all classical dynamical (Hamiltonian) systems of normal type!9We may ask, in view of this spreading throughout phase space, how is it possible at all to make predictions in classical mechanics? That is, indeed, a good question. What this spreading tells us is that, no matter how accurately we know the initial state of a system (within some reasonable limits), the uncertainties will tend to grow in time and our initial information may become almost useless. Classical mechanics is, in this kind of sense, essentially unpredictable. (Recall the concept of ‘chaos’ considered above.)How is it, then, that Newtonian dynamics has been seen to be so successful? In the case of celestial mechanics (i.e. the motion of heavenly bodies under gravity), the reasons seem to be, first, that one is concerned with a comparatively small number of coherent bodies (the sun, planets, and moons) which are greatly segregated with regard to mass – so that to a first approximation one can ignore the perturbing effect of the less massive bodies and treat the larger ones as just a few bodies acting under each other’s influence – and, second, that the dynamical laws that apply to the individual particles which constitute those bodies can be seen also to operate at the level of the bodies themselves – so that to a very good approximation, the sun, planets, and moons can themselves be actually treated as particles, and we do not have to worry about all the little detailed motions of the individual particles that actually compose these heavenly bodies!10 Again we get away with considering just a ‘few’ bodies, and the spread in phase space is not important.Apart from celestial mechanics and the behaviour of projectiles (which is really just a special case of celestial mechanics), and the study of simple systems where small numbers of particles are involved, the main ways that Newtonian mechanics is used appear not to be in this detailed ‘deterministically predictive’ way at all. Rather, one uses the general Newtonian scheme to make models from which overall properties of behaviour can be inferred. Certain precise consequences of the laws, such as conservation of energy, momentum, and angular momentum do, indeed, have relevance at all scales. Moreover, there are statistical properties that can be combined with the dynamical laws governing the individual particles and which can be used to make overall predictions concerning behaviour. (See the discussion of thermodynamics in Chapter 7; the phase-space spreading effect that we have been discussing has some intimate relation to the second law of thermodynamics, and by exercising due care one can use these ideas in genuinely predictive ways.) Newton’s own remarkable calculation of the speed of sound in air (subtly corrected over a century later by Laplace) was a good example of this. However, it is very rare indeed that the determinism inherent in Newtonian (or, more generally, Hamiltonian) dynamics is actually used.This spreading effect in phase space has another remarkable implication. It tells us, in effect, that classical mechanics cannot
actually be true of our world! I am somewhat overstating this implication, but perhaps not greatly so. Classical mechanics can account well for the behaviour of fluid bodies – particularly gases, but also liquids to a good extent – where one is concerned only with overall ‘averaged’ properties of systems of particles, but it has problems in accounting for the structure of solids, where a more detailed organized structure is needed. There is a problem of how a solid body can hold its shape when it is composed of myriads of point-like particles whose organizational arrangement is continually being reduced, because of the spreading in phase space. As we now know, quantum theory is needed in order that the actual structure of solids can be properly understood. Somehow, quantum effects can prevent this phase-space spreading. This is an important issue to which we shall have to return later (see Chapters 8 and 9).This issue is a matter of relevance, also, to the question of the construction of a ‘computing machine’. The phase-space spreading is something that needs to be controlled. A region in phase space that corresponds to a ‘discrete’ state for a computing device (such as R0, as described above) must not be allowed to spread unduly. Recall that even the Fredkin–Toffoli ‘billiard-ball computer’ required some extraneous solid walls in order that it could function. ‘Solidity’ for an object itself composed of many particles is something that really needs quantum mechanics in order to work. It seems that even a ‘classical’ computing machine must borrow from the effects of quantum physics if it is to function effectively!MAXWELL’S ELECTROMAGNETIC THEORYIn the Newtonian picture of the world, one thinks of tiny particles acting upon one another by forces which operate at a distance – where the particles, if not entirely point-like, may be considered to rebound off one another occasionally by actual physical contact. As I have stated before (p. 217), the forces of electricity and magnetism (the existence of both having been known since antiquity, and studied in some detail by William Gilbert in 1600 and Benjamin Franklin in 1752), act in a way similar to gravitational forces in that they also fall off as the inverse square of the distance, though repulsively rather than attractively – i.e. with like repelling like – and electric charge (and magnetic pole strength), rather than the mass, measures the strength of the force. At this level there is no difficulty about incorporating electricity and magnetism into the Newtonian scheme. The behaviour of light, also, can be roughly accommodated (though with some definite difficulties), either by regarding light at being composed of individual particles (‘photons’, as we should now call them) or by regarding light as a wave motion in some medium, where in the latter case this medium (‘ether’) must itself be thought of as being composed of particles.The fact that moving electric charges can give rise to magnetic forces caused some additional complication, but it did not disrupt the scheme as a whole. Numerous mathematicians and physicists (including Gauss) had proposed systems of equations for the effects of moving electric charges which had seemed to be satisfactory within the general Newtonian framework. The first scientist to have made a serious challenge to the ‘Newtonian’ picture seems to have been the great English experimentalist and theoretician Michael Faraday (1791–1867).To understand the nature of this challenge, we must first come to terms with the concept of a physical field. Consider a magnetic field first. Most readers will have come across the behaviour of iron filings when placed on a piece of paper lying on a magnet. The filings line up in a striking way along what are called ‘magnetic lines of force’. We imagine that the lines of force remain present even when the filings are not there. They constitute what we refer to as a magnetic field. At each point in space, this ‘field’ is oriented in a certain direction, namely the direction of the line of force at that point. Actually, we have a vector at each point, so the magnetic field provides us with an example of a vector field. (We may compare this with the Hamiltonian vector field that we considered in the previous section, but now this is a vector field in ordinary space rather than in phase space.) Similarly, an electrically charged body will be surrounded by a different kind of field, known as an electric field, and a gravitational field likewise surrounds any massive body. These, also, will be vector fields in space.Such ideas were known a long time before Faraday, and they had become very much part of the armoury of theorists in Newtonian mechanics. But the prevailing viewpoint was not to regard such ‘fields’ as constituting, in themselves, actual physical substance. Rather, they would be thought of as providing the necessary ‘bookkeeping’ for the forces that would act, were a suitable particle to be placed at various different points. However, Faraday’s profound experimental findings (with moving coils, magnets, and the like) led him to believe that electric and magnetic fields are real physical ‘stuff’ and, moreover, that varying electric and magnetic fields might sometimes be able to ‘push’ each other along through otherwise empty space to produce a kind of disembodied wave! He conjectured that light itself might consist of such waves. Such a view would have been at variance with the prevailing ‘Newtonian wisdom’, whereby such fields were not thought of as ‘real’ in any sense, but merely convenient mathematical auxiliaries to the ‘true’ Newtonian point-particle–action-at-a-distance picture of ‘actual reality’.Confronted with Faraday’s experimental findings, together with earlier ones by the remarkable French physicist Andre Marie Amperé (1775–1836) and others, and inspired by Faraday’s vision, the great Scottish physicist and mathematician James Clerk Maxwell (1831–1879) puzzled about the mathematical form of the equations for electric and magnetic fields that arose from those findings. With a remarkable stroke of insight he proposed a change in the equations – seemingly perhaps rather slight, but fundamental in its implications. This change was not at all suggested by (although it was consistent with) the known experimental facts. It was a result of Maxwell’s own theoretical requirements, partly physical, partly mathematical, and partly aesthetic. One implication of Maxwell’s equations was that electric and magnetic fields would indeed ‘push’ each other along through empty space. An oscillating magnetic field would give rise to an oscillating electric field (this was implied by Faraday’s experimental findings), and this oscillating electric field would, in turn, give rise to an oscillating magnetic field (by Maxwell’s theoretical inference), and this again would give rise to an electric field and so on. (See Figs 6.26, 6.27 on pp. 350 and 351 for detailed pictures of such waves.) Maxwell was able to calculate the speed that this effect would propagate through space – and he found that it would be the speed of light! Moreover these so-called electromagnetic waves would exhibit the interference and puzzling polarization properties of light that had long been known (we shall come to these in Chapter 6, pp. 304, 349). In addition to accounting for the properties of visible light, for which the waves would have a particular range of wavelengths (4–7 × 10–7m), electromagnetic waves of other wavelengths were predicted to occur and to be produced by electric currents in wires. The existence of such waves was established experimentally by the remarkable German physicist Heinrich Hertz in 1888. Faraday’s inspired hope had indeed found a firm basis in the marvellous equations of Maxwell!Although it will not be necessary for us to appreciate the details of Maxwell’s equations here, there will be no harm in our just taking a peek at them:Here, E, B, and j are vector fields describing the electric field, the magnetic field, and the electric current, respectively; ϱ describes the density of electric charge, and c is just a constant: the speed of light.11 Do not worry about the terms ‘curl’ and ‘div’, which simply refer to different kinds of spatial variation. (They are certain combinations of partial derivative operators, taken with respect to the space coordinates. Recall the ‘partial derivative’ operation, with symbol ∂, that we encountered in connection with Hamilton’s equations.) The operators ∂/∂t appearing on the left-hand sides of the first two equations are, in effect, the same as the ‘dot’ that was used in Hamilton’s equations, the difference being just technical. Thus, ∂E/∂t means ‘the rate of change of the electric field’ and ∂B/∂t means ‘the rate of change of the magnetic field’. The first equation* tells how the electric field is changing with time, in terms of what the magnetic field and electric current are doing at the moment; while the second equation tells how the magnetic field is changing with time in terms of what the electric field is doing at the moment. The third equation is, roughly speaking, a coded form of the inverse square law telling how the electric field (at the moment) must be related to the distribution of charges; while the fourth equation says what would be the same thing about the magnetic field, except that in this case there are no ‘magnetic charges’ (separate ‘north pole’ or ‘south pole’ particles).These equations are somewhat like Hamilton’s in that they tell what must be the rate of change, with time, of the relevant quantities (here the electric and magnetic fields) in terms of what their values are at any given time. Thus Maxwell’s equations are deterministic, just as are ordinary Hamiltonian theories. The only difference – and it is an important difference – is that Maxwell’s equations are field equations rather than particle equations, which means that one needs an infinite number of parameters to describe the state of the system (the field vectors at every single point in space), rather than just the finite number that is needed for a particle theory (three coordinates of position and three of momenta for each particle). Thus the phase space for Maxwell theory is a space of an infinite number of dimensions! (As I have mentioned earlier, Maxwell’s equations can actually be encompassed by the general Hamiltonian framework, but this framework must be extended slightly, because of this infinite-dimensionality.12)The fundamentally new ingredient in our picture of physical reality as presented by Maxwell theory, over and above what had been the case previously, is that now fields must be taken seriously in their own right and cannot be regarded as mere mathematical appendages to what were the ‘real’ particles of Newtonian theory. Indeed, Maxwell showed that when the fields propagate as electromagnetic waves they actually carry definite amounts of energy
with them. He was able to provide an explicit expression for this energy. The remarkable fact that energy can indeed be transported from place to place by these ‘disembodied’ electromagnetic waves was, in effect, experimentally confirmed by Hertz’s detection of such waves. It is now something familiar to us – though still a very striking fact – that radio waves can actually carry energy!COMPUTABILITY AND THE WAVE EQUATIONMaxwell was able to deduce directly from his equations that, in regions of space where there are no charges or currents (i.e. where j = 0, ϱ = 0 in the equations above), all the components of the electric and magnetic fields must satisfy an equation known as the wave equation.* The wave equation may be regarded as a ‘simplified version’ of the Maxwell equations since it is an equation for a single quantity, rather than for all the six components of the electric and magnetic fields. Its solutions exemplify wavelike behaviour without added complications, such as of the ‘polarization’ of Maxwell theory (direction of the electric field vector, see p. 349).The wave equation has an additional interest for us here because it has been explicitly studied in relation to its computability properties. In fact Marian Boykan Pour-El and Ian Richards (1979, 1981, 1982, cf. also 1989) have been able to show that even though solutions of the wave equation behave deterministically in the ordinary sense – i.e. data provided at an initial time will determine the solution at all other times – there exist computable initial data, of a certain ‘peculiar’ kind, with the property that for a later computable time the determined value of the field is actually not computable. Thus the equations of a plausible physical field theory (albeit not quite the Maxwell theory that actually holds true in our world) can, in the sense of Pour-El and Richards, give rise to a non-computable evolution!On the face of it this is a rather startling result – and it would seem to contradict what I had been conjecturing in the last section but one concerning the probable computability of ‘reasonable’ Hamiltonian systems. However, while the Pour-El–Richards result is certainly striking and mathematically relevant, it does not really contradict that conjecture in a way that makes good physical sense. The reason is that their ‘peculiar’ kind of initial data is not ‘smoothly varying’,13 in a way that one would normally require for a physically sensible field. Indeed, Pour-El and Richards actually prove that non-computability cannot arise for the wave equation if we disallow this kind of field. In any case, even if fields of this kind were permitted, it would be hard to see how any physical ‘device’ (such as a human brain?) could make use of such ‘non-computability’. It could have relevance only when measurements of arbitrarily high precision are allowed, which, as I described earlier, is not very realistic physically. Nevertheless, the Pour-El–Richards results represent an intriguing beginning to an important area of investigation in which little work has been done so far.THE LORENTZ EQUATION OF MOTION; RUNAWAY PARTICLESAs they stand, Maxwell’s equations are not really quite complete as a system of equations. They provide a wonderful description of the way that electric and magnetic fields propagate if we are given the distribution of electric charges and currents. These charges are physically given to us as charged particles – mainly electrons and protons, as we now know – and the currents arise from the motions of such particles. If we know where these particles are and how they are moving, then Maxwell’s equations tell us how the electromagnetic field is to behave. What Maxwell’s equations do not tell us is how the particles themselves are to behave. A partial answer to this question had been known in Maxwell’s day, but a satisfactory system of equations had not been settled upon until, in 1895 the remarkable Dutch physicist Hendrick Antoon Lorentz used ideas related to those of special relativity theory in order to derive what are now known as the Lorentz equations of
motion for a charged particle (cf. Whittaker 1910, pp. 310, 395). These equations tell us how the velocity of a charged particle continuously changes, owing to the electric and magnetic fields at the point where the particle is located.14 When the Lorentz equations are adjoined to those of Maxwell, one obtains rules for the time-evolution both of the charged particles and of the electromagnetic field.However, all is not entirely well with this system of equations. They provide excellent results if the fields are very uniform down at the scale of size of the diameters of the particles themselves (taking this size as the electron’s ‘classical radius’ – about 10–15m), and the particles’ motions are not too violent. However, there is a difficulty of principle here which can become important in other circumstances. What the Lorentz equations tell us to do is to examine the electromagnetic field at the precise point at which the charged particle is located (and, in effect, to provide us with a ‘force’ at that point). Where is that point to be taken to be if the particle has a finite size? Do we take the ‘centre’ of the particle, or else do we average the field (for the ‘force’) over all points 